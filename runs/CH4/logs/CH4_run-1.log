2022-12-29 17:00:09.031 INFO: {
    "bag_scale": 6,
    "beta": "-10",
    "canvas_size": 6,
    "clip_ratio": 0.2,
    "cutoff": 4.0,
    "data_dir": "runs/CH4/data",
    "device": "cuda",
    "discount": 1.0,
    "entropy_coef": 0.06,
    "eval_formulas": "CH4",
    "eval_freq": 1,
    "formulas": "CH4",
    "gradient_clip": 0.5,
    "keep_models": false,
    "lam": 1.0,
    "learning_rate": 0.0003,
    "load_latest": false,
    "load_model": null,
    "log_dir": "runs/CH4/logs",
    "log_level": "DEBUG",
    "max_mean_distance": 1.8,
    "max_num_steps": 30000,
    "max_num_train_iters": 10,
    "max_solo_distance": 2.0,
    "maxl": 4,
    "min_atomic_distance": 0.6,
    "min_mean_distance": 0.8,
    "min_reward": -20.0,
    "mini_batch_size": 64,
    "model": "schnet_edge",
    "model_dir": "runs/CH4/models",
    "name": "CH4",
    "network_width": 128,
    "num_cg_levels": 3,
    "num_channels_hidden": 10,
    "num_channels_per_element": 4,
    "num_envs": 12,
    "num_eval_episodes": null,
    "num_gaussians": 3,
    "num_interactions": 3,
    "num_steps_per_iter": 216,
    "optimizer": "adam",
    "results_dir": "runs/CH4/results",
    "save_freq": 10,
    "save_rollouts": "train",
    "seed": 1,
    "symbols": "X,H,C",
    "target_kl": 0.03,
    "update_edges": true,
    "vf_coef": 0.001
}
2022-12-29 17:00:09.069 INFO: CUDA Device: 0
2022-12-29 17:00:09.070 INFO: Training bags: ['CH4']
2022-12-29 17:00:09.070 INFO: Evaluation bags: ['CH4']
2022-12-29 17:00:30.385 INFO: Number of parameters: 299732
2022-12-29 17:00:30.398 INFO: Starting PPO
2022-12-29 17:00:30.398 INFO: Iteration: 0/137, steps: 0
2022-12-29 17:00:34.932 DEBUG: There is a single atom floating around
2022-12-29 17:00:35.209 DEBUG: There is a single atom floating around
2022-12-29 17:00:35.849 DEBUG: There is a single atom floating around
2022-12-29 17:00:37.042 DEBUG: There is a single atom floating around
2022-12-29 17:00:39.046 DEBUG: Atoms are too close
2022-12-29 17:00:39.372 DEBUG: Atoms are too close
2022-12-29 17:00:43.638 DEBUG: Atoms are too close
2022-12-29 17:00:43.639 DEBUG: Atoms are too close
2022-12-29 17:00:43.640 DEBUG: Atoms are too close
2022-12-29 17:00:46.007 DEBUG: There is a single atom floating around
2022-12-29 17:00:49.345 DEBUG: There is a single atom floating around
2022-12-29 17:00:51.264 DEBUG: Atoms are too close
2022-12-29 17:00:51.333 DEBUG: Atoms are too close
2022-12-29 17:00:51.630 DEBUG: Atoms are too close
2022-12-29 17:00:53.465 DEBUG: Atoms are too close
2022-12-29 17:00:54.908 DEBUG: Atoms are too close
2022-12-29 17:00:55.221 DEBUG: Atoms are too close
2022-12-29 17:00:57.264 DEBUG: Atoms are too close
2022-12-29 17:00:57.266 DEBUG: Atoms are too close
2022-12-29 17:00:58.139 DEBUG: Atoms are too close
2022-12-29 17:00:59.142 DEBUG: Atoms are too close
2022-12-29 17:01:00.420 DEBUG: There is a single atom floating around
2022-12-29 17:01:01.148 DEBUG: Atoms are too close
2022-12-29 17:01:04.709 DEBUG: There is a single atom floating around
2022-12-29 17:01:04.710 DEBUG: There is a single atom floating around
2022-12-29 17:01:05.683 DEBUG: Atoms are too close
2022-12-29 17:01:06.124 DEBUG: Atoms are too close
2022-12-29 17:01:07.482 DEBUG: Atoms are too close
2022-12-29 17:01:09.538 DEBUG: There is a single atom floating around
2022-12-29 17:01:09.884 DEBUG: There is a single atom floating around
2022-12-29 17:01:11.631 DEBUG: Atoms are too close
2022-12-29 17:01:11.632 DEBUG: There is a single atom floating around
2022-12-29 17:01:13.649 DEBUG: Atoms are too close
2022-12-29 17:01:13.650 DEBUG: Atoms are too close
2022-12-29 17:01:13.956 DEBUG: There is a single atom floating around
2022-12-29 17:01:14.352 DEBUG: Atoms are too close
2022-12-29 17:01:14.353 DEBUG: There is a single atom floating around
2022-12-29 17:01:14.642 DEBUG: There is a single atom floating around
2022-12-29 17:01:17.095 DEBUG: Atoms are too close
2022-12-29 17:01:18.597 DEBUG: Atoms are too close
2022-12-29 17:01:18.978 INFO: Training rollout: return=-15.220 (8.4), episode length=3.8
2022-12-29 17:01:18.979 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:01:18.985 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-0_train.pkl
2022-12-29 17:01:19.900 DEBUG: Taking gradient step
2022-12-29 17:01:19.918 DEBUG: Loss 0: {'policy_loss': 0.0602344601284792, 'entropy_loss': -0.04163164272904396, 'vf_loss': 0.27446289679293606, 'total_loss': 0.2930657141923713, 'approx_kl': -8.176236221402178e-08, 'clip_fraction': 0.0, 'grad_norm': 18.75990867614746}
2022-12-29 17:01:20.765 DEBUG: Early stopping at step 1 for reaching max KL.
2022-12-29 17:01:20.765 INFO: Optimization: policy loss=0.060, vf loss=0.274, entropy loss=-0.042, total loss=0.293, num steps=1
2022-12-29 17:01:20.765 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:01:22.291 INFO: Evaluation rollout: return=0.321 (0.0), episode length=5.0
2022-12-29 17:01:22.292 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:01:22.296 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-216.model
2022-12-29 17:01:22.329 INFO: Iteration: 1/137, steps: 216
2022-12-29 17:01:25.171 DEBUG: There is a single atom floating around
2022-12-29 17:01:26.346 DEBUG: There is a single atom floating around
2022-12-29 17:01:26.347 DEBUG: There is a single atom floating around
2022-12-29 17:01:27.088 DEBUG: Atoms are too close
2022-12-29 17:01:31.714 DEBUG: Atoms are too close
2022-12-29 17:01:33.622 DEBUG: Atoms are too close
2022-12-29 17:01:34.274 DEBUG: Atoms are too close
2022-12-29 17:01:34.729 DEBUG: Atoms are too close
2022-12-29 17:01:35.374 DEBUG: Atoms are too close
2022-12-29 17:01:35.375 DEBUG: Atoms are too close
2022-12-29 17:01:36.631 DEBUG: There is a single atom floating around
2022-12-29 17:01:39.079 DEBUG: There is a single atom floating around
2022-12-29 17:01:39.920 DEBUG: There is a single atom floating around
2022-12-29 17:01:39.921 DEBUG: Atoms are too close
2022-12-29 17:01:41.785 DEBUG: Atoms are too close
2022-12-29 17:01:41.786 DEBUG: There is a single atom floating around
2022-12-29 17:01:44.258 DEBUG: There is a single atom floating around
2022-12-29 17:01:44.872 DEBUG: There is a single atom floating around
2022-12-29 17:01:45.769 DEBUG: There is a single atom floating around
2022-12-29 17:01:50.756 DEBUG: There is a single atom floating around
2022-12-29 17:01:52.004 DEBUG: There is a single atom floating around
2022-12-29 17:01:52.006 DEBUG: Atoms are too close
2022-12-29 17:01:52.322 DEBUG: Atoms are too close
2022-12-29 17:01:53.130 DEBUG: Atoms are too close
2022-12-29 17:01:53.458 DEBUG: There is a single atom floating around
2022-12-29 17:01:54.981 DEBUG: Atoms are too close
2022-12-29 17:01:55.286 DEBUG: There is a single atom floating around
2022-12-29 17:01:56.969 DEBUG: There is a single atom floating around
2022-12-29 17:01:56.971 DEBUG: Atoms are too close
2022-12-29 17:01:56.971 DEBUG: There is a single atom floating around
2022-12-29 17:01:57.152 DEBUG: Atoms are too close
2022-12-29 17:01:57.638 DEBUG: Atoms are too close
2022-12-29 17:01:58.325 DEBUG: Atoms are too close
2022-12-29 17:01:59.877 DEBUG: Atoms are too close
2022-12-29 17:02:01.330 DEBUG: There is a single atom floating around
2022-12-29 17:02:01.331 DEBUG: There is a single atom floating around
2022-12-29 17:02:01.672 DEBUG: There is a single atom floating around
2022-12-29 17:02:02.473 DEBUG: Atoms are too close
2022-12-29 17:02:03.163 DEBUG: Atoms are too close
2022-12-29 17:02:03.824 DEBUG: Atoms are too close
2022-12-29 17:02:06.117 DEBUG: There is a single atom floating around
2022-12-29 17:02:07.833 DEBUG: There is a single atom floating around
2022-12-29 17:02:07.896 INFO: Training rollout: return=-15.695 (8.2), episode length=3.6
2022-12-29 17:02:07.897 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:02:07.899 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-216_train.pkl
2022-12-29 17:02:08.745 DEBUG: Taking gradient step
2022-12-29 17:02:08.755 DEBUG: Loss 0: {'policy_loss': 0.008174789750472013, 'entropy_loss': -0.04296259395778179, 'vf_loss': 0.25751500674178074, 'total_loss': 0.22272720253447095, 'approx_kl': -3.2984341258313066e-08, 'clip_fraction': 0.0, 'grad_norm': 15.562313079833984}
2022-12-29 17:02:09.512 DEBUG: Taking gradient step
2022-12-29 17:02:09.522 DEBUG: Loss 1: {'policy_loss': -0.005621901735854827, 'entropy_loss': -0.04388580471277237, 'vf_loss': 0.25467287585801746, 'total_loss': 0.20516516940939025, 'approx_kl': 0.017526533920317888, 'clip_fraction': 0.23046875, 'grad_norm': 11.051620483398438}
2022-12-29 17:02:10.332 DEBUG: Taking gradient step
2022-12-29 17:02:10.342 DEBUG: Loss 2: {'policy_loss': 0.006097838430313876, 'entropy_loss': -0.044583857990801334, 'vf_loss': 0.2540058276970667, 'total_loss': 0.21551980813657923, 'approx_kl': 0.030568762682378292, 'clip_fraction': 0.30859375, 'grad_norm': 12.722668647766113}
2022-12-29 17:02:11.134 DEBUG: Taking gradient step
2022-12-29 17:02:11.144 DEBUG: Loss 3: {'policy_loss': -0.03922451316013575, 'entropy_loss': -0.044402957893908024, 'vf_loss': 0.24645355499886665, 'total_loss': 0.16282608394482287, 'approx_kl': 0.03819811996072531, 'clip_fraction': 0.3606770858168602, 'grad_norm': 13.793928146362305}
2022-12-29 17:02:11.946 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 17:02:11.946 INFO: Optimization: policy loss=-0.039, vf loss=0.246, entropy loss=-0.044, total loss=0.163, num steps=4
2022-12-29 17:02:11.947 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:02:13.445 INFO: Evaluation rollout: return=0.394 (0.0), episode length=5.0
2022-12-29 17:02:13.446 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:02:13.448 INFO: Iteration: 2/137, steps: 432
2022-12-29 17:02:15.508 DEBUG: There is a single atom floating around
2022-12-29 17:02:15.841 DEBUG: There is a single atom floating around
2022-12-29 17:02:16.171 DEBUG: There is a single atom floating around
2022-12-29 17:02:18.512 DEBUG: Atoms are too close
2022-12-29 17:02:26.065 DEBUG: Atoms are too close
2022-12-29 17:02:26.969 DEBUG: Atoms are too close
2022-12-29 17:02:27.533 DEBUG: Atoms are too close
2022-12-29 17:02:27.883 DEBUG: Atoms are too close
2022-12-29 17:02:28.738 DEBUG: Atoms are too close
2022-12-29 17:02:30.305 DEBUG: Atoms are too close
2022-12-29 17:02:32.338 DEBUG: There is a single atom floating around
2022-12-29 17:02:35.765 DEBUG: Atoms are too close
2022-12-29 17:02:36.318 DEBUG: There is a single atom floating around
2022-12-29 17:02:36.669 DEBUG: Atoms are too close
2022-12-29 17:02:38.194 DEBUG: There is a single atom floating around
2022-12-29 17:02:40.727 DEBUG: Atoms are too close
2022-12-29 17:02:41.194 DEBUG: There is a single atom floating around
2022-12-29 17:02:41.258 DEBUG: There is a single atom floating around
2022-12-29 17:02:41.259 DEBUG: Atoms are too close
2022-12-29 17:02:44.682 DEBUG: There is a single atom floating around
2022-12-29 17:02:50.885 DEBUG: Atoms are too close
2022-12-29 17:02:53.363 DEBUG: Atoms are too close
2022-12-29 17:02:53.426 DEBUG: Atoms are too close
2022-12-29 17:02:53.427 DEBUG: Atoms are too close
2022-12-29 17:02:53.590 DEBUG: Atoms are too close
2022-12-29 17:02:53.739 DEBUG: Atoms are too close
2022-12-29 17:02:54.065 DEBUG: Atoms are too close
2022-12-29 17:02:56.915 DEBUG: There is a single atom floating around
2022-12-29 17:02:57.856 DEBUG: Atoms are too close
2022-12-29 17:02:57.857 DEBUG: There is a single atom floating around
2022-12-29 17:03:00.124 DEBUG: Atoms are too close
2022-12-29 17:03:02.682 DEBUG: Atoms are too close
2022-12-29 17:03:02.745 INFO: Training rollout: return=-13.130 (9.5), episode length=4.0
2022-12-29 17:03:02.746 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:03:02.748 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-432_train.pkl
2022-12-29 17:03:03.546 DEBUG: Taking gradient step
2022-12-29 17:03:03.556 DEBUG: Loss 0: {'policy_loss': 0.035802551142251476, 'entropy_loss': -0.043237616308033466, 'vf_loss': 0.20095050761893735, 'total_loss': 0.19351544245315538, 'approx_kl': -4.8118309337041865e-09, 'clip_fraction': 0.0, 'grad_norm': 23.047571182250977}
2022-12-29 17:03:04.385 DEBUG: Taking gradient step
2022-12-29 17:03:04.395 DEBUG: Loss 1: {'policy_loss': -0.03264938243017472, 'entropy_loss': -0.044206044636666775, 'vf_loss': 0.18937109215903628, 'total_loss': 0.11251566509219478, 'approx_kl': 0.005343789933249354, 'clip_fraction': 0.1041666679084301, 'grad_norm': 13.30427360534668}
2022-12-29 17:03:05.208 DEBUG: Taking gradient step
2022-12-29 17:03:05.222 DEBUG: Loss 2: {'policy_loss': -0.03316507632981846, 'entropy_loss': -0.04245896637439728, 'vf_loss': 0.1878947440133137, 'total_loss': 0.11227070130909797, 'approx_kl': 0.010299392510205507, 'clip_fraction': 0.2174479179084301, 'grad_norm': 12.2322998046875}
2022-12-29 17:03:06.108 DEBUG: Taking gradient step
2022-12-29 17:03:06.118 DEBUG: Loss 3: {'policy_loss': -0.0479258206476592, 'entropy_loss': -0.04056636290624738, 'vf_loss': 0.1831774662189021, 'total_loss': 0.09468528266499554, 'approx_kl': 0.027738377917557955, 'clip_fraction': 0.2890625, 'grad_norm': 13.446538925170898}
2022-12-29 17:03:06.899 DEBUG: Taking gradient step
2022-12-29 17:03:06.909 DEBUG: Loss 4: {'policy_loss': -0.013116523697968042, 'entropy_loss': -0.04184952564537525, 'vf_loss': 0.19060911876312697, 'total_loss': 0.13564306941978369, 'approx_kl': 0.04369836952537298, 'clip_fraction': 0.3046875, 'grad_norm': 11.510031700134277}
2022-12-29 17:03:07.696 DEBUG: Taking gradient step
2022-12-29 17:03:07.706 DEBUG: Loss 5: {'policy_loss': -0.015935142374414644, 'entropy_loss': -0.0414779344573617, 'vf_loss': 0.18682236686285497, 'total_loss': 0.1294092900310786, 'approx_kl': 0.033543790224939585, 'clip_fraction': 0.28515625, 'grad_norm': 10.687475204467773}
2022-12-29 17:03:08.512 DEBUG: Taking gradient step
2022-12-29 17:03:08.522 DEBUG: Loss 6: {'policy_loss': -0.04857870301334249, 'entropy_loss': -0.041750370524823666, 'vf_loss': 0.1802248438864255, 'total_loss': 0.08989577034825937, 'approx_kl': 0.02589030982926488, 'clip_fraction': 0.34375, 'grad_norm': 8.832980155944824}
2022-12-29 17:03:09.311 DEBUG: Taking gradient step
2022-12-29 17:03:09.322 DEBUG: Loss 7: {'policy_loss': -0.025556947923220507, 'entropy_loss': -0.04071912541985512, 'vf_loss': 0.18025844177886735, 'total_loss': 0.11398236843579172, 'approx_kl': 0.02492071408778429, 'clip_fraction': 0.3125, 'grad_norm': 11.782214164733887}
2022-12-29 17:03:10.121 DEBUG: Taking gradient step
2022-12-29 17:03:10.130 DEBUG: Loss 8: {'policy_loss': -0.027352316156883077, 'entropy_loss': -0.041343506425619125, 'vf_loss': 0.17881255615109393, 'total_loss': 0.11011673356859168, 'approx_kl': 0.011178270215168595, 'clip_fraction': 0.3046875, 'grad_norm': 10.834806442260742}
2022-12-29 17:03:10.923 DEBUG: Taking gradient step
2022-12-29 17:03:10.933 DEBUG: Loss 9: {'policy_loss': -0.01809456491771388, 'entropy_loss': -0.04061549622565508, 'vf_loss': 0.17946564159401368, 'total_loss': 0.12075558045064474, 'approx_kl': 0.035188671201467514, 'clip_fraction': 0.2630208358168602, 'grad_norm': 18.06374740600586}
2022-12-29 17:03:10.933 INFO: Optimization: policy loss=-0.018, vf loss=0.179, entropy loss=-0.041, total loss=0.121, num steps=10
2022-12-29 17:03:10.934 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:03:12.474 INFO: Evaluation rollout: return=0.380 (0.0), episode length=5.0
2022-12-29 17:03:12.475 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:03:12.477 INFO: Iteration: 3/137, steps: 648
2022-12-29 17:03:15.067 DEBUG: There is a single atom floating around
2022-12-29 17:03:16.029 DEBUG: There is a single atom floating around
2022-12-29 17:03:16.370 DEBUG: There is a single atom floating around
2022-12-29 17:03:16.373 DEBUG: There is a single atom floating around
2022-12-29 17:03:16.706 DEBUG: There is a single atom floating around
2022-12-29 17:03:21.126 DEBUG: Atoms are too close
2022-12-29 17:03:23.681 DEBUG: Atoms are too close
2022-12-29 17:03:23.683 DEBUG: Atoms are too close
2022-12-29 17:03:28.452 DEBUG: Atoms are too close
2022-12-29 17:03:31.355 DEBUG: Atoms are too close
2022-12-29 17:03:32.870 DEBUG: Atoms are too close
2022-12-29 17:03:33.608 DEBUG: Atoms are too close
2022-12-29 17:03:34.596 DEBUG: Atoms are too close
2022-12-29 17:03:35.684 DEBUG: Atoms are too close
2022-12-29 17:03:36.409 DEBUG: There is a single atom floating around
2022-12-29 17:03:37.236 DEBUG: Atoms are too close
2022-12-29 17:03:37.873 DEBUG: Atoms are too close
2022-12-29 17:03:39.038 DEBUG: Atoms are too close
2022-12-29 17:03:39.039 DEBUG: Atoms are too close
2022-12-29 17:03:43.093 DEBUG: Atoms are too close
2022-12-29 17:03:43.714 DEBUG: Atoms are too close
2022-12-29 17:03:44.012 DEBUG: Atoms are too close
2022-12-29 17:03:44.893 DEBUG: Atoms are too close
2022-12-29 17:03:48.326 DEBUG: Atoms are too close
2022-12-29 17:03:48.327 DEBUG: There is a single atom floating around
2022-12-29 17:03:49.785 DEBUG: Atoms are too close
2022-12-29 17:03:50.838 DEBUG: Atoms are too close
2022-12-29 17:03:51.148 DEBUG: Atoms are too close
2022-12-29 17:03:52.320 DEBUG: Atoms are too close
2022-12-29 17:03:52.321 DEBUG: There is a single atom floating around
2022-12-29 17:03:55.517 DEBUG: Atoms are too close
2022-12-29 17:03:57.548 DEBUG: Atoms are too close
2022-12-29 17:04:00.262 DEBUG: There is a single atom floating around
2022-12-29 17:04:01.346 INFO: Training rollout: return=-14.808 (8.7), episode length=4.2
2022-12-29 17:04:01.348 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:04:01.350 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-648_train.pkl
2022-12-29 17:04:02.187 DEBUG: Taking gradient step
2022-12-29 17:04:02.197 DEBUG: Loss 0: {'policy_loss': -0.011271781260081814, 'entropy_loss': -0.04324620962142944, 'vf_loss': 0.20129900878020807, 'total_loss': 0.1467810178986968, 'approx_kl': -2.4059154668520932e-09, 'clip_fraction': 0.0, 'grad_norm': 25.526031494140625}
2022-12-29 17:04:03.014 DEBUG: Taking gradient step
2022-12-29 17:04:03.024 DEBUG: Loss 1: {'policy_loss': -0.009114469276342407, 'entropy_loss': -0.04061381332576275, 'vf_loss': 0.20065096370083374, 'total_loss': 0.15092268109872856, 'approx_kl': 0.004848983429837972, 'clip_fraction': 0.0, 'grad_norm': 22.880109786987305}
2022-12-29 17:04:03.844 DEBUG: Taking gradient step
2022-12-29 17:04:03.853 DEBUG: Loss 2: {'policy_loss': -0.05340727757539933, 'entropy_loss': -0.04305758513510227, 'vf_loss': 0.19429289558223908, 'total_loss': 0.09782803287173748, 'approx_kl': 0.016769475303590298, 'clip_fraction': 0.1627604216337204, 'grad_norm': 13.841679573059082}
2022-12-29 17:04:04.658 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 17:04:04.659 INFO: Optimization: policy loss=-0.053, vf loss=0.194, entropy loss=-0.043, total loss=0.098, num steps=3
2022-12-29 17:04:04.659 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:04:06.134 INFO: Evaluation rollout: return=0.403 (0.0), episode length=5.0
2022-12-29 17:04:06.135 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:04:06.138 INFO: Iteration: 4/137, steps: 864
2022-12-29 17:04:10.678 DEBUG: There is a single atom floating around
2022-12-29 17:04:10.679 DEBUG: There is a single atom floating around
2022-12-29 17:04:15.529 DEBUG: Atoms are too close
2022-12-29 17:04:15.530 DEBUG: Atoms are too close
2022-12-29 17:04:16.461 DEBUG: Atoms are too close
2022-12-29 17:04:20.578 DEBUG: Atoms are too close
2022-12-29 17:04:26.494 DEBUG: There is a single atom floating around
2022-12-29 17:04:27.835 DEBUG: Atoms are too close
2022-12-29 17:04:29.754 DEBUG: Atoms are too close
2022-12-29 17:04:29.755 DEBUG: Atoms are too close
2022-12-29 17:04:30.121 DEBUG: Atoms are too close
2022-12-29 17:04:30.288 DEBUG: Atoms are too close
2022-12-29 17:04:30.959 DEBUG: Atoms are too close
2022-12-29 17:04:33.210 DEBUG: Atoms are too close
2022-12-29 17:04:33.211 DEBUG: Atoms are too close
2022-12-29 17:04:33.821 DEBUG: Atoms are too close
2022-12-29 17:04:36.140 DEBUG: There is a single atom floating around
2022-12-29 17:04:36.142 DEBUG: Atoms are too close
2022-12-29 17:04:39.966 DEBUG: Atoms are too close
2022-12-29 17:04:43.153 DEBUG: Atoms are too close
2022-12-29 17:04:43.389 DEBUG: Atoms are too close
2022-12-29 17:04:43.390 DEBUG: Atoms are too close
2022-12-29 17:04:44.541 DEBUG: Atoms are too close
2022-12-29 17:04:47.891 DEBUG: Atoms are too close
2022-12-29 17:04:50.872 DEBUG: Atoms are too close
2022-12-29 17:04:56.121 DEBUG: There is a single atom floating around
2022-12-29 17:04:58.409 DEBUG: Atoms are too close
2022-12-29 17:04:58.642 INFO: Training rollout: return=-12.629 (9.6), episode length=4.4
2022-12-29 17:04:58.643 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:04:58.646 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-864_train.pkl
2022-12-29 17:04:59.493 DEBUG: Taking gradient step
2022-12-29 17:04:59.505 DEBUG: Loss 0: {'policy_loss': 0.0020924635771369963, 'entropy_loss': -0.04219902027398348, 'vf_loss': 0.16714478218571038, 'total_loss': 0.12703822548886387, 'approx_kl': 3.507981816142092e-08, 'clip_fraction': 0.0, 'grad_norm': 23.681659698486328}
2022-12-29 17:05:00.310 DEBUG: Taking gradient step
2022-12-29 17:05:00.319 DEBUG: Loss 1: {'policy_loss': 0.010605611297785644, 'entropy_loss': -0.043489761650562286, 'vf_loss': 0.1718247393130845, 'total_loss': 0.13894058896030786, 'approx_kl': 0.010011517908424139, 'clip_fraction': 0.109375, 'grad_norm': 11.070252418518066}
2022-12-29 17:05:01.171 DEBUG: Taking gradient step
2022-12-29 17:05:01.185 DEBUG: Loss 2: {'policy_loss': -0.0357533489013509, 'entropy_loss': -0.043412002734839916, 'vf_loss': 0.16092579707332966, 'total_loss': 0.08176044543713885, 'approx_kl': 0.033627929631620646, 'clip_fraction': 0.2955729216337204, 'grad_norm': 12.716071128845215}
2022-12-29 17:05:02.003 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 17:05:02.003 INFO: Optimization: policy loss=-0.036, vf loss=0.161, entropy loss=-0.043, total loss=0.082, num steps=3
2022-12-29 17:05:02.004 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:05:03.445 INFO: Evaluation rollout: return=0.435 (0.0), episode length=5.0
2022-12-29 17:05:03.446 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:05:03.449 INFO: Iteration: 5/137, steps: 1080
2022-12-29 17:05:06.259 DEBUG: There is a single atom floating around
2022-12-29 17:05:13.151 DEBUG: Atoms are too close
2022-12-29 17:05:14.065 DEBUG: Atoms are too close
2022-12-29 17:05:17.033 DEBUG: Atoms are too close
2022-12-29 17:05:17.495 DEBUG: Atoms are too close
2022-12-29 17:05:17.495 DEBUG: Atoms are too close
2022-12-29 17:05:19.309 DEBUG: There is a single atom floating around
2022-12-29 17:05:23.157 DEBUG: There is a single atom floating around
2022-12-29 17:05:27.586 DEBUG: Atoms are too close
2022-12-29 17:05:28.181 DEBUG: Atoms are too close
2022-12-29 17:05:30.459 DEBUG: Atoms are too close
2022-12-29 17:05:31.635 DEBUG: Atoms are too close
2022-12-29 17:05:32.557 DEBUG: There is a single atom floating around
2022-12-29 17:05:34.829 DEBUG: Atoms are too close
2022-12-29 17:05:36.006 DEBUG: Atoms are too close
2022-12-29 17:05:36.294 DEBUG: Atoms are too close
2022-12-29 17:05:38.224 DEBUG: Atoms are too close
2022-12-29 17:05:45.803 DEBUG: Atoms are too close
2022-12-29 17:05:47.152 DEBUG: Atoms are too close
2022-12-29 17:05:47.455 DEBUG: Atoms are too close
2022-12-29 17:05:50.354 DEBUG: There is a single atom floating around
2022-12-29 17:05:56.634 INFO: Training rollout: return=-9.774 (10.0), episode length=4.5
2022-12-29 17:05:56.635 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:05:56.637 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-1080_train.pkl
2022-12-29 17:05:57.475 DEBUG: Taking gradient step
2022-12-29 17:05:57.485 DEBUG: Loss 0: {'policy_loss': -0.00662995930901881, 'entropy_loss': -0.04389016330242157, 'vf_loss': 0.12613653718988374, 'total_loss': 0.07561641457844334, 'approx_kl': -2.918144303976078e-08, 'clip_fraction': 0.0, 'grad_norm': 11.849790573120117}
2022-12-29 17:05:58.379 DEBUG: Taking gradient step
2022-12-29 17:05:58.388 DEBUG: Loss 1: {'policy_loss': 0.01723286401551946, 'entropy_loss': -0.04218996874988079, 'vf_loss': 0.1285600057765301, 'total_loss': 0.10360290104216878, 'approx_kl': 0.020366196520626545, 'clip_fraction': 0.08203125, 'grad_norm': 4.633587837219238}
2022-12-29 17:05:59.232 DEBUG: Taking gradient step
2022-12-29 17:05:59.242 DEBUG: Loss 2: {'policy_loss': -0.030546536919323547, 'entropy_loss': -0.04225216340273619, 'vf_loss': 0.12465207157752028, 'total_loss': 0.05185337125546053, 'approx_kl': 0.030147184850648046, 'clip_fraction': 0.14583333395421505, 'grad_norm': 10.929402351379395}
2022-12-29 17:06:00.086 DEBUG: Taking gradient step
2022-12-29 17:06:00.096 DEBUG: Loss 3: {'policy_loss': 0.005865698503302531, 'entropy_loss': -0.04185523372143507, 'vf_loss': 0.12444422596472034, 'total_loss': 0.0884546907465878, 'approx_kl': 0.039332753513008356, 'clip_fraction': 0.2057291679084301, 'grad_norm': 17.486764907836914}
2022-12-29 17:06:00.906 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 17:06:00.906 INFO: Optimization: policy loss=0.006, vf loss=0.124, entropy loss=-0.042, total loss=0.088, num steps=4
2022-12-29 17:06:00.907 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:06:02.429 INFO: Evaluation rollout: return=0.452 (0.0), episode length=5.0
2022-12-29 17:06:02.430 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:06:02.433 INFO: Iteration: 6/137, steps: 1296
2022-12-29 17:06:15.684 DEBUG: Atoms are too close
2022-12-29 17:06:15.685 DEBUG: Atoms are too close
2022-12-29 17:06:17.017 DEBUG: There is a single atom floating around
2022-12-29 17:06:17.019 DEBUG: Atoms are too close
2022-12-29 17:06:17.952 DEBUG: Atoms are too close
2022-12-29 17:06:29.032 DEBUG: Atoms are too close
2022-12-29 17:06:40.032 DEBUG: There is a single atom floating around
2022-12-29 17:06:54.228 DEBUG: There is a single atom floating around
2022-12-29 17:07:00.526 INFO: Training rollout: return=-3.879 (8.2), episode length=4.8
2022-12-29 17:07:00.528 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:07:00.530 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-1296_train.pkl
2022-12-29 17:07:01.421 DEBUG: Taking gradient step
2022-12-29 17:07:01.430 DEBUG: Loss 0: {'policy_loss': 0.06128197080863716, 'entropy_loss': -0.043794467113912106, 'vf_loss': 0.06319112237987098, 'total_loss': 0.08067862607459605, 'approx_kl': -2.5999421726652372e-08, 'clip_fraction': 0.0, 'grad_norm': 14.899065017700195}
2022-12-29 17:07:02.277 DEBUG: Taking gradient step
2022-12-29 17:07:02.287 DEBUG: Loss 1: {'policy_loss': -0.01893275610085627, 'entropy_loss': -0.04298782721161842, 'vf_loss': 0.055062968567407465, 'total_loss': -0.006857614745067225, 'approx_kl': 0.0010896035237237811, 'clip_fraction': 0.00390625, 'grad_norm': 15.22188663482666}
2022-12-29 17:07:03.106 DEBUG: Taking gradient step
2022-12-29 17:07:03.115 DEBUG: Loss 2: {'policy_loss': -0.047420963197993646, 'entropy_loss': -0.04438807349652052, 'vf_loss': 0.0563000679356364, 'total_loss': -0.03550896875887777, 'approx_kl': -0.003051947453059256, 'clip_fraction': 0.1197916679084301, 'grad_norm': 10.889655113220215}
2022-12-29 17:07:03.975 DEBUG: Taking gradient step
2022-12-29 17:07:03.990 DEBUG: Loss 3: {'policy_loss': -0.02940981260169391, 'entropy_loss': -0.04341038316488266, 'vf_loss': 0.059036982278683384, 'total_loss': -0.013783213487893187, 'approx_kl': 0.007956911460496485, 'clip_fraction': 0.2421875, 'grad_norm': 9.37687873840332}
2022-12-29 17:07:04.830 DEBUG: Taking gradient step
2022-12-29 17:07:04.840 DEBUG: Loss 4: {'policy_loss': 0.012788436210627169, 'entropy_loss': -0.04267513193190098, 'vf_loss': 0.063829506738467, 'total_loss': 0.03394281101719319, 'approx_kl': 0.01762748369947076, 'clip_fraction': 0.3190104216337204, 'grad_norm': 11.674694061279297}
2022-12-29 17:07:05.724 DEBUG: Taking gradient step
2022-12-29 17:07:05.738 DEBUG: Loss 5: {'policy_loss': -0.04950262894833603, 'entropy_loss': -0.041731931269168854, 'vf_loss': 0.058394179212546485, 'total_loss': -0.03284038100495839, 'approx_kl': 0.013704403769224882, 'clip_fraction': 0.37890625, 'grad_norm': 10.098088264465332}
2022-12-29 17:07:06.551 DEBUG: Taking gradient step
2022-12-29 17:07:06.560 DEBUG: Loss 6: {'policy_loss': 0.02114226186079793, 'entropy_loss': -0.041317389346659184, 'vf_loss': 0.0625703135510235, 'total_loss': 0.04239518606516224, 'approx_kl': 0.022031911183148623, 'clip_fraction': 0.39453125, 'grad_norm': 8.815705299377441}
2022-12-29 17:07:07.396 DEBUG: Taking gradient step
2022-12-29 17:07:07.406 DEBUG: Loss 7: {'policy_loss': -0.044831643700769364, 'entropy_loss': -0.041940271854400635, 'vf_loss': 0.056366332696843977, 'total_loss': -0.030405582858326022, 'approx_kl': 0.019367750734090805, 'clip_fraction': 0.3736979216337204, 'grad_norm': 8.513677597045898}
2022-12-29 17:07:08.214 DEBUG: Taking gradient step
2022-12-29 17:07:08.224 DEBUG: Loss 8: {'policy_loss': -0.010192547066571085, 'entropy_loss': -0.04124827682971954, 'vf_loss': 0.06100505753367973, 'total_loss': 0.009564233637389095, 'approx_kl': 0.006641973741352558, 'clip_fraction': 0.2838541679084301, 'grad_norm': 3.9073116779327393}
2022-12-29 17:07:09.077 DEBUG: Taking gradient step
2022-12-29 17:07:09.091 DEBUG: Loss 9: {'policy_loss': 0.005238769388010767, 'entropy_loss': -0.04103046376258135, 'vf_loss': 0.05937963632209681, 'total_loss': 0.023587941947526225, 'approx_kl': 0.011278631980530918, 'clip_fraction': 0.2018229179084301, 'grad_norm': 16.29631805419922}
2022-12-29 17:07:09.091 INFO: Optimization: policy loss=0.005, vf loss=0.059, entropy loss=-0.041, total loss=0.024, num steps=10
2022-12-29 17:07:09.091 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:07:10.654 INFO: Evaluation rollout: return=0.449 (0.0), episode length=5.0
2022-12-29 17:07:10.655 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:07:10.658 INFO: Iteration: 7/137, steps: 1512
2022-12-29 17:07:23.750 DEBUG: There is a single atom floating around
2022-12-29 17:07:24.899 DEBUG: Atoms are too close
2022-12-29 17:07:25.517 DEBUG: Atoms are too close
2022-12-29 17:07:35.967 DEBUG: Atoms are too close
2022-12-29 17:07:39.943 DEBUG: Atoms are too close
2022-12-29 17:07:39.945 DEBUG: Atoms are too close
2022-12-29 17:07:40.264 DEBUG: There is a single atom floating around
2022-12-29 17:07:40.885 DEBUG: Atoms are too close
2022-12-29 17:07:41.516 DEBUG: Atoms are too close
2022-12-29 17:07:44.865 DEBUG: There is a single atom floating around
2022-12-29 17:07:46.434 DEBUG: There is a single atom floating around
2022-12-29 17:07:55.356 DEBUG: Atoms are too close
2022-12-29 17:07:55.949 DEBUG: Atoms are too close
2022-12-29 17:07:55.951 DEBUG: There is a single atom floating around
2022-12-29 17:08:05.800 INFO: Training rollout: return=-7.049 (9.7), episode length=4.8
2022-12-29 17:08:05.802 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:08:05.804 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-1512_train.pkl
2022-12-29 17:08:06.645 DEBUG: Taking gradient step
2022-12-29 17:08:06.655 DEBUG: Loss 0: {'policy_loss': -0.0114143924972285, 'entropy_loss': -0.041687154211103916, 'vf_loss': 0.08747589435320084, 'total_loss': 0.034374347644868436, 'approx_kl': -1.393103321589706e-08, 'clip_fraction': 0.0, 'grad_norm': 15.029080390930176}
2022-12-29 17:08:07.488 DEBUG: Taking gradient step
2022-12-29 17:08:07.498 DEBUG: Loss 1: {'policy_loss': 0.012257940810652514, 'entropy_loss': -0.04327697306871414, 'vf_loss': 0.09639786389867502, 'total_loss': 0.0653788316406134, 'approx_kl': 0.0013500310597009957, 'clip_fraction': 0.0, 'grad_norm': 16.61268424987793}
2022-12-29 17:08:08.302 DEBUG: Taking gradient step
2022-12-29 17:08:08.312 DEBUG: Loss 2: {'policy_loss': 0.03213879785685607, 'entropy_loss': -0.0418940270319581, 'vf_loss': 0.09489329888248385, 'total_loss': 0.08513806970738182, 'approx_kl': 0.01261601666919887, 'clip_fraction': 0.0078125, 'grad_norm': 12.661831855773926}
2022-12-29 17:08:09.125 DEBUG: Taking gradient step
2022-12-29 17:08:09.134 DEBUG: Loss 3: {'policy_loss': -0.0537500195558115, 'entropy_loss': -0.042209901846945286, 'vf_loss': 0.08812132579932698, 'total_loss': -0.007838595603429796, 'approx_kl': 0.024613262619823217, 'clip_fraction': 0.12239583395421505, 'grad_norm': 13.36019515991211}
2022-12-29 17:08:09.965 DEBUG: Taking gradient step
2022-12-29 17:08:09.976 DEBUG: Loss 4: {'policy_loss': 0.03657538553610991, 'entropy_loss': -0.04212430026382208, 'vf_loss': 0.09831391941394328, 'total_loss': 0.09276500468623111, 'approx_kl': 0.03555057942867279, 'clip_fraction': 0.2018229216337204, 'grad_norm': 11.703780174255371}
2022-12-29 17:08:10.824 DEBUG: Taking gradient step
2022-12-29 17:08:10.840 DEBUG: Loss 5: {'policy_loss': -0.026348219647270504, 'entropy_loss': -0.04134436883032322, 'vf_loss': 0.08562145401006563, 'total_loss': 0.01792886553247193, 'approx_kl': 0.03515601111575961, 'clip_fraction': 0.20833333395421505, 'grad_norm': 12.08011245727539}
2022-12-29 17:08:11.654 DEBUG: Taking gradient step
2022-12-29 17:08:11.664 DEBUG: Loss 6: {'policy_loss': -0.017572108502001103, 'entropy_loss': -0.041677688248455524, 'vf_loss': 0.08737997022678859, 'total_loss': 0.02813017347633195, 'approx_kl': 0.03728885087184608, 'clip_fraction': 0.25390625, 'grad_norm': 9.514715194702148}
2022-12-29 17:08:12.495 DEBUG: Taking gradient step
2022-12-29 17:08:12.505 DEBUG: Loss 7: {'policy_loss': -0.016378130568528687, 'entropy_loss': -0.04217053763568401, 'vf_loss': 0.0892643152784914, 'total_loss': 0.030715647074278693, 'approx_kl': 0.03587947180494666, 'clip_fraction': 0.2291666716337204, 'grad_norm': 14.90731430053711}
2022-12-29 17:08:13.337 DEBUG: Taking gradient step
2022-12-29 17:08:13.348 DEBUG: Loss 8: {'policy_loss': -0.029227255070313825, 'entropy_loss': -0.0430041765794158, 'vf_loss': 0.08908161213145814, 'total_loss': 0.016850180481728507, 'approx_kl': 0.035763959400355816, 'clip_fraction': 0.15755208395421505, 'grad_norm': 9.277276039123535}
2022-12-29 17:08:14.216 DEBUG: Taking gradient step
2022-12-29 17:08:14.225 DEBUG: Loss 9: {'policy_loss': 0.00034991516859484914, 'entropy_loss': -0.04176343325525522, 'vf_loss': 0.09049930794474575, 'total_loss': 0.04908578985808538, 'approx_kl': 0.03567999787628651, 'clip_fraction': 0.1588541679084301, 'grad_norm': 6.340003967285156}
2022-12-29 17:08:14.226 INFO: Optimization: policy loss=0.000, vf loss=0.090, entropy loss=-0.042, total loss=0.049, num steps=10
2022-12-29 17:08:14.226 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:08:15.675 INFO: Evaluation rollout: return=0.462 (0.0), episode length=5.0
2022-12-29 17:08:15.676 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:08:15.678 INFO: Iteration: 8/137, steps: 1728
2022-12-29 17:08:19.432 DEBUG: There is a single atom floating around
2022-12-29 17:08:25.496 DEBUG: Atoms are too close
2022-12-29 17:08:28.681 DEBUG: Atoms are too close
2022-12-29 17:08:28.832 DEBUG: Atoms are too close
2022-12-29 17:08:30.782 DEBUG: Atoms are too close
2022-12-29 17:08:45.130 DEBUG: Atoms are too close
2022-12-29 17:08:46.687 DEBUG: There is a single atom floating around
2022-12-29 17:08:56.998 DEBUG: Atoms are too close
2022-12-29 17:09:02.475 DEBUG: Atoms are too close
2022-12-29 17:09:02.476 DEBUG: Atoms are too close
2022-12-29 17:09:07.031 DEBUG: There is a single atom floating around
2022-12-29 17:09:12.212 INFO: Training rollout: return=-5.628 (9.2), episode length=4.9
2022-12-29 17:09:12.213 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:09:12.216 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-1728_train.pkl
2022-12-29 17:09:13.025 DEBUG: Taking gradient step
2022-12-29 17:09:13.035 DEBUG: Loss 0: {'policy_loss': -0.0049882979802954985, 'entropy_loss': -0.04132089205086231, 'vf_loss': 0.076756026100044, 'total_loss': 0.030446836068886186, 'approx_kl': -1.3737007975578308e-08, 'clip_fraction': 0.0, 'grad_norm': 13.231864929199219}
2022-12-29 17:09:13.841 DEBUG: Taking gradient step
2022-12-29 17:09:13.850 DEBUG: Loss 1: {'policy_loss': -0.028986830968689276, 'entropy_loss': -0.041407533921301365, 'vf_loss': 0.0735080319498578, 'total_loss': 0.0031136670598671673, 'approx_kl': 0.0029890413861721754, 'clip_fraction': 0.06380208395421505, 'grad_norm': 8.234065055847168}
2022-12-29 17:09:14.678 DEBUG: Taking gradient step
2022-12-29 17:09:14.688 DEBUG: Loss 2: {'policy_loss': 0.004331925726309641, 'entropy_loss': -0.038553874008357525, 'vf_loss': 0.07190736855370326, 'total_loss': 0.03768542027165537, 'approx_kl': 0.009526097448542714, 'clip_fraction': 0.234375, 'grad_norm': 16.17997169494629}
2022-12-29 17:09:15.511 DEBUG: Taking gradient step
2022-12-29 17:09:15.522 DEBUG: Loss 3: {'policy_loss': -0.052882173308240396, 'entropy_loss': -0.04157715663313866, 'vf_loss': 0.07252744894542584, 'total_loss': -0.021931880995953217, 'approx_kl': 0.024934482527896762, 'clip_fraction': 0.3046875, 'grad_norm': 19.858278274536133}
2022-12-29 17:09:16.348 DEBUG: Taking gradient step
2022-12-29 17:09:16.358 DEBUG: Loss 4: {'policy_loss': -0.017400073444965403, 'entropy_loss': -0.03971799090504646, 'vf_loss': 0.07656317232339946, 'total_loss': 0.01944510797338758, 'approx_kl': 0.017872116528451443, 'clip_fraction': 0.2669270858168602, 'grad_norm': 11.307713508605957}
2022-12-29 17:09:17.164 DEBUG: Taking gradient step
2022-12-29 17:09:17.176 DEBUG: Loss 5: {'policy_loss': 0.004631608022433949, 'entropy_loss': -0.03839812334626913, 'vf_loss': 0.07562768290906327, 'total_loss': 0.04186116758522809, 'approx_kl': 0.03207273967564106, 'clip_fraction': 0.2369791716337204, 'grad_norm': 9.677400588989258}
2022-12-29 17:09:18.006 DEBUG: Taking gradient step
2022-12-29 17:09:18.017 DEBUG: Loss 6: {'policy_loss': 0.0014378633024360581, 'entropy_loss': -0.038749233819544315, 'vf_loss': 0.0752107283120134, 'total_loss': 0.03789935779490515, 'approx_kl': 0.017602890264242887, 'clip_fraction': 0.20833333395421505, 'grad_norm': 14.546062469482422}
2022-12-29 17:09:18.853 DEBUG: Taking gradient step
2022-12-29 17:09:18.862 DEBUG: Loss 7: {'policy_loss': 0.0027872520585090384, 'entropy_loss': -0.03834264352917671, 'vf_loss': 0.07843473333095308, 'total_loss': 0.042879341860285414, 'approx_kl': 0.012500849552452564, 'clip_fraction': 0.2591145858168602, 'grad_norm': 9.854741096496582}
2022-12-29 17:09:19.667 DEBUG: Taking gradient step
2022-12-29 17:09:19.676 DEBUG: Loss 8: {'policy_loss': -0.03562566918612006, 'entropy_loss': -0.038516923785209656, 'vf_loss': 0.07195352282742902, 'total_loss': -0.002189070143900698, 'approx_kl': 0.0048805170226842165, 'clip_fraction': 0.25, 'grad_norm': 12.372505187988281}
2022-12-29 17:09:20.545 DEBUG: Taking gradient step
2022-12-29 17:09:20.555 DEBUG: Loss 9: {'policy_loss': -0.03688692186047858, 'entropy_loss': -0.039795296266674995, 'vf_loss': 0.07472466273720475, 'total_loss': -0.001957555389948816, 'approx_kl': -0.00719764013774693, 'clip_fraction': 0.28125, 'grad_norm': 11.806936264038086}
2022-12-29 17:09:20.555 INFO: Optimization: policy loss=-0.037, vf loss=0.075, entropy loss=-0.040, total loss=-0.002, num steps=10
2022-12-29 17:09:20.555 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:09:22.119 INFO: Evaluation rollout: return=0.488 (0.0), episode length=5.0
2022-12-29 17:09:22.120 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:09:22.123 INFO: Iteration: 9/137, steps: 1944
2022-12-29 17:09:33.201 DEBUG: Atoms are too close
2022-12-29 17:09:35.429 DEBUG: There is a single atom floating around
2022-12-29 17:09:37.063 DEBUG: Atoms are too close
2022-12-29 17:09:37.671 DEBUG: There is a single atom floating around
2022-12-29 17:09:51.217 DEBUG: There is a single atom floating around
2022-12-29 17:09:51.219 DEBUG: Atoms are too close
2022-12-29 17:09:51.219 DEBUG: Atoms are too close
2022-12-29 17:09:53.515 DEBUG: Atoms are too close
2022-12-29 17:09:56.275 DEBUG: There is a single atom floating around
2022-12-29 17:10:05.060 DEBUG: Atoms are too close
2022-12-29 17:10:08.166 DEBUG: There is a single atom floating around
2022-12-29 17:10:08.589 DEBUG: Atoms are too close
2022-12-29 17:10:20.665 INFO: Training rollout: return=-6.178 (9.4), episode length=4.9
2022-12-29 17:10:20.667 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:10:20.669 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-1944_train.pkl
2022-12-29 17:10:21.541 DEBUG: Taking gradient step
2022-12-29 17:10:21.551 DEBUG: Loss 0: {'policy_loss': 0.02757767027763619, 'entropy_loss': -0.03853885270655155, 'vf_loss': 0.08430832311521594, 'total_loss': 0.07334714068630058, 'approx_kl': -6.092401871882203e-08, 'clip_fraction': 0.0, 'grad_norm': 16.53860855102539}
2022-12-29 17:10:22.392 DEBUG: Taking gradient step
2022-12-29 17:10:22.403 DEBUG: Loss 1: {'policy_loss': 0.0019169105715553109, 'entropy_loss': -0.038803732953965664, 'vf_loss': 0.08209310907308205, 'total_loss': 0.0452062866906717, 'approx_kl': 0.006557001237524673, 'clip_fraction': 0.03125, 'grad_norm': 15.962347030639648}
2022-12-29 17:10:23.358 DEBUG: Taking gradient step
2022-12-29 17:10:23.368 DEBUG: Loss 2: {'policy_loss': -0.05431106985704719, 'entropy_loss': -0.03941520024091005, 'vf_loss': 0.07767341960868984, 'total_loss': -0.016052850489267398, 'approx_kl': 0.031781766563653946, 'clip_fraction': 0.2096354216337204, 'grad_norm': 14.169445037841797}
2022-12-29 17:10:24.183 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 17:10:24.183 INFO: Optimization: policy loss=-0.054, vf loss=0.078, entropy loss=-0.039, total loss=-0.016, num steps=3
2022-12-29 17:10:24.184 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:10:25.613 INFO: Evaluation rollout: return=0.504 (0.0), episode length=5.0
2022-12-29 17:10:25.614 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:10:25.616 INFO: Iteration: 10/137, steps: 2160
2022-12-29 17:10:36.546 DEBUG: Atoms are too close
2022-12-29 17:10:39.425 DEBUG: Atoms are too close
2022-12-29 17:10:40.166 DEBUG: Atoms are too close
2022-12-29 17:10:40.475 DEBUG: Atoms are too close
2022-12-29 17:10:40.760 DEBUG: Atoms are too close
2022-12-29 17:10:55.004 DEBUG: Atoms are too close
2022-12-29 17:10:56.104 DEBUG: Atoms are too close
2022-12-29 17:11:10.802 DEBUG: Atoms are too close
2022-12-29 17:11:12.434 DEBUG: Atoms are too close
2022-12-29 17:11:23.332 INFO: Training rollout: return=-4.644 (8.7), episode length=5.0
2022-12-29 17:11:23.333 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:11:23.336 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-2160_train.pkl
2022-12-29 17:11:24.144 DEBUG: Taking gradient step
2022-12-29 17:11:24.153 DEBUG: Loss 0: {'policy_loss': 0.002547883025210444, 'entropy_loss': -0.04077035188674927, 'vf_loss': 0.06857619077269081, 'total_loss': 0.03035372191115198, 'approx_kl': -2.5494955480098724e-08, 'clip_fraction': 0.0, 'grad_norm': 20.10212516784668}
2022-12-29 17:11:24.954 DEBUG: Taking gradient step
2022-12-29 17:11:24.963 DEBUG: Loss 1: {'policy_loss': -0.01275252144188152, 'entropy_loss': -0.04032556526362896, 'vf_loss': 0.06906221798802611, 'total_loss': 0.015984131282515638, 'approx_kl': 0.015898958081379533, 'clip_fraction': 0.07682291697710752, 'grad_norm': 13.590845108032227}
2022-12-29 17:11:25.810 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 17:11:25.811 INFO: Optimization: policy loss=-0.013, vf loss=0.069, entropy loss=-0.040, total loss=0.016, num steps=2
2022-12-29 17:11:25.811 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:11:27.249 INFO: Evaluation rollout: return=0.506 (0.0), episode length=5.0
2022-12-29 17:11:27.251 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:11:27.253 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-216.model
2022-12-29 17:11:27.256 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-2376.model
2022-12-29 17:11:27.285 INFO: Iteration: 11/137, steps: 2376
2022-12-29 17:11:40.039 DEBUG: Atoms are too close
2022-12-29 17:11:42.311 DEBUG: Atoms are too close
2022-12-29 17:11:53.437 DEBUG: There is a single atom floating around
2022-12-29 17:11:58.242 DEBUG: Atoms are too close
2022-12-29 17:12:05.005 DEBUG: There is a single atom floating around
2022-12-29 17:12:09.417 DEBUG: Atoms are too close
2022-12-29 17:12:09.706 DEBUG: Atoms are too close
2022-12-29 17:12:12.576 DEBUG: Atoms are too close
2022-12-29 17:12:14.004 DEBUG: Atoms are too close
2022-12-29 17:12:14.316 DEBUG: There is a single atom floating around
2022-12-29 17:12:24.558 INFO: Training rollout: return=-5.034 (8.9), episode length=4.8
2022-12-29 17:12:24.560 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:12:24.563 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-2376_train.pkl
2022-12-29 17:12:25.410 DEBUG: Taking gradient step
2022-12-29 17:12:25.421 DEBUG: Loss 0: {'policy_loss': -0.023399222548522853, 'entropy_loss': -0.03887275233864784, 'vf_loss': 0.06750473250928114, 'total_loss': 0.005232757622110458, 'approx_kl': -8.036537835209856e-08, 'clip_fraction': 0.0, 'grad_norm': 10.754349708557129}
2022-12-29 17:12:26.256 DEBUG: Taking gradient step
2022-12-29 17:12:26.270 DEBUG: Loss 1: {'policy_loss': 0.027394065716311214, 'entropy_loss': -0.04011521860957146, 'vf_loss': 0.07281036289654881, 'total_loss': 0.06008921000328857, 'approx_kl': 0.0007042884826660156, 'clip_fraction': 0.0807291679084301, 'grad_norm': 8.246283531188965}
2022-12-29 17:12:27.122 DEBUG: Taking gradient step
2022-12-29 17:12:27.132 DEBUG: Loss 2: {'policy_loss': 0.057068116321317294, 'entropy_loss': -0.03796192817389965, 'vf_loss': 0.07127216313061799, 'total_loss': 0.09037835127803563, 'approx_kl': 0.008051906246691942, 'clip_fraction': 0.2291666716337204, 'grad_norm': 14.941644668579102}
2022-12-29 17:12:27.941 DEBUG: Taking gradient step
2022-12-29 17:12:27.951 DEBUG: Loss 3: {'policy_loss': 0.012237093532966883, 'entropy_loss': -0.03893282637000084, 'vf_loss': 0.0709535607544747, 'total_loss': 0.04425782791744075, 'approx_kl': 0.005210105096921325, 'clip_fraction': 0.3020833358168602, 'grad_norm': 17.239971160888672}
2022-12-29 17:12:28.764 DEBUG: Taking gradient step
2022-12-29 17:12:28.774 DEBUG: Loss 4: {'policy_loss': 0.011861542424699317, 'entropy_loss': -0.038115352392196655, 'vf_loss': 0.06866244242751136, 'total_loss': 0.042408632460014015, 'approx_kl': 0.010003259289078414, 'clip_fraction': 0.3098958358168602, 'grad_norm': 25.107479095458984}
2022-12-29 17:12:29.645 DEBUG: Taking gradient step
2022-12-29 17:12:29.655 DEBUG: Loss 5: {'policy_loss': 0.0005353116393500782, 'entropy_loss': -0.03771540895104408, 'vf_loss': 0.06998738996595415, 'total_loss': 0.03280729265426015, 'approx_kl': 0.01320090377703309, 'clip_fraction': 0.2591145858168602, 'grad_norm': 10.01016616821289}
2022-12-29 17:12:30.533 DEBUG: Taking gradient step
2022-12-29 17:12:30.547 DEBUG: Loss 6: {'policy_loss': 0.02663848585684442, 'entropy_loss': -0.03726340737193823, 'vf_loss': 0.07209494113454248, 'total_loss': 0.06147001961944869, 'approx_kl': 0.021676470525562763, 'clip_fraction': 0.1940104179084301, 'grad_norm': 9.667102813720703}
2022-12-29 17:12:31.498 DEBUG: Taking gradient step
2022-12-29 17:12:31.512 DEBUG: Loss 7: {'policy_loss': 0.04418727154637375, 'entropy_loss': -0.0369950020685792, 'vf_loss': 0.07614225430165118, 'total_loss': 0.08333452377944572, 'approx_kl': 0.019463742384687066, 'clip_fraction': 0.1666666679084301, 'grad_norm': 10.421231269836426}
2022-12-29 17:12:32.350 DEBUG: Taking gradient step
2022-12-29 17:12:32.361 DEBUG: Loss 8: {'policy_loss': -0.008821513298607411, 'entropy_loss': -0.037899065762758255, 'vf_loss': 0.06663315814015554, 'total_loss': 0.019912579078789863, 'approx_kl': 0.011957369861193001, 'clip_fraction': 0.140625, 'grad_norm': 14.361681938171387}
2022-12-29 17:12:33.199 DEBUG: Taking gradient step
2022-12-29 17:12:33.209 DEBUG: Loss 9: {'policy_loss': -0.00829376210498451, 'entropy_loss': -0.03799233678728342, 'vf_loss': 0.06945902639939881, 'total_loss': 0.023172927507130885, 'approx_kl': 0.007780666812323034, 'clip_fraction': 0.1419270858168602, 'grad_norm': 5.4998393058776855}
2022-12-29 17:12:33.209 INFO: Optimization: policy loss=-0.008, vf loss=0.069, entropy loss=-0.038, total loss=0.023, num steps=10
2022-12-29 17:12:33.209 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:12:34.708 INFO: Evaluation rollout: return=0.502 (0.0), episode length=5.0
2022-12-29 17:12:34.709 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:12:34.711 INFO: Iteration: 12/137, steps: 2592
2022-12-29 17:12:50.248 DEBUG: Atoms are too close
2022-12-29 17:13:01.593 DEBUG: Atoms are too close
2022-12-29 17:13:04.718 DEBUG: There is a single atom floating around
2022-12-29 17:13:04.885 DEBUG: Atoms are too close
2022-12-29 17:13:05.190 DEBUG: Atoms are too close
2022-12-29 17:13:05.500 DEBUG: Atoms are too close
2022-12-29 17:13:06.458 DEBUG: Atoms are too close
2022-12-29 17:13:20.087 DEBUG: Atoms are too close
2022-12-29 17:13:20.893 DEBUG: Atoms are too close
2022-12-29 17:13:32.623 INFO: Training rollout: return=-4.606 (8.7), episode length=5.0
2022-12-29 17:13:32.625 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:13:32.627 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-2592_train.pkl
2022-12-29 17:13:33.484 DEBUG: Taking gradient step
2022-12-29 17:13:33.497 DEBUG: Loss 0: {'policy_loss': 0.01270423609797534, 'entropy_loss': -0.037131293676793575, 'vf_loss': 0.06752849157958912, 'total_loss': 0.04310143400077089, 'approx_kl': -3.321717301218996e-08, 'clip_fraction': 0.0, 'grad_norm': 15.100804328918457}
2022-12-29 17:13:34.394 DEBUG: Taking gradient step
2022-12-29 17:13:34.403 DEBUG: Loss 1: {'policy_loss': 0.012274146651500433, 'entropy_loss': -0.03826953284442425, 'vf_loss': 0.06828500627638942, 'total_loss': 0.04228962008346561, 'approx_kl': 0.002520852372981608, 'clip_fraction': 0.0078125, 'grad_norm': 14.342620849609375}
2022-12-29 17:13:35.230 DEBUG: Taking gradient step
2022-12-29 17:13:35.240 DEBUG: Loss 2: {'policy_loss': 0.03370164378575059, 'entropy_loss': -0.03865018766373396, 'vf_loss': 0.07410275183716962, 'total_loss': 0.06915420795918625, 'approx_kl': 0.012289310805499554, 'clip_fraction': 0.08854166697710752, 'grad_norm': 5.215362548828125}
2022-12-29 17:13:36.110 DEBUG: Taking gradient step
2022-12-29 17:13:36.124 DEBUG: Loss 3: {'policy_loss': -0.020650479195698835, 'entropy_loss': -0.03866786137223244, 'vf_loss': 0.06524506388078084, 'total_loss': 0.005926723312849571, 'approx_kl': 0.021731883753091097, 'clip_fraction': 0.1966145858168602, 'grad_norm': 12.658465385437012}
2022-12-29 17:13:37.027 DEBUG: Taking gradient step
2022-12-29 17:13:37.041 DEBUG: Loss 4: {'policy_loss': 0.019984007452912865, 'entropy_loss': -0.03816122002899647, 'vf_loss': 0.06899911121764964, 'total_loss': 0.050821898641566034, 'approx_kl': 0.020501474224147387, 'clip_fraction': 0.17578125, 'grad_norm': 6.410202503204346}
2022-12-29 17:13:37.915 DEBUG: Taking gradient step
2022-12-29 17:13:37.924 DEBUG: Loss 5: {'policy_loss': -0.04946518220604852, 'entropy_loss': -0.03866108041256666, 'vf_loss': 0.06221741916929771, 'total_loss': -0.025908843449317473, 'approx_kl': 0.022508460329845548, 'clip_fraction': 0.1848958358168602, 'grad_norm': 4.733493804931641}
2022-12-29 17:13:38.758 DEBUG: Taking gradient step
2022-12-29 17:13:38.768 DEBUG: Loss 6: {'policy_loss': -0.00934753840777921, 'entropy_loss': -0.0383754288777709, 'vf_loss': 0.06729352680013301, 'total_loss': 0.01957055951458289, 'approx_kl': 0.028349770233035088, 'clip_fraction': 0.17578125, 'grad_norm': 7.1263933181762695}
2022-12-29 17:13:39.595 DEBUG: Taking gradient step
2022-12-29 17:13:39.604 DEBUG: Loss 7: {'policy_loss': 0.0033451429376791172, 'entropy_loss': -0.038502867333590984, 'vf_loss': 0.06418885643801708, 'total_loss': 0.02903113204210522, 'approx_kl': 0.028544648084789515, 'clip_fraction': 0.1861979179084301, 'grad_norm': 9.238395690917969}
2022-12-29 17:13:40.449 DEBUG: Taking gradient step
2022-12-29 17:13:40.458 DEBUG: Loss 8: {'policy_loss': 0.016291841036963045, 'entropy_loss': -0.038718780502676964, 'vf_loss': 0.06947150371297892, 'total_loss': 0.047044564247265, 'approx_kl': 0.03349699266254902, 'clip_fraction': 0.2421875, 'grad_norm': 9.931687355041504}
2022-12-29 17:13:41.313 DEBUG: Taking gradient step
2022-12-29 17:13:41.323 DEBUG: Loss 9: {'policy_loss': 0.023051387924284125, 'entropy_loss': -0.03897019661962986, 'vf_loss': 0.06989174379614407, 'total_loss': 0.05397293510079833, 'approx_kl': 0.03374115051701665, 'clip_fraction': 0.2486979179084301, 'grad_norm': 7.507719993591309}
2022-12-29 17:13:41.323 INFO: Optimization: policy loss=0.023, vf loss=0.070, entropy loss=-0.039, total loss=0.054, num steps=10
2022-12-29 17:13:41.323 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:13:42.859 INFO: Evaluation rollout: return=0.512 (0.0), episode length=5.0
2022-12-29 17:13:42.861 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:13:42.863 INFO: Iteration: 13/137, steps: 2808
2022-12-29 17:13:53.868 DEBUG: Atoms are too close
2022-12-29 17:13:55.824 DEBUG: There is a single atom floating around
2022-12-29 17:13:56.469 DEBUG: Atoms are too close
2022-12-29 17:13:58.193 DEBUG: Atoms are too close
2022-12-29 17:14:12.396 DEBUG: Atoms are too close
2022-12-29 17:14:13.818 DEBUG: There is a single atom floating around
2022-12-29 17:14:13.819 DEBUG: Atoms are too close
2022-12-29 17:14:27.020 DEBUG: Atoms are too close
2022-12-29 17:14:27.998 DEBUG: There is a single atom floating around
2022-12-29 17:14:28.751 DEBUG: Atoms are too close
2022-12-29 17:14:28.752 DEBUG: Atoms are too close
2022-12-29 17:14:39.692 INFO: Training rollout: return=-5.713 (9.3), episode length=4.9
2022-12-29 17:14:39.693 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:14:39.696 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-2808_train.pkl
2022-12-29 17:14:40.558 DEBUG: Taking gradient step
2022-12-29 17:14:40.567 DEBUG: Loss 0: {'policy_loss': 0.02707761334419224, 'entropy_loss': -0.037523094564676285, 'vf_loss': 0.07612357271603695, 'total_loss': 0.06567809149555291, 'approx_kl': 2.5883007737093067e-08, 'clip_fraction': 0.0, 'grad_norm': 21.8681583404541}
2022-12-29 17:14:41.427 DEBUG: Taking gradient step
2022-12-29 17:14:41.436 DEBUG: Loss 1: {'policy_loss': -0.008817664610966572, 'entropy_loss': -0.0372114684432745, 'vf_loss': 0.07571178673245646, 'total_loss': 0.029682653678215397, 'approx_kl': 0.0045013666385784745, 'clip_fraction': 0.05078125, 'grad_norm': 10.640433311462402}
2022-12-29 17:14:42.237 DEBUG: Taking gradient step
2022-12-29 17:14:42.246 DEBUG: Loss 2: {'policy_loss': 0.019859600790231153, 'entropy_loss': -0.038672213442623615, 'vf_loss': 0.0781318829019489, 'total_loss': 0.059319270249556445, 'approx_kl': 0.013782032998278737, 'clip_fraction': 0.11979166697710752, 'grad_norm': 6.434723377227783}
2022-12-29 17:14:43.049 DEBUG: Taking gradient step
2022-12-29 17:14:43.064 DEBUG: Loss 3: {'policy_loss': -0.06026999208273413, 'entropy_loss': -0.03865558188408613, 'vf_loss': 0.07174995386209254, 'total_loss': -0.027175620104727724, 'approx_kl': 0.02257690392434597, 'clip_fraction': 0.2044270858168602, 'grad_norm': 8.96240520477295}
2022-12-29 17:14:43.911 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 17:14:43.911 INFO: Optimization: policy loss=-0.060, vf loss=0.072, entropy loss=-0.039, total loss=-0.027, num steps=4
2022-12-29 17:14:43.911 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:14:45.445 INFO: Evaluation rollout: return=0.520 (0.0), episode length=5.0
2022-12-29 17:14:45.446 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:14:45.449 INFO: Iteration: 14/137, steps: 3024
2022-12-29 17:14:51.794 DEBUG: There is a single atom floating around
2022-12-29 17:15:17.924 DEBUG: Atoms are too close
2022-12-29 17:15:19.174 DEBUG: Atoms are too close
2022-12-29 17:15:36.865 DEBUG: There is a single atom floating around
2022-12-29 17:15:48.212 INFO: Training rollout: return=-1.735 (6.3), episode length=4.9
2022-12-29 17:15:48.214 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:15:48.217 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-3024_train.pkl
2022-12-29 17:15:52.305 DEBUG: Taking gradient step
2022-12-29 17:15:52.320 DEBUG: Loss 0: {'policy_loss': 0.019899930607028604, 'entropy_loss': -0.03850792720913887, 'vf_loss': 0.0395954306594926, 'total_loss': 0.020987434057382334, 'approx_kl': 4.190951585769653e-08, 'clip_fraction': 0.0, 'grad_norm': 17.832773208618164}
2022-12-29 17:15:56.568 DEBUG: Taking gradient step
2022-12-29 17:15:56.579 DEBUG: Loss 1: {'policy_loss': -0.043144451057750555, 'entropy_loss': -0.038136779330670834, 'vf_loss': 0.03804753049715368, 'total_loss': -0.04323369989126771, 'approx_kl': -0.007492600823752582, 'clip_fraction': 0.022135416977107525, 'grad_norm': 7.980342388153076}
2022-12-29 17:16:00.900 DEBUG: Taking gradient step
2022-12-29 17:16:00.910 DEBUG: Loss 2: {'policy_loss': 0.04147548451484404, 'entropy_loss': -0.037616562098264694, 'vf_loss': 0.04158691601231688, 'total_loss': 0.045445838428896226, 'approx_kl': -0.006783449789509177, 'clip_fraction': 0.1341145858168602, 'grad_norm': 6.757818222045898}
2022-12-29 17:16:04.842 DEBUG: Taking gradient step
2022-12-29 17:16:04.857 DEBUG: Loss 3: {'policy_loss': 0.07880506859845443, 'entropy_loss': -0.037296270951628685, 'vf_loss': 0.04326443567029275, 'total_loss': 0.0847732333171185, 'approx_kl': -0.002870380412787199, 'clip_fraction': 0.234375, 'grad_norm': 9.745769500732422}
2022-12-29 17:16:09.212 DEBUG: Taking gradient step
2022-12-29 17:16:09.227 DEBUG: Loss 4: {'policy_loss': 0.008153397758787071, 'entropy_loss': -0.038423013873398304, 'vf_loss': 0.03935576586190432, 'total_loss': 0.00908614974729309, 'approx_kl': 0.008942089742049575, 'clip_fraction': 0.2786458358168602, 'grad_norm': 11.862817764282227}
2022-12-29 17:16:13.341 DEBUG: Taking gradient step
2022-12-29 17:16:13.351 DEBUG: Loss 5: {'policy_loss': 0.04750325356636284, 'entropy_loss': -0.035164250526577234, 'vf_loss': 0.04144989837914478, 'total_loss': 0.05378890141893038, 'approx_kl': 0.01325000450015068, 'clip_fraction': 0.2591145858168602, 'grad_norm': 9.4387845993042}
2022-12-29 17:16:17.682 DEBUG: Taking gradient step
2022-12-29 17:16:17.691 DEBUG: Loss 6: {'policy_loss': -0.013075019831609108, 'entropy_loss': -0.03579251188784838, 'vf_loss': 0.036986012879009364, 'total_loss': -0.011881518840448123, 'approx_kl': 0.005625012796372175, 'clip_fraction': 0.2604166716337204, 'grad_norm': 8.967793464660645}
2022-12-29 17:16:22.040 DEBUG: Taking gradient step
2022-12-29 17:16:22.050 DEBUG: Loss 7: {'policy_loss': 0.007696116732375958, 'entropy_loss': -0.03581683058291674, 'vf_loss': 0.03519426546958866, 'total_loss': 0.007073551619047887, 'approx_kl': 0.015008364571258426, 'clip_fraction': 0.2252604179084301, 'grad_norm': 6.607303142547607}
2022-12-29 17:16:25.846 DEBUG: Taking gradient step
2022-12-29 17:16:25.857 DEBUG: Loss 8: {'policy_loss': -0.05559956514359948, 'entropy_loss': -0.03737191390246153, 'vf_loss': 0.03392832998382787, 'total_loss': -0.05904314906223313, 'approx_kl': 0.030604869592934847, 'clip_fraction': 0.1953125, 'grad_norm': 10.0078125}
2022-12-29 17:16:30.160 DEBUG: Taking gradient step
2022-12-29 17:16:30.170 DEBUG: Loss 9: {'policy_loss': -0.029268776040835706, 'entropy_loss': -0.03591085784137249, 'vf_loss': 0.03332601954817445, 'total_loss': -0.03185361433403375, 'approx_kl': 0.021186361846048385, 'clip_fraction': 0.21875, 'grad_norm': 14.784805297851562}
2022-12-29 17:16:30.170 INFO: Optimization: policy loss=-0.029, vf loss=0.033, entropy loss=-0.036, total loss=-0.032, num steps=10
2022-12-29 17:16:30.170 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:16:32.021 INFO: Evaluation rollout: return=0.514 (0.0), episode length=5.0
2022-12-29 17:16:32.022 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:16:32.025 INFO: Iteration: 15/137, steps: 3240
2022-12-29 17:16:46.433 DEBUG: Atoms are too close
2022-12-29 17:16:47.346 DEBUG: Atoms are too close
2022-12-29 17:16:47.663 DEBUG: Atoms are too close
2022-12-29 17:16:48.254 DEBUG: There is a single atom floating around
2022-12-29 17:16:58.450 DEBUG: There is a single atom floating around
2022-12-29 17:17:01.954 DEBUG: Atoms are too close
2022-12-29 17:17:02.303 DEBUG: There is a single atom floating around
2022-12-29 17:17:02.304 DEBUG: Atoms are too close
2022-12-29 17:17:18.306 DEBUG: Atoms are too close
2022-12-29 17:17:29.235 INFO: Training rollout: return=-4.586 (8.7), episode length=5.0
2022-12-29 17:17:29.237 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:17:29.239 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-3240_train.pkl
2022-12-29 17:17:30.065 DEBUG: Taking gradient step
2022-12-29 17:17:30.075 DEBUG: Loss 0: {'policy_loss': -0.030247411844445633, 'entropy_loss': -0.03466281620785594, 'vf_loss': 0.061341337455903575, 'total_loss': -0.0035688905963979917, 'approx_kl': 5.2619725465774536e-08, 'clip_fraction': 0.0, 'grad_norm': 13.612029075622559}
2022-12-29 17:17:30.910 DEBUG: Taking gradient step
2022-12-29 17:17:30.926 DEBUG: Loss 1: {'policy_loss': -0.032798075963743616, 'entropy_loss': -0.03595923958346248, 'vf_loss': 0.06314033797957319, 'total_loss': -0.00561697756763288, 'approx_kl': -0.002979450160637498, 'clip_fraction': 0.041666666977107525, 'grad_norm': 9.844592094421387}
2022-12-29 17:17:31.782 DEBUG: Taking gradient step
2022-12-29 17:17:31.792 DEBUG: Loss 2: {'policy_loss': 0.00043123670138438003, 'entropy_loss': -0.03485317714512348, 'vf_loss': 0.06449264809035911, 'total_loss': 0.03007070764662, 'approx_kl': 0.004881608998402953, 'clip_fraction': 0.14322916697710752, 'grad_norm': 11.47658634185791}
2022-12-29 17:17:32.610 DEBUG: Taking gradient step
2022-12-29 17:17:32.619 DEBUG: Loss 3: {'policy_loss': 0.010813458795939573, 'entropy_loss': -0.035244411788880825, 'vf_loss': 0.06438709569888983, 'total_loss': 0.03995614270594858, 'approx_kl': 0.008759221178479493, 'clip_fraction': 0.22005208395421505, 'grad_norm': 18.853031158447266}
2022-12-29 17:17:33.471 DEBUG: Taking gradient step
2022-12-29 17:17:33.482 DEBUG: Loss 4: {'policy_loss': -0.03781932673958614, 'entropy_loss': -0.03459838405251503, 'vf_loss': 0.06129901785397875, 'total_loss': -0.01111869293812242, 'approx_kl': 0.0073555526323616505, 'clip_fraction': 0.2838541716337204, 'grad_norm': 13.76175308227539}
2022-12-29 17:17:34.321 DEBUG: Taking gradient step
2022-12-29 17:17:34.331 DEBUG: Loss 5: {'policy_loss': -0.028438380291433897, 'entropy_loss': -0.03429812518879771, 'vf_loss': 0.06061464469408009, 'total_loss': -0.002121860786151522, 'approx_kl': 0.006005620118230581, 'clip_fraction': 0.2877604216337204, 'grad_norm': 10.551344871520996}
2022-12-29 17:17:35.197 DEBUG: Taking gradient step
2022-12-29 17:17:35.206 DEBUG: Loss 6: {'policy_loss': 0.013310292055626657, 'entropy_loss': -0.034409355372190475, 'vf_loss': 0.06684893519460874, 'total_loss': 0.04574987187804492, 'approx_kl': 0.011718409601598978, 'clip_fraction': 0.2473958358168602, 'grad_norm': 13.765018463134766}
2022-12-29 17:17:36.078 DEBUG: Taking gradient step
2022-12-29 17:17:36.088 DEBUG: Loss 7: {'policy_loss': -0.02682131352805762, 'entropy_loss': -0.03515791893005371, 'vf_loss': 0.06535750669065424, 'total_loss': 0.0033782742325428987, 'approx_kl': 0.014839892275631428, 'clip_fraction': 0.2083333358168602, 'grad_norm': 5.342951774597168}
2022-12-29 17:17:36.946 DEBUG: Taking gradient step
2022-12-29 17:17:36.955 DEBUG: Loss 8: {'policy_loss': -0.04567717055663752, 'entropy_loss': -0.035465750843286514, 'vf_loss': 0.06113920266193976, 'total_loss': -0.02000371873798426, 'approx_kl': 0.013102719793096185, 'clip_fraction': 0.2044270858168602, 'grad_norm': 10.56495475769043}
2022-12-29 17:17:37.792 DEBUG: Taking gradient step
2022-12-29 17:17:37.802 DEBUG: Loss 9: {'policy_loss': 0.014836142477582144, 'entropy_loss': -0.03296065656468272, 'vf_loss': 0.06912638231929501, 'total_loss': 0.051001868232194436, 'approx_kl': 0.002799005014821887, 'clip_fraction': 0.2057291679084301, 'grad_norm': 9.065072059631348}
2022-12-29 17:17:37.802 INFO: Optimization: policy loss=0.015, vf loss=0.069, entropy loss=-0.033, total loss=0.051, num steps=10
2022-12-29 17:17:37.803 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:17:39.319 INFO: Evaluation rollout: return=0.502 (0.0), episode length=5.0
2022-12-29 17:17:39.320 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:17:39.322 INFO: Iteration: 16/137, steps: 3456
2022-12-29 17:18:09.684 DEBUG: There is a single atom floating around
2022-12-29 17:18:12.193 DEBUG: Atoms are too close
2022-12-29 17:18:29.142 DEBUG: There is a single atom floating around
2022-12-29 17:18:32.357 DEBUG: There is a single atom floating around
2022-12-29 17:18:39.258 INFO: Training rollout: return=-1.732 (6.3), episode length=4.9
2022-12-29 17:18:39.259 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:18:39.262 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-3456_train.pkl
2022-12-29 17:18:42.301 DEBUG: Taking gradient step
2022-12-29 17:18:42.311 DEBUG: Loss 0: {'policy_loss': -0.022776058988797962, 'entropy_loss': -0.033910430036485195, 'vf_loss': 0.029841405637346607, 'total_loss': -0.02684508338793655, 'approx_kl': -8.449812227695475e-08, 'clip_fraction': 0.0, 'grad_norm': 18.64606475830078}
2022-12-29 17:18:45.727 DEBUG: Taking gradient step
2022-12-29 17:18:45.737 DEBUG: Loss 1: {'policy_loss': -0.003872333180869945, 'entropy_loss': -0.03457287233322859, 'vf_loss': 0.03139848034741939, 'total_loss': -0.007046725166679142, 'approx_kl': 0.011515422724187374, 'clip_fraction': 0.057291666977107525, 'grad_norm': 6.142034530639648}
2022-12-29 17:18:49.167 DEBUG: Taking gradient step
2022-12-29 17:18:49.178 DEBUG: Loss 2: {'policy_loss': -0.020151802988659345, 'entropy_loss': -0.03383374400436878, 'vf_loss': 0.03006728825070193, 'total_loss': -0.0239182587423262, 'approx_kl': 0.029186795465648174, 'clip_fraction': 0.09765625, 'grad_norm': 8.798966407775879}
2022-12-29 17:18:52.503 DEBUG: Taking gradient step
2022-12-29 17:18:52.513 DEBUG: Loss 3: {'policy_loss': 0.0599794108647446, 'entropy_loss': -0.03410482173785567, 'vf_loss': 0.037173112024214874, 'total_loss': 0.06304770115110381, 'approx_kl': 0.02870985446497798, 'clip_fraction': 0.1315104179084301, 'grad_norm': 10.43974494934082}
2022-12-29 17:18:55.972 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 17:18:55.972 INFO: Optimization: policy loss=0.060, vf loss=0.037, entropy loss=-0.034, total loss=0.063, num steps=4
2022-12-29 17:18:55.973 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:18:57.731 INFO: Evaluation rollout: return=0.506 (0.0), episode length=5.0
2022-12-29 17:18:57.733 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:18:57.736 INFO: Iteration: 17/137, steps: 3672
2022-12-29 17:19:14.002 DEBUG: Atoms are too close
2022-12-29 17:19:42.185 DEBUG: Atoms are too close
2022-12-29 17:19:45.877 DEBUG: Atoms are too close
2022-12-29 17:19:47.327 DEBUG: Atoms are too close
2022-12-29 17:19:57.161 INFO: Training rollout: return=-1.789 (6.3), episode length=5.0
2022-12-29 17:19:57.162 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:19:57.164 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-3672_train.pkl
2022-12-29 17:19:58.006 DEBUG: Taking gradient step
2022-12-29 17:19:58.016 DEBUG: Loss 0: {'policy_loss': -0.05633889907145411, 'entropy_loss': -0.033367318101227283, 'vf_loss': 0.029926311171848956, 'total_loss': -0.05977990600083243, 'approx_kl': -1.4745937804150344e-09, 'clip_fraction': 0.0, 'grad_norm': 13.25895881652832}
2022-12-29 17:19:58.872 DEBUG: Taking gradient step
2022-12-29 17:19:58.881 DEBUG: Loss 1: {'policy_loss': 0.03983993277665119, 'entropy_loss': -0.032795543782413006, 'vf_loss': 0.036921502211283416, 'total_loss': 0.0439658912055216, 'approx_kl': -0.0003647400881163776, 'clip_fraction': 0.0078125, 'grad_norm': 23.04463005065918}
2022-12-29 17:19:59.719 DEBUG: Taking gradient step
2022-12-29 17:19:59.732 DEBUG: Loss 2: {'policy_loss': 0.05152291334970144, 'entropy_loss': -0.032141890842467546, 'vf_loss': 0.0382637612402251, 'total_loss': 0.05764478374745899, 'approx_kl': -0.007090519764460623, 'clip_fraction': 0.07161458395421505, 'grad_norm': 18.19271469116211}
2022-12-29 17:20:00.608 DEBUG: Taking gradient step
2022-12-29 17:20:00.618 DEBUG: Loss 3: {'policy_loss': 0.048605669890065345, 'entropy_loss': -0.03244438860565424, 'vf_loss': 0.03879109971640221, 'total_loss': 0.054952381000813316, 'approx_kl': -0.008363510482013226, 'clip_fraction': 0.2408854216337204, 'grad_norm': 13.757546424865723}
2022-12-29 17:20:01.425 DEBUG: Taking gradient step
2022-12-29 17:20:01.435 DEBUG: Loss 4: {'policy_loss': -0.027904397272854535, 'entropy_loss': -0.03112983563914895, 'vf_loss': 0.033551959518090146, 'total_loss': -0.025482273393913336, 'approx_kl': -0.005949610844254494, 'clip_fraction': 0.2981770858168602, 'grad_norm': 10.087353706359863}
2022-12-29 17:20:02.242 DEBUG: Taking gradient step
2022-12-29 17:20:02.252 DEBUG: Loss 5: {'policy_loss': -0.01007429092758862, 'entropy_loss': -0.031296778935939074, 'vf_loss': 0.03479504748379965, 'total_loss': -0.00657602237972804, 'approx_kl': -0.006179793504998088, 'clip_fraction': 0.3098958358168602, 'grad_norm': 6.289260387420654}
2022-12-29 17:20:03.102 DEBUG: Taking gradient step
2022-12-29 17:20:03.112 DEBUG: Loss 6: {'policy_loss': 0.019737041645409667, 'entropy_loss': -0.0319810975342989, 'vf_loss': 0.036264574127530175, 'total_loss': 0.024020518238640945, 'approx_kl': 0.005397661589086056, 'clip_fraction': 0.30078125, 'grad_norm': 6.3411030769348145}
2022-12-29 17:20:03.926 DEBUG: Taking gradient step
2022-12-29 17:20:03.935 DEBUG: Loss 7: {'policy_loss': -0.03584298560563618, 'entropy_loss': -0.031581811606884, 'vf_loss': 0.03287897047738583, 'total_loss': -0.03454582673513435, 'approx_kl': 0.02035976480692625, 'clip_fraction': 0.3658854216337204, 'grad_norm': 4.979977130889893}
2022-12-29 17:20:04.775 DEBUG: Taking gradient step
2022-12-29 17:20:04.786 DEBUG: Loss 8: {'policy_loss': -0.022942625506672875, 'entropy_loss': -0.03331191372126341, 'vf_loss': 0.03265362657766977, 'total_loss': -0.023600912650266516, 'approx_kl': 0.0368907805532217, 'clip_fraction': 0.4752604216337204, 'grad_norm': 6.425941467285156}
2022-12-29 17:20:05.631 DEBUG: Taking gradient step
2022-12-29 17:20:05.641 DEBUG: Loss 9: {'policy_loss': -0.02278847469534658, 'entropy_loss': -0.03299437928944826, 'vf_loss': 0.032699949233490344, 'total_loss': -0.023082904751304496, 'approx_kl': 0.03578780731186271, 'clip_fraction': 0.4778645858168602, 'grad_norm': 6.960534572601318}
2022-12-29 17:20:05.641 INFO: Optimization: policy loss=-0.023, vf loss=0.033, entropy loss=-0.033, total loss=-0.023, num steps=10
2022-12-29 17:20:05.642 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:20:07.131 INFO: Evaluation rollout: return=0.503 (0.0), episode length=5.0
2022-12-29 17:20:07.132 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:20:07.134 INFO: Iteration: 18/137, steps: 3888
2022-12-29 17:20:22.228 DEBUG: Atoms are too close
2022-12-29 17:20:39.584 DEBUG: Atoms are too close
2022-12-29 17:20:41.553 DEBUG: Atoms are too close
2022-12-29 17:20:57.500 DEBUG: Atoms are too close
2022-12-29 17:21:10.404 INFO: Training rollout: return=-1.780 (6.3), episode length=5.0
2022-12-29 17:21:10.405 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:21:10.408 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-3888_train.pkl
2022-12-29 17:21:15.027 DEBUG: Taking gradient step
2022-12-29 17:21:15.037 DEBUG: Loss 0: {'policy_loss': -0.02921227562909844, 'entropy_loss': -0.03236433211714029, 'vf_loss': 0.03186670735997095, 'total_loss': -0.02970990038626778, 'approx_kl': -4.346173199110126e-09, 'clip_fraction': 0.0, 'grad_norm': 16.02174186706543}
2022-12-29 17:21:18.752 DEBUG: Taking gradient step
2022-12-29 17:21:18.762 DEBUG: Loss 1: {'policy_loss': -0.03054289093605017, 'entropy_loss': -0.03417197009548545, 'vf_loss': 0.03141157207735527, 'total_loss': -0.033303288954180346, 'approx_kl': 0.000155830814037472, 'clip_fraction': 0.0, 'grad_norm': 7.096643447875977}
2022-12-29 17:21:23.325 DEBUG: Taking gradient step
2022-12-29 17:21:23.335 DEBUG: Loss 2: {'policy_loss': -0.019426164721392154, 'entropy_loss': -0.034929110668599606, 'vf_loss': 0.03343238101705657, 'total_loss': -0.02092289437293519, 'approx_kl': 0.0020122838905081153, 'clip_fraction': 0.033854166977107525, 'grad_norm': 8.807888984680176}
2022-12-29 17:21:27.873 DEBUG: Taking gradient step
2022-12-29 17:21:27.883 DEBUG: Loss 3: {'policy_loss': -0.043901286236793166, 'entropy_loss': -0.03558521717786789, 'vf_loss': 0.03151509369411562, 'total_loss': -0.047971409720545435, 'approx_kl': 0.013212988851591945, 'clip_fraction': 0.12630208395421505, 'grad_norm': 7.929097652435303}
2022-12-29 17:21:31.771 DEBUG: Taking gradient step
2022-12-29 17:21:31.785 DEBUG: Loss 4: {'policy_loss': -0.04945791846766262, 'entropy_loss': -0.035167443100363016, 'vf_loss': 0.031492013362082114, 'total_loss': -0.05313334820594352, 'approx_kl': 0.015263935551047325, 'clip_fraction': 0.2552083358168602, 'grad_norm': 10.575082778930664}
2022-12-29 17:21:36.385 DEBUG: Taking gradient step
2022-12-29 17:21:36.394 DEBUG: Loss 5: {'policy_loss': -0.05170195242820266, 'entropy_loss': -0.03778340481221676, 'vf_loss': 0.03137399171916126, 'total_loss': -0.058111365521258146, 'approx_kl': 0.02091458090581, 'clip_fraction': 0.3528645858168602, 'grad_norm': 6.216089725494385}
2022-12-29 17:21:40.745 DEBUG: Early stopping at step 6 for reaching max KL.
2022-12-29 17:21:40.745 INFO: Optimization: policy loss=-0.052, vf loss=0.031, entropy loss=-0.038, total loss=-0.058, num steps=6
2022-12-29 17:21:40.746 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:21:42.583 INFO: Evaluation rollout: return=0.514 (0.0), episode length=5.0
2022-12-29 17:21:42.585 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:21:42.587 INFO: Iteration: 19/137, steps: 4104
2022-12-29 17:21:59.000 DEBUG: Atoms are too close
2022-12-29 17:22:15.514 DEBUG: Atoms are too close
2022-12-29 17:22:42.412 INFO: Training rollout: return=-0.696 (4.6), episode length=5.0
2022-12-29 17:22:42.424 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:22:42.427 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-4104_train.pkl
2022-12-29 17:22:43.285 DEBUG: Taking gradient step
2022-12-29 17:22:43.294 DEBUG: Loss 0: {'policy_loss': -0.00856742483395372, 'entropy_loss': -0.03797557996585965, 'vf_loss': 0.019537942292320507, 'total_loss': -0.027005062507492864, 'approx_kl': -1.0632600577764606e-08, 'clip_fraction': 0.0, 'grad_norm': 7.3694353103637695}
2022-12-29 17:22:44.086 DEBUG: Taking gradient step
2022-12-29 17:22:44.096 DEBUG: Loss 1: {'policy_loss': -0.03986298188267464, 'entropy_loss': -0.0377199649810791, 'vf_loss': 0.01746268370458256, 'total_loss': -0.06012026315917118, 'approx_kl': 0.019687433261424303, 'clip_fraction': 0.0716145858168602, 'grad_norm': 6.660972595214844}
2022-12-29 17:22:44.901 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 17:22:44.902 INFO: Optimization: policy loss=-0.040, vf loss=0.017, entropy loss=-0.038, total loss=-0.060, num steps=2
2022-12-29 17:22:44.902 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:22:46.379 INFO: Evaluation rollout: return=0.501 (0.0), episode length=5.0
2022-12-29 17:22:46.380 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:22:46.383 INFO: Iteration: 20/137, steps: 4320
2022-12-29 17:23:19.854 DEBUG: There is a single atom floating around
2022-12-29 17:23:32.529 DEBUG: There is a single atom floating around
2022-12-29 17:23:34.949 DEBUG: Atoms are too close
2022-12-29 17:23:36.046 DEBUG: Atoms are too close
2022-12-29 17:23:47.096 INFO: Training rollout: return=-1.809 (6.3), episode length=5.0
2022-12-29 17:23:47.098 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:23:47.100 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-4320_train.pkl
2022-12-29 17:23:51.124 DEBUG: Taking gradient step
2022-12-29 17:23:51.136 DEBUG: Loss 0: {'policy_loss': -0.00023054769990402818, 'entropy_loss': -0.03852961678057909, 'vf_loss': 0.03168016023576934, 'total_loss': -0.0070800042447137805, 'approx_kl': 9.167706593871117e-09, 'clip_fraction': 0.0, 'grad_norm': 20.682674407958984}
2022-12-29 17:23:55.382 DEBUG: Taking gradient step
2022-12-29 17:23:55.396 DEBUG: Loss 1: {'policy_loss': -0.013768470671281294, 'entropy_loss': -0.03886287286877632, 'vf_loss': 0.03160771050087656, 'total_loss': -0.02102363303918106, 'approx_kl': 0.0053110625594854355, 'clip_fraction': 0.0703125, 'grad_norm': 16.38008689880371}
2022-12-29 17:23:59.664 DEBUG: Taking gradient step
2022-12-29 17:23:59.674 DEBUG: Loss 2: {'policy_loss': -0.01756481997689069, 'entropy_loss': -0.03911924781277776, 'vf_loss': 0.03167843957570487, 'total_loss': -0.02500562821396358, 'approx_kl': 0.01124146452639252, 'clip_fraction': 0.2434895858168602, 'grad_norm': 9.70695972442627}
2022-12-29 17:24:03.396 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 17:24:03.397 INFO: Optimization: policy loss=-0.018, vf loss=0.032, entropy loss=-0.039, total loss=-0.025, num steps=3
2022-12-29 17:24:03.397 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:24:05.209 INFO: Evaluation rollout: return=0.479 (0.0), episode length=5.0
2022-12-29 17:24:05.210 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:24:05.212 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-2376.model
2022-12-29 17:24:05.217 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-4536.model
2022-12-29 17:24:05.248 INFO: Iteration: 21/137, steps: 4536
2022-12-29 17:24:21.514 DEBUG: Atoms are too close
2022-12-29 17:24:22.467 DEBUG: Atoms are too close
2022-12-29 17:24:37.184 DEBUG: There is a single atom floating around
2022-12-29 17:24:37.185 DEBUG: There is a single atom floating around
2022-12-29 17:24:39.288 DEBUG: Atoms are too close
2022-12-29 17:24:53.063 DEBUG: Atoms are too close
2022-12-29 17:24:54.241 DEBUG: Atoms are too close
2022-12-29 17:25:04.868 INFO: Training rollout: return=-3.531 (8.0), episode length=5.0
2022-12-29 17:25:04.869 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:25:04.872 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-4536_train.pkl
2022-12-29 17:25:05.742 DEBUG: Taking gradient step
2022-12-29 17:25:05.752 DEBUG: Loss 0: {'policy_loss': -0.011144001946302468, 'entropy_loss': -0.0388619638979435, 'vf_loss': 0.05286049590930492, 'total_loss': 0.0028545300650589557, 'approx_kl': -8.024896303027163e-08, 'clip_fraction': 0.0, 'grad_norm': 14.400409698486328}
2022-12-29 17:25:06.562 DEBUG: Taking gradient step
2022-12-29 17:25:06.572 DEBUG: Loss 1: {'policy_loss': -0.01069601231750858, 'entropy_loss': -0.039695615880191326, 'vf_loss': 0.053221041472365255, 'total_loss': 0.0028294132746653523, 'approx_kl': -0.0014955326332710683, 'clip_fraction': 0.0, 'grad_norm': 12.432710647583008}
2022-12-29 17:25:07.402 DEBUG: Taking gradient step
2022-12-29 17:25:07.412 DEBUG: Loss 2: {'policy_loss': -0.04809331204692442, 'entropy_loss': -0.03953251428902149, 'vf_loss': 0.04915907698540449, 'total_loss': -0.03846674935054142, 'approx_kl': 0.004713383212219924, 'clip_fraction': 0.0234375, 'grad_norm': 10.281227111816406}
2022-12-29 17:25:08.230 DEBUG: Taking gradient step
2022-12-29 17:25:08.240 DEBUG: Loss 3: {'policy_loss': 0.006605525495224637, 'entropy_loss': -0.03740904154255986, 'vf_loss': 0.05729234904872146, 'total_loss': 0.026488833001386238, 'approx_kl': 0.016146584413945675, 'clip_fraction': 0.10677083395421505, 'grad_norm': 8.670145988464355}
2022-12-29 17:25:09.131 DEBUG: Taking gradient step
2022-12-29 17:25:09.147 DEBUG: Loss 4: {'policy_loss': 0.029472058723027023, 'entropy_loss': -0.03712773276492953, 'vf_loss': 0.05976085409918035, 'total_loss': 0.05210518005727784, 'approx_kl': 0.023682543076574802, 'clip_fraction': 0.2096354179084301, 'grad_norm': 11.03818416595459}
2022-12-29 17:25:09.988 DEBUG: Taking gradient step
2022-12-29 17:25:09.999 DEBUG: Loss 5: {'policy_loss': -0.022633970977148827, 'entropy_loss': -0.03838988393545151, 'vf_loss': 0.053426548558666395, 'total_loss': -0.007597306353933936, 'approx_kl': 0.039111436577513814, 'clip_fraction': 0.265625, 'grad_norm': 8.883512496948242}
2022-12-29 17:25:10.794 DEBUG: Taking gradient step
2022-12-29 17:25:10.804 DEBUG: Loss 6: {'policy_loss': -0.011119246330707929, 'entropy_loss': -0.0400918610394001, 'vf_loss': 0.05547799448846573, 'total_loss': 0.004266887118357704, 'approx_kl': 0.029368882533162832, 'clip_fraction': 0.2838541716337204, 'grad_norm': 9.554000854492188}
2022-12-29 17:25:11.614 DEBUG: Taking gradient step
2022-12-29 17:25:11.624 DEBUG: Loss 7: {'policy_loss': 0.056619361899633625, 'entropy_loss': -0.03909885976463556, 'vf_loss': 0.06145207926257343, 'total_loss': 0.07897258139757149, 'approx_kl': 0.02949374169111252, 'clip_fraction': 0.2604166716337204, 'grad_norm': 8.60934829711914}
2022-12-29 17:25:12.489 DEBUG: Taking gradient step
2022-12-29 17:25:12.501 DEBUG: Loss 8: {'policy_loss': -0.060391124681796575, 'entropy_loss': -0.0377538688480854, 'vf_loss': 0.049113577677875546, 'total_loss': -0.049031415852006426, 'approx_kl': 0.03004777047317475, 'clip_fraction': 0.2734375, 'grad_norm': 9.581045150756836}
2022-12-29 17:25:13.327 DEBUG: Taking gradient step
2022-12-29 17:25:13.342 DEBUG: Loss 9: {'policy_loss': 0.0049380037590282255, 'entropy_loss': -0.03904348332434893, 'vf_loss': 0.05711186174484702, 'total_loss': 0.02300638217952631, 'approx_kl': 0.026791520416736603, 'clip_fraction': 0.265625, 'grad_norm': 8.157899856567383}
2022-12-29 17:25:13.342 INFO: Optimization: policy loss=0.005, vf loss=0.057, entropy loss=-0.039, total loss=0.023, num steps=10
2022-12-29 17:25:13.342 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:25:14.836 INFO: Evaluation rollout: return=0.483 (0.0), episode length=5.0
2022-12-29 17:25:14.837 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:25:14.840 INFO: Iteration: 22/137, steps: 4752
2022-12-29 17:25:28.985 DEBUG: Atoms are too close
2022-12-29 17:25:41.245 DEBUG: Atoms are too close
2022-12-29 17:25:44.043 DEBUG: Atoms are too close
2022-12-29 17:25:44.845 DEBUG: Atoms are too close
2022-12-29 17:25:45.486 DEBUG: Atoms are too close
2022-12-29 17:26:00.022 DEBUG: Atoms are too close
2022-12-29 17:26:00.782 DEBUG: Atoms are too close
2022-12-29 17:26:01.403 DEBUG: There is a single atom floating around
2022-12-29 17:26:11.974 INFO: Training rollout: return=-4.051 (8.4), episode length=5.0
2022-12-29 17:26:11.975 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:26:11.978 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-4752_train.pkl
2022-12-29 17:26:12.813 DEBUG: Taking gradient step
2022-12-29 17:26:12.822 DEBUG: Loss 0: {'policy_loss': -0.019033904138245124, 'entropy_loss': -0.03818825539201498, 'vf_loss': 0.058038188237069605, 'total_loss': 0.0008160287068094997, 'approx_kl': 4.2763229401998615e-08, 'clip_fraction': 0.0, 'grad_norm': 30.16460609436035}
2022-12-29 17:26:13.673 DEBUG: Taking gradient step
2022-12-29 17:26:13.682 DEBUG: Loss 1: {'policy_loss': -0.01366639308305016, 'entropy_loss': -0.03810652904212475, 'vf_loss': 0.05976569170648855, 'total_loss': 0.007992769581313654, 'approx_kl': 0.0020416813204064965, 'clip_fraction': 0.0234375, 'grad_norm': 8.691764831542969}
2022-12-29 17:26:14.546 DEBUG: Taking gradient step
2022-12-29 17:26:14.555 DEBUG: Loss 2: {'policy_loss': -0.04356445082369228, 'entropy_loss': -0.039291889406740665, 'vf_loss': 0.05772001886083613, 'total_loss': -0.025136321369596815, 'approx_kl': 0.007836511824280024, 'clip_fraction': 0.1966145858168602, 'grad_norm': 16.179582595825195}
2022-12-29 17:26:15.400 DEBUG: Taking gradient step
2022-12-29 17:26:15.414 DEBUG: Loss 3: {'policy_loss': -0.03629870785481072, 'entropy_loss': -0.03625273751094937, 'vf_loss': 0.057519898953295345, 'total_loss': -0.015031546412464764, 'approx_kl': 0.016931147081777453, 'clip_fraction': 0.3111979216337204, 'grad_norm': 12.694061279296875}
2022-12-29 17:26:16.233 DEBUG: Taking gradient step
2022-12-29 17:26:16.247 DEBUG: Loss 4: {'policy_loss': 0.006726734992355179, 'entropy_loss': -0.03743104916065931, 'vf_loss': 0.0633407594366127, 'total_loss': 0.03263644526830857, 'approx_kl': 0.03392493166029453, 'clip_fraction': 0.3411458358168602, 'grad_norm': 16.182748794555664}
2022-12-29 17:26:17.097 DEBUG: Taking gradient step
2022-12-29 17:26:17.108 DEBUG: Loss 5: {'policy_loss': -0.023583781386210347, 'entropy_loss': -0.03801274113357067, 'vf_loss': 0.05938766343375387, 'total_loss': -0.002208859086027146, 'approx_kl': 0.02585615962743759, 'clip_fraction': 0.3385416716337204, 'grad_norm': 14.24720573425293}
2022-12-29 17:26:17.997 DEBUG: Taking gradient step
2022-12-29 17:26:18.006 DEBUG: Loss 6: {'policy_loss': -0.00833023466337016, 'entropy_loss': -0.03773390036076307, 'vf_loss': 0.06149076586067061, 'total_loss': 0.015426630836537374, 'approx_kl': 0.007508676499128342, 'clip_fraction': 0.2994791716337204, 'grad_norm': 8.845112800598145}
2022-12-29 17:26:18.812 DEBUG: Taking gradient step
2022-12-29 17:26:18.821 DEBUG: Loss 7: {'policy_loss': -0.005642720378557491, 'entropy_loss': -0.03751622699201107, 'vf_loss': 0.06117911067860397, 'total_loss': 0.01802016330803542, 'approx_kl': 0.006976309698075056, 'clip_fraction': 0.3046875, 'grad_norm': 21.976806640625}
2022-12-29 17:26:19.667 DEBUG: Taking gradient step
2022-12-29 17:26:19.681 DEBUG: Loss 8: {'policy_loss': -0.008169077190730978, 'entropy_loss': -0.03576319292187691, 'vf_loss': 0.0608910592149946, 'total_loss': 0.01695878910238672, 'approx_kl': 0.011978097725659609, 'clip_fraction': 0.2760416716337204, 'grad_norm': 18.032878875732422}
2022-12-29 17:26:20.504 DEBUG: Taking gradient step
2022-12-29 17:26:20.514 DEBUG: Loss 9: {'policy_loss': -0.03704117618514298, 'entropy_loss': -0.03801606874912977, 'vf_loss': 0.05683039158414389, 'total_loss': -0.018226853350128865, 'approx_kl': 0.02107399608939886, 'clip_fraction': 0.3059895858168602, 'grad_norm': 7.000382900238037}
2022-12-29 17:26:20.514 INFO: Optimization: policy loss=-0.037, vf loss=0.057, entropy loss=-0.038, total loss=-0.018, num steps=10
2022-12-29 17:26:20.514 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:26:22.013 INFO: Evaluation rollout: return=0.484 (0.0), episode length=5.0
2022-12-29 17:26:22.014 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:26:22.017 INFO: Iteration: 23/137, steps: 4968
2022-12-29 17:26:46.018 DEBUG: There is a single atom floating around
2022-12-29 17:26:52.244 DEBUG: Atoms are too close
2022-12-29 17:27:07.740 DEBUG: Atoms are too close
2022-12-29 17:27:20.735 INFO: Training rollout: return=-1.199 (5.5), episode length=4.9
2022-12-29 17:27:20.737 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:27:20.739 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-4968_train.pkl
2022-12-29 17:27:21.594 DEBUG: Taking gradient step
2022-12-29 17:27:21.608 DEBUG: Loss 0: {'policy_loss': -0.04862139542935063, 'entropy_loss': -0.03612432023510337, 'vf_loss': 0.021722202586851973, 'total_loss': -0.06302351307760204, 'approx_kl': -8.15683360855246e-08, 'clip_fraction': 0.0, 'grad_norm': 16.4625244140625}
2022-12-29 17:27:22.511 DEBUG: Taking gradient step
2022-12-29 17:27:22.520 DEBUG: Loss 1: {'policy_loss': -0.02737541118982061, 'entropy_loss': -0.03802002966403961, 'vf_loss': 0.023762988194797358, 'total_loss': -0.04163245265906286, 'approx_kl': -0.003045935765840113, 'clip_fraction': 0.022135416977107525, 'grad_norm': 15.29993724822998}
2022-12-29 17:27:23.338 DEBUG: Taking gradient step
2022-12-29 17:27:23.347 DEBUG: Loss 2: {'policy_loss': -0.036167658075834073, 'entropy_loss': -0.03718172758817673, 'vf_loss': 0.023952878662905144, 'total_loss': -0.04939650700110566, 'approx_kl': 0.005092281324323267, 'clip_fraction': 0.1861979179084301, 'grad_norm': 8.574197769165039}
2022-12-29 17:27:24.202 DEBUG: Taking gradient step
2022-12-29 17:27:24.213 DEBUG: Loss 3: {'policy_loss': -0.016840483341875993, 'entropy_loss': -0.03648135159164667, 'vf_loss': 0.025573106200758422, 'total_loss': -0.027748728732764242, 'approx_kl': 0.02391486195847392, 'clip_fraction': 0.2994791716337204, 'grad_norm': 7.428472518920898}
2022-12-29 17:27:25.023 DEBUG: Taking gradient step
2022-12-29 17:27:25.032 DEBUG: Loss 4: {'policy_loss': -0.041355826083105736, 'entropy_loss': -0.039062642492353916, 'vf_loss': 0.02393670396514847, 'total_loss': -0.05648176461031119, 'approx_kl': 0.012929527088999748, 'clip_fraction': 0.3971354216337204, 'grad_norm': 5.7808837890625}
2022-12-29 17:27:25.847 DEBUG: Taking gradient step
2022-12-29 17:27:25.856 DEBUG: Loss 5: {'policy_loss': 0.012280673695466926, 'entropy_loss': -0.0370550686493516, 'vf_loss': 0.02755804202084271, 'total_loss': 0.0027836470669580435, 'approx_kl': 0.04056741762906313, 'clip_fraction': 0.38671875, 'grad_norm': 5.370820999145508}
2022-12-29 17:27:26.682 DEBUG: Taking gradient step
2022-12-29 17:27:26.696 DEBUG: Loss 6: {'policy_loss': -0.048478208044434666, 'entropy_loss': -0.03732418268918991, 'vf_loss': 0.021670450378851316, 'total_loss': -0.06413194035477326, 'approx_kl': 0.02874608477577567, 'clip_fraction': 0.38671875, 'grad_norm': 5.289913177490234}
2022-12-29 17:27:27.526 DEBUG: Early stopping at step 7 for reaching max KL.
2022-12-29 17:27:27.527 INFO: Optimization: policy loss=-0.048, vf loss=0.022, entropy loss=-0.037, total loss=-0.064, num steps=7
2022-12-29 17:27:27.527 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:27:29.047 INFO: Evaluation rollout: return=0.505 (0.0), episode length=5.0
2022-12-29 17:27:29.048 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:27:29.051 INFO: Iteration: 24/137, steps: 5184
2022-12-29 17:27:44.685 DEBUG: Atoms are too close
2022-12-29 17:27:44.985 DEBUG: There is a single atom floating around
2022-12-29 17:27:58.888 DEBUG: Atoms are too close
2022-12-29 17:28:01.188 DEBUG: Atoms are too close
2022-12-29 17:28:16.411 DEBUG: There is a single atom floating around
2022-12-29 17:28:16.988 DEBUG: Atoms are too close
2022-12-29 17:28:17.310 DEBUG: There is a single atom floating around
2022-12-29 17:28:26.481 INFO: Training rollout: return=-3.477 (7.9), episode length=5.0
2022-12-29 17:28:26.483 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:28:26.485 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-5184_train.pkl
2022-12-29 17:28:27.348 DEBUG: Taking gradient step
2022-12-29 17:28:27.362 DEBUG: Loss 0: {'policy_loss': 0.002101973724407588, 'entropy_loss': -0.03811182454228401, 'vf_loss': 0.05409395304738501, 'total_loss': 0.01808410222950859, 'approx_kl': 5.607338238178272e-08, 'clip_fraction': 0.0, 'grad_norm': 13.661482810974121}
2022-12-29 17:28:28.210 DEBUG: Taking gradient step
2022-12-29 17:28:28.219 DEBUG: Loss 1: {'policy_loss': -0.030690818357502744, 'entropy_loss': -0.03792466223239899, 'vf_loss': 0.050018194364992344, 'total_loss': -0.018597286224909387, 'approx_kl': 0.004099835525266826, 'clip_fraction': 0.0078125, 'grad_norm': 10.693340301513672}
2022-12-29 17:28:29.079 DEBUG: Taking gradient step
2022-12-29 17:28:29.089 DEBUG: Loss 2: {'policy_loss': -0.011829369956248887, 'entropy_loss': -0.038303498178720474, 'vf_loss': 0.05432672935473276, 'total_loss': 0.0041938612197634, 'approx_kl': 0.007300753379240632, 'clip_fraction': 0.1041666679084301, 'grad_norm': 10.612873077392578}
2022-12-29 17:28:29.910 DEBUG: Taking gradient step
2022-12-29 17:28:29.920 DEBUG: Loss 3: {'policy_loss': 0.03180221507423039, 'entropy_loss': -0.03849279787391424, 'vf_loss': 0.05575689477034407, 'total_loss': 0.049066311970660206, 'approx_kl': 0.008163045276887715, 'clip_fraction': 0.14453125, 'grad_norm': 13.109521865844727}
2022-12-29 17:28:30.767 DEBUG: Taking gradient step
2022-12-29 17:28:30.777 DEBUG: Loss 4: {'policy_loss': -0.026805268000357648, 'entropy_loss': -0.03935984801501036, 'vf_loss': 0.0524481545681684, 'total_loss': -0.013716961447199603, 'approx_kl': 0.009596145479008555, 'clip_fraction': 0.2317708358168602, 'grad_norm': 10.234110832214355}
2022-12-29 17:28:31.586 DEBUG: Taking gradient step
2022-12-29 17:28:31.595 DEBUG: Loss 5: {'policy_loss': -0.027836624000110683, 'entropy_loss': -0.03959908988326788, 'vf_loss': 0.05223224509097183, 'total_loss': -0.015203468792406739, 'approx_kl': 0.019569916184991598, 'clip_fraction': 0.2408854216337204, 'grad_norm': 10.604813575744629}
2022-12-29 17:28:32.399 DEBUG: Taking gradient step
2022-12-29 17:28:32.408 DEBUG: Loss 6: {'policy_loss': 0.03425337632382311, 'entropy_loss': -0.03949821554124355, 'vf_loss': 0.058044507341523575, 'total_loss': 0.05279966812410313, 'approx_kl': 0.006071019684895873, 'clip_fraction': 0.25, 'grad_norm': 12.45858097076416}
2022-12-29 17:28:33.227 DEBUG: Taking gradient step
2022-12-29 17:28:33.238 DEBUG: Loss 7: {'policy_loss': -0.028201699639306646, 'entropy_loss': -0.03861966077238321, 'vf_loss': 0.05234957817749796, 'total_loss': -0.014471782234191904, 'approx_kl': 0.02113567339256406, 'clip_fraction': 0.1588541679084301, 'grad_norm': 8.153669357299805}
2022-12-29 17:28:34.043 DEBUG: Taking gradient step
2022-12-29 17:28:34.058 DEBUG: Loss 8: {'policy_loss': -0.023953209256308377, 'entropy_loss': -0.03934438247233629, 'vf_loss': 0.052256982630309215, 'total_loss': -0.011040609098335468, 'approx_kl': 0.008607940748333931, 'clip_fraction': 0.125, 'grad_norm': 11.72518539428711}
2022-12-29 17:28:34.924 DEBUG: Taking gradient step
2022-12-29 17:28:34.933 DEBUG: Loss 9: {'policy_loss': -0.019798627295312357, 'entropy_loss': -0.03853456024080515, 'vf_loss': 0.052330154161119816, 'total_loss': -0.0060030333749976975, 'approx_kl': 0.01899456139653921, 'clip_fraction': 0.11458333395421505, 'grad_norm': 13.000411987304688}
2022-12-29 17:28:34.933 INFO: Optimization: policy loss=-0.020, vf loss=0.052, entropy loss=-0.039, total loss=-0.006, num steps=10
2022-12-29 17:28:34.934 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:28:36.416 INFO: Evaluation rollout: return=0.506 (0.0), episode length=5.0
2022-12-29 17:28:36.417 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:28:36.420 INFO: Iteration: 25/137, steps: 5400
2022-12-29 17:28:49.490 DEBUG: Atoms are too close
2022-12-29 17:28:52.277 DEBUG: Atoms are too close
2022-12-29 17:29:25.152 DEBUG: There is a single atom floating around
2022-12-29 17:29:34.982 INFO: Training rollout: return=-1.243 (5.6), episode length=5.0
2022-12-29 17:29:34.984 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:29:34.986 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-5400_train.pkl
2022-12-29 17:29:35.871 DEBUG: Taking gradient step
2022-12-29 17:29:35.881 DEBUG: Loss 0: {'policy_loss': -0.023068193905020073, 'entropy_loss': -0.03891857899725437, 'vf_loss': 0.02538357774510077, 'total_loss': -0.03660319515717367, 'approx_kl': -3.473057041958327e-08, 'clip_fraction': 0.0, 'grad_norm': 16.643953323364258}
2022-12-29 17:29:36.706 DEBUG: Taking gradient step
2022-12-29 17:29:36.716 DEBUG: Loss 1: {'policy_loss': 0.04449954648325856, 'entropy_loss': -0.03909512981772423, 'vf_loss': 0.03098073827969972, 'total_loss': 0.036385154945234054, 'approx_kl': -0.0003301709075458348, 'clip_fraction': 0.0078125, 'grad_norm': 11.687172889709473}
2022-12-29 17:29:37.560 DEBUG: Taking gradient step
2022-12-29 17:29:37.573 DEBUG: Loss 2: {'policy_loss': -0.015549357996580773, 'entropy_loss': -0.039260732010006905, 'vf_loss': 0.027560318418113225, 'total_loss': -0.02724977158847445, 'approx_kl': 0.0028700841357931495, 'clip_fraction': 0.16015625, 'grad_norm': 6.864634037017822}
2022-12-29 17:29:38.397 DEBUG: Taking gradient step
2022-12-29 17:29:38.406 DEBUG: Loss 3: {'policy_loss': 0.016264560645237347, 'entropy_loss': -0.038695311173796654, 'vf_loss': 0.0292819619071964, 'total_loss': 0.00685121137863709, 'approx_kl': 0.013272879645228386, 'clip_fraction': 0.2825520858168602, 'grad_norm': 8.382844924926758}
2022-12-29 17:29:39.191 DEBUG: Taking gradient step
2022-12-29 17:29:39.200 DEBUG: Loss 4: {'policy_loss': -0.04246877271907065, 'entropy_loss': -0.03856644593179226, 'vf_loss': 0.0254702333052936, 'total_loss': -0.05556498534556931, 'approx_kl': 0.007353421300649643, 'clip_fraction': 0.3528645858168602, 'grad_norm': 6.844646453857422}
2022-12-29 17:29:40.027 DEBUG: Taking gradient step
2022-12-29 17:29:40.036 DEBUG: Loss 5: {'policy_loss': 0.015336817915217314, 'entropy_loss': -0.03543771104887128, 'vf_loss': 0.02890292227064758, 'total_loss': 0.008802029136993607, 'approx_kl': 0.014659707434475422, 'clip_fraction': 0.4348958358168602, 'grad_norm': 5.670941352844238}
2022-12-29 17:29:40.863 DEBUG: Taking gradient step
2022-12-29 17:29:40.872 DEBUG: Loss 6: {'policy_loss': -0.060779640946299396, 'entropy_loss': -0.03698203153908253, 'vf_loss': 0.02346767152651244, 'total_loss': -0.07429400095886948, 'approx_kl': 0.039088441990315914, 'clip_fraction': 0.4544270858168602, 'grad_norm': 6.773525714874268}
2022-12-29 17:29:41.664 DEBUG: Taking gradient step
2022-12-29 17:29:41.673 DEBUG: Loss 7: {'policy_loss': -0.01693804194337909, 'entropy_loss': -0.037780395708978176, 'vf_loss': 0.02720979718397015, 'total_loss': -0.027508640468387118, 'approx_kl': 0.037343465723097324, 'clip_fraction': 0.45703125, 'grad_norm': 6.226457118988037}
2022-12-29 17:29:42.472 DEBUG: Taking gradient step
2022-12-29 17:29:42.482 DEBUG: Loss 8: {'policy_loss': -0.04130906357835619, 'entropy_loss': -0.036076340824365616, 'vf_loss': 0.025186682317489797, 'total_loss': -0.052198722085232, 'approx_kl': 0.025252762716263533, 'clip_fraction': 0.4192708358168602, 'grad_norm': 3.728182792663574}
2022-12-29 17:29:43.352 DEBUG: Taking gradient step
2022-12-29 17:29:43.362 DEBUG: Loss 9: {'policy_loss': 0.001982998930855684, 'entropy_loss': -0.03782375529408455, 'vf_loss': 0.028978442044537808, 'total_loss': -0.006862314318691054, 'approx_kl': 0.015808628988452256, 'clip_fraction': 0.4309895858168602, 'grad_norm': 4.1462602615356445}
2022-12-29 17:29:43.362 INFO: Optimization: policy loss=0.002, vf loss=0.029, entropy loss=-0.038, total loss=-0.007, num steps=10
2022-12-29 17:29:43.362 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:29:44.828 INFO: Evaluation rollout: return=0.512 (0.0), episode length=5.0
2022-12-29 17:29:44.829 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:29:44.832 INFO: Iteration: 26/137, steps: 5616
2022-12-29 17:30:10.809 DEBUG: Atoms are too close
2022-12-29 17:30:10.810 DEBUG: Atoms are too close
2022-12-29 17:30:31.168 DEBUG: Atoms are too close
2022-12-29 17:30:42.927 INFO: Training rollout: return=-1.257 (5.6), episode length=4.9
2022-12-29 17:30:42.929 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:30:42.931 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-5616_train.pkl
2022-12-29 17:30:43.773 DEBUG: Taking gradient step
2022-12-29 17:30:43.784 DEBUG: Loss 0: {'policy_loss': 0.06159520399054772, 'entropy_loss': -0.0371325733140111, 'vf_loss': 0.029229770327953684, 'total_loss': 0.0536924010044903, 'approx_kl': -3.651560831485767e-08, 'clip_fraction': 0.0, 'grad_norm': 25.035104751586914}
2022-12-29 17:30:44.594 DEBUG: Taking gradient step
2022-12-29 17:30:44.603 DEBUG: Loss 1: {'policy_loss': -0.019724326330582034, 'entropy_loss': -0.03845072817057371, 'vf_loss': 0.022934367504306698, 'total_loss': -0.03524068699684905, 'approx_kl': 0.0033048262703232467, 'clip_fraction': 0.0651041679084301, 'grad_norm': 7.510700702667236}
2022-12-29 17:30:45.426 DEBUG: Taking gradient step
2022-12-29 17:30:45.443 DEBUG: Loss 2: {'policy_loss': -0.024340565919159684, 'entropy_loss': -0.037209609523415565, 'vf_loss': 0.02287953455831907, 'total_loss': -0.03867064088425617, 'approx_kl': 0.018662470276467502, 'clip_fraction': 0.18359375, 'grad_norm': 13.136112213134766}
2022-12-29 17:30:46.251 DEBUG: Taking gradient step
2022-12-29 17:30:46.261 DEBUG: Loss 3: {'policy_loss': -0.0318003324304269, 'entropy_loss': -0.03720676340162754, 'vf_loss': 0.02283867695178664, 'total_loss': -0.0461684188802678, 'approx_kl': 0.03212178172543645, 'clip_fraction': 0.3229166716337204, 'grad_norm': 10.654120445251465}
2022-12-29 17:30:47.055 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 17:30:47.055 INFO: Optimization: policy loss=-0.032, vf loss=0.023, entropy loss=-0.037, total loss=-0.046, num steps=4
2022-12-29 17:30:47.056 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:30:48.492 INFO: Evaluation rollout: return=0.501 (0.0), episode length=5.0
2022-12-29 17:30:48.493 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:30:48.496 INFO: Iteration: 27/137, steps: 5832
2022-12-29 17:31:02.821 DEBUG: Atoms are too close
2022-12-29 17:31:14.039 DEBUG: Atoms are too close
2022-12-29 17:31:18.960 DEBUG: Atoms are too close
2022-12-29 17:31:19.270 DEBUG: Atoms are too close
2022-12-29 17:31:31.580 DEBUG: Atoms are too close
2022-12-29 17:31:45.682 INFO: Training rollout: return=-2.335 (7.0), episode length=4.9
2022-12-29 17:31:45.683 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:31:45.686 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-5832_train.pkl
2022-12-29 17:31:46.517 DEBUG: Taking gradient step
2022-12-29 17:31:46.527 DEBUG: Loss 0: {'policy_loss': -0.011982488450393361, 'entropy_loss': -0.03719559218734503, 'vf_loss': 0.03650227461957171, 'total_loss': -0.012675806018166681, 'approx_kl': -5.75091689825058e-08, 'clip_fraction': 0.0, 'grad_norm': 13.082998275756836}
2022-12-29 17:31:47.397 DEBUG: Taking gradient step
2022-12-29 17:31:47.407 DEBUG: Loss 1: {'policy_loss': -0.0050548104506192015, 'entropy_loss': -0.03646889794617891, 'vf_loss': 0.03876628228119802, 'total_loss': -0.002757426115600087, 'approx_kl': -0.0076588933588936925, 'clip_fraction': 0.041666666977107525, 'grad_norm': 12.668516159057617}
2022-12-29 17:31:48.231 DEBUG: Taking gradient step
2022-12-29 17:31:48.240 DEBUG: Loss 2: {'policy_loss': -0.008191971389423002, 'entropy_loss': -0.03587513882666826, 'vf_loss': 0.038579690712602734, 'total_loss': -0.005487419503488536, 'approx_kl': -0.01068389997817576, 'clip_fraction': 0.2447916716337204, 'grad_norm': 11.08544921875}
2022-12-29 17:31:49.064 DEBUG: Taking gradient step
2022-12-29 17:31:49.073 DEBUG: Loss 3: {'policy_loss': 0.01617784697095176, 'entropy_loss': -0.03600550536066294, 'vf_loss': 0.040681548793025255, 'total_loss': 0.02085389040331407, 'approx_kl': 0.001343137410003692, 'clip_fraction': 0.3736979216337204, 'grad_norm': 7.363706588745117}
2022-12-29 17:31:49.885 DEBUG: Taking gradient step
2022-12-29 17:31:49.895 DEBUG: Loss 4: {'policy_loss': -0.014407825174632717, 'entropy_loss': -0.034926900174468756, 'vf_loss': 0.03641431588491822, 'total_loss': -0.012920409464183252, 'approx_kl': 9.031954687088728e-05, 'clip_fraction': 0.42578125, 'grad_norm': 8.934436798095703}
2022-12-29 17:31:50.720 DEBUG: Taking gradient step
2022-12-29 17:31:50.730 DEBUG: Loss 5: {'policy_loss': -0.0021368937232672178, 'entropy_loss': -0.03617148660123348, 'vf_loss': 0.03853220091444908, 'total_loss': 0.0002238205899483814, 'approx_kl': 0.013084955979138613, 'clip_fraction': 0.4440104216337204, 'grad_norm': 8.719213485717773}
2022-12-29 17:31:51.557 DEBUG: Taking gradient step
2022-12-29 17:31:51.573 DEBUG: Loss 6: {'policy_loss': -0.009597345394992282, 'entropy_loss': -0.03648112341761589, 'vf_loss': 0.038614884137423644, 'total_loss': -0.0074635846751845225, 'approx_kl': 0.014190141810104251, 'clip_fraction': 0.4205729216337204, 'grad_norm': 6.693756103515625}
2022-12-29 17:31:52.414 DEBUG: Taking gradient step
2022-12-29 17:31:52.425 DEBUG: Loss 7: {'policy_loss': -0.010814286897357623, 'entropy_loss': -0.03383007366210222, 'vf_loss': 0.038589240620446184, 'total_loss': -0.006055119939013664, 'approx_kl': 0.008834541542455554, 'clip_fraction': 0.3919270858168602, 'grad_norm': 6.415412425994873}
2022-12-29 17:31:53.270 DEBUG: Taking gradient step
2022-12-29 17:31:53.284 DEBUG: Loss 8: {'policy_loss': -0.022366861003217036, 'entropy_loss': -0.034544185269623995, 'vf_loss': 0.03656235403379946, 'total_loss': -0.020348692239041573, 'approx_kl': 0.004192743799649179, 'clip_fraction': 0.3567708358168602, 'grad_norm': 9.410335540771484}
2022-12-29 17:31:54.137 DEBUG: Taking gradient step
2022-12-29 17:31:54.147 DEBUG: Loss 9: {'policy_loss': -0.06401868600192426, 'entropy_loss': -0.03449996840208769, 'vf_loss': 0.03237029600614115, 'total_loss': -0.0661483583978708, 'approx_kl': 0.008457070216536522, 'clip_fraction': 0.3333333358168602, 'grad_norm': 6.920199871063232}
2022-12-29 17:31:54.147 INFO: Optimization: policy loss=-0.064, vf loss=0.032, entropy loss=-0.034, total loss=-0.066, num steps=10
2022-12-29 17:31:54.147 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:31:55.622 INFO: Evaluation rollout: return=0.505 (0.0), episode length=5.0
2022-12-29 17:31:55.623 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:31:55.625 INFO: Iteration: 28/137, steps: 6048
2022-12-29 17:32:41.522 DEBUG: Atoms are too close
2022-12-29 17:32:43.011 DEBUG: There is a single atom floating around
2022-12-29 17:32:53.782 INFO: Training rollout: return=-0.672 (4.6), episode length=5.0
2022-12-29 17:32:53.784 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:32:53.786 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-6048_train.pkl
2022-12-29 17:32:54.626 DEBUG: Taking gradient step
2022-12-29 17:32:54.637 DEBUG: Loss 0: {'policy_loss': -0.007706496284913109, 'entropy_loss': -0.03595411591231823, 'vf_loss': 0.01845216387395688, 'total_loss': -0.025208448323274457, 'approx_kl': -3.00739584346843e-08, 'clip_fraction': 0.0, 'grad_norm': 26.632545471191406}
2022-12-29 17:32:55.474 DEBUG: Taking gradient step
2022-12-29 17:32:55.484 DEBUG: Loss 1: {'policy_loss': -0.023273378641064168, 'entropy_loss': -0.036593115888535976, 'vf_loss': 0.018335388843334275, 'total_loss': -0.04153110568626586, 'approx_kl': -0.0032167212339118123, 'clip_fraction': 0.05208333395421505, 'grad_norm': 17.721799850463867}
2022-12-29 17:32:56.302 DEBUG: Taking gradient step
2022-12-29 17:32:56.314 DEBUG: Loss 2: {'policy_loss': -0.012439982025118056, 'entropy_loss': -0.03587741777300835, 'vf_loss': 0.020568476609432562, 'total_loss': -0.027748923188693847, 'approx_kl': 0.00955357775092125, 'clip_fraction': 0.296875, 'grad_norm': 4.909879684448242}
2022-12-29 17:32:57.126 DEBUG: Taking gradient step
2022-12-29 17:32:57.137 DEBUG: Loss 3: {'policy_loss': -0.018372762848117542, 'entropy_loss': -0.03520480263978243, 'vf_loss': 0.018384111296520268, 'total_loss': -0.03519345419137971, 'approx_kl': 0.018016679910942912, 'clip_fraction': 0.3411458358168602, 'grad_norm': 10.455740928649902}
2022-12-29 17:32:57.935 DEBUG: Taking gradient step
2022-12-29 17:32:57.945 DEBUG: Loss 4: {'policy_loss': -0.007143800833648316, 'entropy_loss': -0.036234527826309204, 'vf_loss': 0.020412659030078655, 'total_loss': -0.022965669629878865, 'approx_kl': 0.04290098138153553, 'clip_fraction': 0.4296875, 'grad_norm': 5.698367118835449}
2022-12-29 17:32:58.764 DEBUG: Early stopping at step 5 for reaching max KL.
2022-12-29 17:32:58.765 INFO: Optimization: policy loss=-0.007, vf loss=0.020, entropy loss=-0.036, total loss=-0.023, num steps=5
2022-12-29 17:32:58.765 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:33:00.190 INFO: Evaluation rollout: return=0.514 (0.0), episode length=5.0
2022-12-29 17:33:00.191 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:33:00.194 INFO: Iteration: 29/137, steps: 6264
2022-12-29 17:33:10.640 DEBUG: There is a single atom floating around
2022-12-29 17:33:13.806 DEBUG: Atoms are too close
2022-12-29 17:33:48.450 DEBUG: Atoms are too close
2022-12-29 17:33:58.178 INFO: Training rollout: return=-1.220 (5.6), episode length=5.0
2022-12-29 17:33:58.179 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:33:58.182 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-6264_train.pkl
2022-12-29 17:33:59.019 DEBUG: Taking gradient step
2022-12-29 17:33:59.035 DEBUG: Loss 0: {'policy_loss': 0.01101479740667459, 'entropy_loss': -0.03655557334423065, 'vf_loss': 0.025572240946942895, 'total_loss': 3.146500938683294e-05, 'approx_kl': -5.1300352410521555e-08, 'clip_fraction': 0.0, 'grad_norm': 22.319538116455078}
2022-12-29 17:33:59.936 DEBUG: Taking gradient step
2022-12-29 17:33:59.945 DEBUG: Loss 1: {'policy_loss': 0.003206676486465443, 'entropy_loss': -0.03606341127306223, 'vf_loss': 0.025479083672793487, 'total_loss': -0.0073776511138033025, 'approx_kl': 0.0005083905416540802, 'clip_fraction': 0.0, 'grad_norm': 13.927124977111816}
2022-12-29 17:34:00.756 DEBUG: Taking gradient step
2022-12-29 17:34:00.765 DEBUG: Loss 2: {'policy_loss': -0.030993558461998912, 'entropy_loss': -0.03555952291935682, 'vf_loss': 0.02338330008557294, 'total_loss': -0.043169781295782785, 'approx_kl': 0.002004290698096156, 'clip_fraction': 0.06640625, 'grad_norm': 8.97812557220459}
2022-12-29 17:34:01.584 DEBUG: Taking gradient step
2022-12-29 17:34:01.593 DEBUG: Loss 3: {'policy_loss': -0.04137261577437054, 'entropy_loss': -0.0369073823094368, 'vf_loss': 0.02339374205985542, 'total_loss': -0.05488625602395192, 'approx_kl': 0.008427128661423922, 'clip_fraction': 0.2408854179084301, 'grad_norm': 4.855886459350586}
2022-12-29 17:34:02.395 DEBUG: Taking gradient step
2022-12-29 17:34:02.404 DEBUG: Loss 4: {'policy_loss': -0.01513291036819158, 'entropy_loss': -0.03666594438254833, 'vf_loss': 0.02568646642207746, 'total_loss': -0.026112388328662455, 'approx_kl': 0.01580278144683689, 'clip_fraction': 0.3684895858168602, 'grad_norm': 11.041330337524414}
2022-12-29 17:34:03.230 DEBUG: Taking gradient step
2022-12-29 17:34:03.239 DEBUG: Loss 5: {'policy_loss': -0.0533115570114609, 'entropy_loss': -0.03713586088269949, 'vf_loss': 0.02115832859345309, 'total_loss': -0.0692890893007073, 'approx_kl': 0.03741855674888939, 'clip_fraction': 0.4388020858168602, 'grad_norm': 5.627018928527832}
2022-12-29 17:34:04.044 DEBUG: Early stopping at step 6 for reaching max KL.
2022-12-29 17:34:04.044 INFO: Optimization: policy loss=-0.053, vf loss=0.021, entropy loss=-0.037, total loss=-0.069, num steps=6
2022-12-29 17:34:04.045 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:34:05.472 INFO: Evaluation rollout: return=0.557 (0.0), episode length=5.0
2022-12-29 17:34:05.474 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:34:05.476 INFO: Iteration: 30/137, steps: 6480
2022-12-29 17:34:12.288 DEBUG: Atoms are too close
2022-12-29 17:34:28.689 DEBUG: Atoms are too close
2022-12-29 17:34:34.904 DEBUG: Atoms are too close
2022-12-29 17:34:47.596 DEBUG: Atoms are too close
2022-12-29 17:34:59.859 DEBUG: Atoms are too close
2022-12-29 17:35:03.007 INFO: Training rollout: return=-2.131 (6.8), episode length=4.8
2022-12-29 17:35:03.008 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:35:03.011 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-6480_train.pkl
2022-12-29 17:35:03.838 DEBUG: Taking gradient step
2022-12-29 17:35:03.849 DEBUG: Loss 0: {'policy_loss': -0.02073547654053123, 'entropy_loss': -0.037202611565589905, 'vf_loss': 0.028488494971804328, 'total_loss': -0.029449593134316804, 'approx_kl': 3.958120942115784e-08, 'clip_fraction': 0.0, 'grad_norm': 18.68830108642578}
2022-12-29 17:35:04.666 DEBUG: Taking gradient step
2022-12-29 17:35:04.677 DEBUG: Loss 1: {'policy_loss': 0.006171686932626676, 'entropy_loss': -0.038573759607970715, 'vf_loss': 0.03287272297830589, 'total_loss': 0.00047065030296185306, 'approx_kl': 0.0024783170083537698, 'clip_fraction': 0.06640625, 'grad_norm': 24.177099227905273}
2022-12-29 17:35:05.501 DEBUG: Taking gradient step
2022-12-29 17:35:05.511 DEBUG: Loss 2: {'policy_loss': 0.06784321472145761, 'entropy_loss': -0.03717954736202955, 'vf_loss': 0.03945411824743551, 'total_loss': 0.07011778560686356, 'approx_kl': 0.0226319064386189, 'clip_fraction': 0.2213541679084301, 'grad_norm': 8.103410720825195}
2022-12-29 17:35:06.452 DEBUG: Taking gradient step
2022-12-29 17:35:06.466 DEBUG: Loss 3: {'policy_loss': -0.03137653308479996, 'entropy_loss': -0.03888283111155033, 'vf_loss': 0.02844750922173683, 'total_loss': -0.04181185497461346, 'approx_kl': 0.04219185188412666, 'clip_fraction': 0.3294270858168602, 'grad_norm': 5.387609958648682}
2022-12-29 17:35:07.347 DEBUG: Taking gradient step
2022-12-29 17:35:07.356 DEBUG: Loss 4: {'policy_loss': -0.02584742955445731, 'entropy_loss': -0.037860019132494926, 'vf_loss': 0.028327185476422195, 'total_loss': -0.03538026321053003, 'approx_kl': 0.041531831957399845, 'clip_fraction': 0.3372395858168602, 'grad_norm': 5.181705951690674}
2022-12-29 17:35:08.180 DEBUG: Taking gradient step
2022-12-29 17:35:08.190 DEBUG: Loss 5: {'policy_loss': 0.027203513563696435, 'entropy_loss': -0.03753233514726162, 'vf_loss': 0.034801370738023925, 'total_loss': 0.02447254915445874, 'approx_kl': 0.03403308358974755, 'clip_fraction': 0.2578125, 'grad_norm': 4.128201961517334}
2022-12-29 17:35:09.016 DEBUG: Taking gradient step
2022-12-29 17:35:09.029 DEBUG: Loss 6: {'policy_loss': -0.037725314296865126, 'entropy_loss': -0.03711726050823927, 'vf_loss': 0.028514845747857197, 'total_loss': -0.0463277290572472, 'approx_kl': 0.035386129980906844, 'clip_fraction': 0.2630208358168602, 'grad_norm': 4.605278491973877}
2022-12-29 17:35:09.858 DEBUG: Taking gradient step
2022-12-29 17:35:09.868 DEBUG: Loss 7: {'policy_loss': 0.021329931042415748, 'entropy_loss': -0.03489155974239111, 'vf_loss': 0.03529000911719606, 'total_loss': 0.0217283804172207, 'approx_kl': 0.03716157888993621, 'clip_fraction': 0.234375, 'grad_norm': 3.5254921913146973}
2022-12-29 17:35:10.690 DEBUG: Taking gradient step
2022-12-29 17:35:10.699 DEBUG: Loss 8: {'policy_loss': -0.0009056446933177996, 'entropy_loss': -0.036068130284547806, 'vf_loss': 0.032606871141247766, 'total_loss': -0.004366903836617836, 'approx_kl': 0.02841119165532291, 'clip_fraction': 0.2643229179084301, 'grad_norm': 6.046905994415283}
2022-12-29 17:35:11.524 DEBUG: Taking gradient step
2022-12-29 17:35:11.534 DEBUG: Loss 9: {'policy_loss': 0.02243623291405763, 'entropy_loss': -0.036220052279531956, 'vf_loss': 0.034995203249447715, 'total_loss': 0.021211383883973395, 'approx_kl': 0.024645504308864474, 'clip_fraction': 0.2760416716337204, 'grad_norm': 7.732654571533203}
2022-12-29 17:35:11.534 INFO: Optimization: policy loss=0.022, vf loss=0.035, entropy loss=-0.036, total loss=0.021, num steps=10
2022-12-29 17:35:11.535 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:35:13.022 INFO: Evaluation rollout: return=0.567 (0.0), episode length=5.0
2022-12-29 17:35:13.023 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:35:13.025 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-4536.model
2022-12-29 17:35:13.028 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-6696.model
2022-12-29 17:35:13.058 INFO: Iteration: 31/137, steps: 6696
2022-12-29 17:35:23.453 DEBUG: Atoms are too close
2022-12-29 17:35:24.695 DEBUG: Atoms are too close
2022-12-29 17:35:59.943 DEBUG: There is a single atom floating around
2022-12-29 17:36:11.317 INFO: Training rollout: return=-1.222 (5.6), episode length=4.9
2022-12-29 17:36:11.319 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:36:11.321 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-6696_train.pkl
2022-12-29 17:36:12.162 DEBUG: Taking gradient step
2022-12-29 17:36:12.172 DEBUG: Loss 0: {'policy_loss': -0.041043466933419676, 'entropy_loss': -0.0376466391608119, 'vf_loss': 0.019626894528144017, 'total_loss': -0.059063211566087556, 'approx_kl': 3.570070106206913e-08, 'clip_fraction': 0.0, 'grad_norm': 16.591999053955078}
2022-12-29 17:36:12.984 DEBUG: Taking gradient step
2022-12-29 17:36:12.993 DEBUG: Loss 1: {'policy_loss': -0.016046240387061435, 'entropy_loss': -0.034997540060430765, 'vf_loss': 0.021806943064569304, 'total_loss': -0.029236837382922896, 'approx_kl': 0.01344397704815492, 'clip_fraction': 0.015625, 'grad_norm': 13.956939697265625}
2022-12-29 17:36:13.837 DEBUG: Taking gradient step
2022-12-29 17:36:13.847 DEBUG: Loss 2: {'policy_loss': -0.018121619915106957, 'entropy_loss': -0.03607458993792534, 'vf_loss': 0.021772049768180042, 'total_loss': -0.03242416008485226, 'approx_kl': 0.03176771476864815, 'clip_fraction': 0.09635416697710752, 'grad_norm': 15.644339561462402}
2022-12-29 17:36:14.718 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 17:36:14.719 INFO: Optimization: policy loss=-0.018, vf loss=0.022, entropy loss=-0.036, total loss=-0.032, num steps=3
2022-12-29 17:36:14.719 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:36:16.187 INFO: Evaluation rollout: return=0.571 (0.0), episode length=5.0
2022-12-29 17:36:16.188 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:36:16.191 INFO: Iteration: 32/137, steps: 6912
2022-12-29 17:36:31.104 DEBUG: Atoms are too close
2022-12-29 17:36:45.764 DEBUG: Atoms are too close
2022-12-29 17:36:48.462 DEBUG: Atoms are too close
2022-12-29 17:36:56.450 DEBUG: Atoms are too close
2022-12-29 17:36:58.405 DEBUG: Atoms are too close
2022-12-29 17:37:14.143 INFO: Training rollout: return=-2.239 (6.9), episode length=4.9
2022-12-29 17:37:14.145 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:37:14.149 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-6912_train.pkl
2022-12-29 17:37:15.070 DEBUG: Taking gradient step
2022-12-29 17:37:15.084 DEBUG: Loss 0: {'policy_loss': 0.009696968079525936, 'entropy_loss': -0.036369032226502895, 'vf_loss': 0.03780789947649402, 'total_loss': 0.011135835329517056, 'approx_kl': -3.348880772691132e-08, 'clip_fraction': 0.0, 'grad_norm': 11.229606628417969}
2022-12-29 17:37:15.960 DEBUG: Taking gradient step
2022-12-29 17:37:15.969 DEBUG: Loss 1: {'policy_loss': -0.041649014568069784, 'entropy_loss': -0.03511663991957903, 'vf_loss': 0.03359750057952677, 'total_loss': -0.043168153908122044, 'approx_kl': -0.007787430542521179, 'clip_fraction': 0.057291666977107525, 'grad_norm': 9.378745079040527}
2022-12-29 17:37:16.791 DEBUG: Taking gradient step
2022-12-29 17:37:16.807 DEBUG: Loss 2: {'policy_loss': -0.003996045035488919, 'entropy_loss': -0.034868660382926464, 'vf_loss': 0.037744725423109395, 'total_loss': -0.001119979995305987, 'approx_kl': 0.00886960793286562, 'clip_fraction': 0.2903645858168602, 'grad_norm': 9.211932182312012}
2022-12-29 17:37:17.705 DEBUG: Taking gradient step
2022-12-29 17:37:17.715 DEBUG: Loss 3: {'policy_loss': -0.04911134151368881, 'entropy_loss': -0.03418288193643093, 'vf_loss': 0.03370290649941071, 'total_loss': -0.04959131695070901, 'approx_kl': 0.02122538350522518, 'clip_fraction': 0.32421875, 'grad_norm': 10.545727729797363}
2022-12-29 17:37:18.552 DEBUG: Taking gradient step
2022-12-29 17:37:18.562 DEBUG: Loss 4: {'policy_loss': -0.02264217974841703, 'entropy_loss': -0.03602275159209967, 'vf_loss': 0.035736332571646795, 'total_loss': -0.0229285987688699, 'approx_kl': 0.03224161174148321, 'clip_fraction': 0.3658854216337204, 'grad_norm': 10.173323631286621}
2022-12-29 17:37:19.384 DEBUG: Taking gradient step
2022-12-29 17:37:19.395 DEBUG: Loss 5: {'policy_loss': -0.02097815152360829, 'entropy_loss': -0.03470669873058796, 'vf_loss': 0.03579759624837722, 'total_loss': -0.019887254005819022, 'approx_kl': 0.032641383819282055, 'clip_fraction': 0.3671875, 'grad_norm': 6.92262601852417}
2022-12-29 17:37:20.250 DEBUG: Taking gradient step
2022-12-29 17:37:20.261 DEBUG: Loss 6: {'policy_loss': -0.018334185237664183, 'entropy_loss': -0.03412324097007513, 'vf_loss': 0.03825835836376107, 'total_loss': -0.01419906784397825, 'approx_kl': 0.04346651816740632, 'clip_fraction': 0.3359375, 'grad_norm': 5.165614604949951}
2022-12-29 17:37:21.079 DEBUG: Taking gradient step
2022-12-29 17:37:21.089 DEBUG: Loss 7: {'policy_loss': -0.007001450343853563, 'entropy_loss': -0.03406589850783348, 'vf_loss': 0.03773757529668745, 'total_loss': -0.0033297735549995995, 'approx_kl': 0.02723513450473547, 'clip_fraction': 0.30078125, 'grad_norm': 5.6264424324035645}
2022-12-29 17:37:21.918 DEBUG: Taking gradient step
2022-12-29 17:37:21.932 DEBUG: Loss 8: {'policy_loss': 0.0026343896766599234, 'entropy_loss': -0.03338942863047123, 'vf_loss': 0.04006301838676241, 'total_loss': 0.0093079794329511, 'approx_kl': 0.01024101977236569, 'clip_fraction': 0.29296875, 'grad_norm': 10.613112449645996}
2022-12-29 17:37:22.742 DEBUG: Taking gradient step
2022-12-29 17:37:22.752 DEBUG: Loss 9: {'policy_loss': 0.0006277304968744513, 'entropy_loss': -0.03567326767370105, 'vf_loss': 0.04033947372021608, 'total_loss': 0.005293936543389499, 'approx_kl': 0.020348374266177416, 'clip_fraction': 0.41015625, 'grad_norm': 9.560258865356445}
2022-12-29 17:37:22.752 INFO: Optimization: policy loss=0.001, vf loss=0.040, entropy loss=-0.036, total loss=0.005, num steps=10
2022-12-29 17:37:22.752 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:37:24.279 INFO: Evaluation rollout: return=0.585 (0.0), episode length=5.0
2022-12-29 17:37:24.281 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:37:24.283 INFO: Iteration: 33/137, steps: 7128
2022-12-29 17:37:52.215 DEBUG: There is a single atom floating around
2022-12-29 17:37:55.678 DEBUG: Atoms are too close
2022-12-29 17:38:04.070 DEBUG: Atoms are too close
2022-12-29 17:38:22.095 INFO: Training rollout: return=-1.152 (5.5), episode length=4.9
2022-12-29 17:38:22.096 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:38:22.099 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-7128_train.pkl
2022-12-29 17:38:22.920 DEBUG: Taking gradient step
2022-12-29 17:38:22.930 DEBUG: Loss 0: {'policy_loss': 0.05320528969892122, 'entropy_loss': -0.03448546538129449, 'vf_loss': 0.025261666619862455, 'total_loss': 0.04398149093748918, 'approx_kl': -5.0640664994716644e-09, 'clip_fraction': 0.0, 'grad_norm': 30.57541275024414}
2022-12-29 17:38:23.763 DEBUG: Taking gradient step
2022-12-29 17:38:23.773 DEBUG: Loss 1: {'policy_loss': -0.03651845405564623, 'entropy_loss': -0.035262598656117916, 'vf_loss': 0.01857554601902004, 'total_loss': -0.0532055066927441, 'approx_kl': -0.005170210148207843, 'clip_fraction': 0.08723958395421505, 'grad_norm': 7.223985195159912}
2022-12-29 17:38:24.667 DEBUG: Taking gradient step
2022-12-29 17:38:24.682 DEBUG: Loss 2: {'policy_loss': -0.01652065896681815, 'entropy_loss': -0.03390151355415583, 'vf_loss': 0.02079988778985744, 'total_loss': -0.029622284731116538, 'approx_kl': 0.011955413268879056, 'clip_fraction': 0.26171875, 'grad_norm': 9.434707641601562}
2022-12-29 17:38:25.572 DEBUG: Taking gradient step
2022-12-29 17:38:25.585 DEBUG: Loss 3: {'policy_loss': -0.02459791927605696, 'entropy_loss': -0.03330726455897093, 'vf_loss': 0.020881635361329143, 'total_loss': -0.03702354847369875, 'approx_kl': 0.008149177301675081, 'clip_fraction': 0.27734375, 'grad_norm': 8.416669845581055}
2022-12-29 17:38:26.412 DEBUG: Taking gradient step
2022-12-29 17:38:26.421 DEBUG: Loss 4: {'policy_loss': -0.052718936763107495, 'entropy_loss': -0.03306608321145177, 'vf_loss': 0.018718035953137594, 'total_loss': -0.06706698402142167, 'approx_kl': 0.02860909979790449, 'clip_fraction': 0.3151041716337204, 'grad_norm': 6.447277069091797}
2022-12-29 17:38:27.236 DEBUG: Taking gradient step
2022-12-29 17:38:27.245 DEBUG: Loss 5: {'policy_loss': -0.029071628701409245, 'entropy_loss': -0.0324427061714232, 'vf_loss': 0.02089115409079121, 'total_loss': -0.04062318078204123, 'approx_kl': 0.03784728096798062, 'clip_fraction': 0.37109375, 'grad_norm': 10.619123458862305}
2022-12-29 17:38:28.069 DEBUG: Early stopping at step 6 for reaching max KL.
2022-12-29 17:38:28.070 INFO: Optimization: policy loss=-0.029, vf loss=0.021, entropy loss=-0.032, total loss=-0.041, num steps=6
2022-12-29 17:38:28.070 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:38:29.474 INFO: Evaluation rollout: return=0.619 (0.0), episode length=5.0
2022-12-29 17:38:29.475 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:38:29.477 INFO: Iteration: 34/137, steps: 7344
2022-12-29 17:39:15.156 DEBUG: Atoms are too close
2022-12-29 17:39:16.904 DEBUG: Atoms are too close
2022-12-29 17:39:25.932 DEBUG: Atoms are too close
2022-12-29 17:39:26.860 INFO: Training rollout: return=-1.126 (5.5), episode length=4.9
2022-12-29 17:39:26.861 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:39:26.864 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-7344_train.pkl
2022-12-29 17:39:27.742 DEBUG: Taking gradient step
2022-12-29 17:39:27.753 DEBUG: Loss 0: {'policy_loss': -0.03423975539784031, 'entropy_loss': -0.032837734557688236, 'vf_loss': 0.01978683230678061, 'total_loss': -0.047290657648747944, 'approx_kl': 4.1211023926734924e-08, 'clip_fraction': 0.0, 'grad_norm': 9.639718055725098}
2022-12-29 17:39:28.581 DEBUG: Taking gradient step
2022-12-29 17:39:28.591 DEBUG: Loss 1: {'policy_loss': -0.04157080460069655, 'entropy_loss': -0.03273935057222843, 'vf_loss': 0.019786899632072166, 'total_loss': -0.054523255540852814, 'approx_kl': -0.002134568989276886, 'clip_fraction': 0.00390625, 'grad_norm': 9.531134605407715}
2022-12-29 17:39:29.490 DEBUG: Taking gradient step
2022-12-29 17:39:29.500 DEBUG: Loss 2: {'policy_loss': -0.0016006547550849945, 'entropy_loss': -0.03174754697829485, 'vf_loss': 0.024319491927331478, 'total_loss': -0.009028709806048366, 'approx_kl': 0.009122535935603082, 'clip_fraction': 0.1783854179084301, 'grad_norm': 7.618719577789307}
2022-12-29 17:39:30.373 DEBUG: Taking gradient step
2022-12-29 17:39:30.382 DEBUG: Loss 3: {'policy_loss': -0.046562138572223244, 'entropy_loss': -0.031011165119707584, 'vf_loss': 0.019800860019596887, 'total_loss': -0.05777244367233394, 'approx_kl': 0.022215162171050906, 'clip_fraction': 0.2526041679084301, 'grad_norm': 6.955109119415283}
2022-12-29 17:39:31.211 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 17:39:31.212 INFO: Optimization: policy loss=-0.047, vf loss=0.020, entropy loss=-0.031, total loss=-0.058, num steps=4
2022-12-29 17:39:31.212 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:39:32.657 INFO: Evaluation rollout: return=0.639 (0.0), episode length=5.0
2022-12-29 17:39:32.658 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:39:32.661 INFO: Iteration: 35/137, steps: 7560
2022-12-29 17:39:45.057 DEBUG: There is a single atom floating around
2022-12-29 17:40:02.632 DEBUG: Atoms are too close
2022-12-29 17:40:30.485 INFO: Training rollout: return=-0.607 (4.6), episode length=5.0
2022-12-29 17:40:30.487 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:40:30.490 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-7560_train.pkl
2022-12-29 17:40:31.319 DEBUG: Taking gradient step
2022-12-29 17:40:31.329 DEBUG: Loss 0: {'policy_loss': -0.0017884375519105428, 'entropy_loss': -0.03209954313933849, 'vf_loss': 0.01669789573745098, 'total_loss': -0.017190084953798056, 'approx_kl': 3.971702788874154e-08, 'clip_fraction': 0.0, 'grad_norm': 10.442850112915039}
2022-12-29 17:40:32.199 DEBUG: Taking gradient step
2022-12-29 17:40:32.210 DEBUG: Loss 1: {'policy_loss': -0.04182684954737375, 'entropy_loss': -0.0318801598623395, 'vf_loss': 0.014651380973193741, 'total_loss': -0.0590556284365195, 'approx_kl': 0.014665021328255534, 'clip_fraction': 0.06770833395421505, 'grad_norm': 9.731393814086914}
2022-12-29 17:40:33.046 DEBUG: Taking gradient step
2022-12-29 17:40:33.055 DEBUG: Loss 2: {'policy_loss': -0.03614401968027709, 'entropy_loss': -0.03194950707256794, 'vf_loss': 0.014477927975501816, 'total_loss': -0.053615598777343204, 'approx_kl': 0.03546439157798886, 'clip_fraction': 0.20833333395421505, 'grad_norm': 8.421826362609863}
2022-12-29 17:40:33.871 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 17:40:33.871 INFO: Optimization: policy loss=-0.036, vf loss=0.014, entropy loss=-0.032, total loss=-0.054, num steps=3
2022-12-29 17:40:33.871 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:40:35.442 INFO: Evaluation rollout: return=0.651 (0.0), episode length=5.0
2022-12-29 17:40:35.443 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:40:35.447 INFO: Iteration: 36/137, steps: 7776
2022-12-29 17:40:48.625 DEBUG: There is a single atom floating around
2022-12-29 17:40:49.843 DEBUG: Atoms are too close
2022-12-29 17:41:33.509 INFO: Training rollout: return=-0.574 (4.6), episode length=5.0
2022-12-29 17:41:33.510 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:41:33.513 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-7776_train.pkl
2022-12-29 17:41:34.341 DEBUG: Taking gradient step
2022-12-29 17:41:34.351 DEBUG: Loss 0: {'policy_loss': -0.02748890205000768, 'entropy_loss': -0.03219826566055417, 'vf_loss': 0.015728053906166804, 'total_loss': -0.043959113804395045, 'approx_kl': 4.6352700344698405e-08, 'clip_fraction': 0.0, 'grad_norm': 18.824602127075195}
2022-12-29 17:41:35.194 DEBUG: Taking gradient step
2022-12-29 17:41:35.208 DEBUG: Loss 1: {'policy_loss': -0.009389512430558758, 'entropy_loss': -0.032781930174678564, 'vf_loss': 0.017823114338721236, 'total_loss': -0.024348328266516086, 'approx_kl': 0.01101395656587556, 'clip_fraction': 0.041666666977107525, 'grad_norm': 10.64426040649414}
2022-12-29 17:41:36.085 DEBUG: Taking gradient step
2022-12-29 17:41:36.095 DEBUG: Loss 2: {'policy_loss': 0.013980952072729057, 'entropy_loss': -0.033270072657614946, 'vf_loss': 0.020302995630152375, 'total_loss': 0.0010138750452664824, 'approx_kl': 0.03322038729675114, 'clip_fraction': 0.1549479179084301, 'grad_norm': 8.967915534973145}
2022-12-29 17:41:36.884 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 17:41:36.884 INFO: Optimization: policy loss=0.014, vf loss=0.020, entropy loss=-0.033, total loss=0.001, num steps=3
2022-12-29 17:41:36.884 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:41:38.387 INFO: Evaluation rollout: return=0.631 (0.0), episode length=5.0
2022-12-29 17:41:38.388 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:41:38.390 INFO: Iteration: 37/137, steps: 7992
2022-12-29 17:41:53.810 DEBUG: There is a single atom floating around
2022-12-29 17:42:04.858 DEBUG: Atoms are too close
2022-12-29 17:42:36.621 INFO: Training rollout: return=-0.578 (4.6), episode length=5.0
2022-12-29 17:42:36.622 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:42:36.625 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-7992_train.pkl
2022-12-29 17:42:37.459 DEBUG: Taking gradient step
2022-12-29 17:42:37.468 DEBUG: Loss 0: {'policy_loss': -0.03901273932584724, 'entropy_loss': -0.0321597452275455, 'vf_loss': 0.014608976858882053, 'total_loss': -0.056563507694510684, 'approx_kl': 1.4939966597182774e-08, 'clip_fraction': 0.0, 'grad_norm': 14.38935661315918}
2022-12-29 17:42:38.301 DEBUG: Taking gradient step
2022-12-29 17:42:38.311 DEBUG: Loss 1: {'policy_loss': 0.036326625870704184, 'entropy_loss': -0.032619474455714226, 'vf_loss': 0.018936681575735234, 'total_loss': 0.02264383299072519, 'approx_kl': 0.007260310172569007, 'clip_fraction': 0.07682291697710752, 'grad_norm': 22.812702178955078}
2022-12-29 17:42:39.211 DEBUG: Taking gradient step
2022-12-29 17:42:39.225 DEBUG: Loss 2: {'policy_loss': -0.030858831972930417, 'entropy_loss': -0.03217317583039403, 'vf_loss': 0.01451336986444581, 'total_loss': -0.04851863793887863, 'approx_kl': 0.009917798801325262, 'clip_fraction': 0.1953125, 'grad_norm': 15.257740020751953}
2022-12-29 17:42:40.127 DEBUG: Taking gradient step
2022-12-29 17:42:40.137 DEBUG: Loss 3: {'policy_loss': -0.0022926416349740835, 'entropy_loss': -0.032486523035913706, 'vf_loss': 0.016740687113242017, 'total_loss': -0.01803847755764577, 'approx_kl': 0.01861381344497204, 'clip_fraction': 0.2356770858168602, 'grad_norm': 10.728577613830566}
2022-12-29 17:42:40.964 DEBUG: Taking gradient step
2022-12-29 17:42:40.975 DEBUG: Loss 4: {'policy_loss': 0.02590192359167619, 'entropy_loss': -0.03206727979704738, 'vf_loss': 0.018827402655465016, 'total_loss': 0.012662046450093833, 'approx_kl': 0.02816059999167919, 'clip_fraction': 0.2083333358168602, 'grad_norm': 11.85481071472168}
2022-12-29 17:42:41.773 DEBUG: Taking gradient step
2022-12-29 17:42:41.782 DEBUG: Loss 5: {'policy_loss': -0.018896539735921324, 'entropy_loss': -0.03102514101192355, 'vf_loss': 0.016725488371338543, 'total_loss': -0.03319619237650633, 'approx_kl': 0.012165915220975876, 'clip_fraction': 0.1940104179084301, 'grad_norm': 8.654678344726562}
2022-12-29 17:42:42.604 DEBUG: Taking gradient step
2022-12-29 17:42:42.614 DEBUG: Loss 6: {'policy_loss': -0.01704895748473567, 'entropy_loss': -0.03162813792005181, 'vf_loss': 0.01665552479189522, 'total_loss': -0.03202157061289226, 'approx_kl': 0.013835335848852992, 'clip_fraction': 0.3125, 'grad_norm': 4.3259172439575195}
2022-12-29 17:42:43.409 DEBUG: Taking gradient step
2022-12-29 17:42:43.418 DEBUG: Loss 7: {'policy_loss': 0.046981522687304614, 'entropy_loss': -0.03041877131909132, 'vf_loss': 0.020815471398538496, 'total_loss': 0.03737822276675179, 'approx_kl': 0.01977865770459175, 'clip_fraction': 0.3515625, 'grad_norm': 6.403367519378662}
2022-12-29 17:42:44.248 DEBUG: Taking gradient step
2022-12-29 17:42:44.258 DEBUG: Loss 8: {'policy_loss': -0.022344621342388068, 'entropy_loss': -0.030757087282836437, 'vf_loss': 0.016676017963193283, 'total_loss': -0.03642569066203122, 'approx_kl': 0.013365806080400944, 'clip_fraction': 0.3307291716337204, 'grad_norm': 4.232257843017578}
2022-12-29 17:42:45.097 DEBUG: Taking gradient step
2022-12-29 17:42:45.108 DEBUG: Loss 9: {'policy_loss': -0.051364297077999985, 'entropy_loss': -0.030039316043257713, 'vf_loss': 0.014262066850583146, 'total_loss': -0.06714154627067456, 'approx_kl': 0.0273151732981205, 'clip_fraction': 0.359375, 'grad_norm': 3.509427785873413}
2022-12-29 17:42:45.109 INFO: Optimization: policy loss=-0.051, vf loss=0.014, entropy loss=-0.030, total loss=-0.067, num steps=10
2022-12-29 17:42:45.109 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:42:46.578 INFO: Evaluation rollout: return=0.646 (0.0), episode length=5.0
2022-12-29 17:42:46.579 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:42:46.582 INFO: Iteration: 38/137, steps: 8208
2022-12-29 17:43:34.129 DEBUG: Atoms are too close
2022-12-29 17:43:45.118 INFO: Training rollout: return=-0.006 (3.3), episode length=5.0
2022-12-29 17:43:45.119 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:43:45.122 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-8208_train.pkl
2022-12-29 17:43:45.955 DEBUG: Taking gradient step
2022-12-29 17:43:45.964 DEBUG: Loss 0: {'policy_loss': -0.02812665206027013, 'entropy_loss': -0.02970610884949565, 'vf_loss': 0.008892089718131393, 'total_loss': -0.04894067119163438, 'approx_kl': 3.138363080523732e-08, 'clip_fraction': 0.0, 'grad_norm': 13.261046409606934}
2022-12-29 17:43:46.850 DEBUG: Taking gradient step
2022-12-29 17:43:46.864 DEBUG: Loss 1: {'policy_loss': 0.05585504686178008, 'entropy_loss': -0.029461362399160862, 'vf_loss': 0.013435984302566697, 'total_loss': 0.03982966876518592, 'approx_kl': 0.0033580363960936666, 'clip_fraction': 0.12239583395421505, 'grad_norm': 21.34923553466797}
2022-12-29 17:43:47.696 DEBUG: Taking gradient step
2022-12-29 17:43:47.706 DEBUG: Loss 2: {'policy_loss': 0.0011242235858146075, 'entropy_loss': -0.029793596360832453, 'vf_loss': 0.011160527578526133, 'total_loss': -0.017508845196491714, 'approx_kl': 0.044618866057135165, 'clip_fraction': 0.3072916716337204, 'grad_norm': 4.708446025848389}
2022-12-29 17:43:48.555 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 17:43:48.555 INFO: Optimization: policy loss=0.001, vf loss=0.011, entropy loss=-0.030, total loss=-0.018, num steps=3
2022-12-29 17:43:48.555 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:43:50.014 INFO: Evaluation rollout: return=0.620 (0.0), episode length=5.0
2022-12-29 17:43:50.015 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:43:50.018 INFO: Iteration: 39/137, steps: 8424
2022-12-29 17:44:04.511 DEBUG: There is a single atom floating around
2022-12-29 17:44:18.008 DEBUG: There is a single atom floating around
2022-12-29 17:44:48.074 INFO: Training rollout: return=-0.577 (4.6), episode length=5.0
2022-12-29 17:44:48.076 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:44:48.078 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-8424_train.pkl
2022-12-29 17:44:48.899 DEBUG: Taking gradient step
2022-12-29 17:44:48.909 DEBUG: Loss 0: {'policy_loss': -0.006138561406517187, 'entropy_loss': -0.03003031900152564, 'vf_loss': 0.016566198544455808, 'total_loss': -0.019602681863587018, 'approx_kl': 9.212332585661898e-08, 'clip_fraction': 0.0, 'grad_norm': 29.369104385375977}
2022-12-29 17:44:49.731 DEBUG: Taking gradient step
2022-12-29 17:44:49.745 DEBUG: Loss 1: {'policy_loss': -0.0345029564729822, 'entropy_loss': -0.03145498875528574, 'vf_loss': 0.014046087653839677, 'total_loss': -0.05191185757442826, 'approx_kl': 0.00025310879573225975, 'clip_fraction': 0.018229166977107525, 'grad_norm': 13.368329048156738}
2022-12-29 17:44:50.665 DEBUG: Taking gradient step
2022-12-29 17:44:50.680 DEBUG: Loss 2: {'policy_loss': -0.035524399084742656, 'entropy_loss': -0.030436554923653603, 'vf_loss': 0.01396696526186226, 'total_loss': -0.05199398874653399, 'approx_kl': -0.003031803877092898, 'clip_fraction': 0.1497395858168602, 'grad_norm': 6.633358955383301}
2022-12-29 17:44:51.518 DEBUG: Taking gradient step
2022-12-29 17:44:51.527 DEBUG: Loss 3: {'policy_loss': -0.0507797961641907, 'entropy_loss': -0.029660074040293694, 'vf_loss': 0.014067894715354046, 'total_loss': -0.06637197548913035, 'approx_kl': 0.007183782057836652, 'clip_fraction': 0.1822916679084301, 'grad_norm': 5.365614414215088}
2022-12-29 17:44:52.328 DEBUG: Taking gradient step
2022-12-29 17:44:52.338 DEBUG: Loss 4: {'policy_loss': 0.007786664056050781, 'entropy_loss': -0.030813963618129492, 'vf_loss': 0.018674141010736167, 'total_loss': -0.004353158551342544, 'approx_kl': 0.013960749376565218, 'clip_fraction': 0.2643229216337204, 'grad_norm': 2.302579164505005}
2022-12-29 17:44:53.163 DEBUG: Taking gradient step
2022-12-29 17:44:53.172 DEBUG: Loss 5: {'policy_loss': 0.060817580731321624, 'entropy_loss': -0.030575068201869726, 'vf_loss': 0.02324994618552502, 'total_loss': 0.053492458714976904, 'approx_kl': 0.014783818274736404, 'clip_fraction': 0.3567708358168602, 'grad_norm': 4.676233291625977}
2022-12-29 17:44:53.991 DEBUG: Taking gradient step
2022-12-29 17:44:54.007 DEBUG: Loss 6: {'policy_loss': -0.04831909050809026, 'entropy_loss': -0.03220592299476266, 'vf_loss': 0.013932151223989797, 'total_loss': -0.06659286227886313, 'approx_kl': 0.03873871918767691, 'clip_fraction': 0.4127604216337204, 'grad_norm': 2.630495309829712}
2022-12-29 17:44:54.889 DEBUG: Early stopping at step 7 for reaching max KL.
2022-12-29 17:44:54.889 INFO: Optimization: policy loss=-0.048, vf loss=0.014, entropy loss=-0.032, total loss=-0.067, num steps=7
2022-12-29 17:44:54.890 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:44:56.387 INFO: Evaluation rollout: return=0.657 (0.0), episode length=5.0
2022-12-29 17:44:56.389 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:44:56.391 INFO: Iteration: 40/137, steps: 8640
2022-12-29 17:45:55.625 INFO: Training rollout: return=0.544 (0.1), episode length=5.0
2022-12-29 17:45:55.626 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:45:55.629 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-8640_train.pkl
2022-12-29 17:45:56.453 DEBUG: Taking gradient step
2022-12-29 17:45:56.463 DEBUG: Loss 0: {'policy_loss': 0.025729435247303714, 'entropy_loss': -0.0336249191313982, 'vf_loss': 0.0013416210231133921, 'total_loss': -0.0065538628609810995, 'approx_kl': 1.6317548201527643e-08, 'clip_fraction': 0.0, 'grad_norm': 18.419212341308594}
2022-12-29 17:45:57.292 DEBUG: Taking gradient step
2022-12-29 17:45:57.309 DEBUG: Loss 1: {'policy_loss': 0.013903339381692657, 'entropy_loss': -0.03250470804050565, 'vf_loss': 0.0013026127483231043, 'total_loss': -0.017298755910489877, 'approx_kl': 0.004780076211318374, 'clip_fraction': 0.045572916977107525, 'grad_norm': 16.574705123901367}
2022-12-29 17:45:58.136 DEBUG: Taking gradient step
2022-12-29 17:45:58.150 DEBUG: Loss 2: {'policy_loss': 0.020479743951013044, 'entropy_loss': -0.0338964881375432, 'vf_loss': 0.0012331598651302528, 'total_loss': -0.0121835843213999, 'approx_kl': 0.02265556389465928, 'clip_fraction': 0.16015625, 'grad_norm': 16.248218536376953}
2022-12-29 17:45:59.038 DEBUG: Taking gradient step
2022-12-29 17:45:59.048 DEBUG: Loss 3: {'policy_loss': 0.02907891365163487, 'entropy_loss': -0.03555015381425619, 'vf_loss': 0.0011778559917039158, 'total_loss': -0.005293384170917417, 'approx_kl': 0.04467120091430843, 'clip_fraction': 0.2838541716337204, 'grad_norm': 14.86970329284668}
2022-12-29 17:45:59.855 DEBUG: Taking gradient step
2022-12-29 17:45:59.864 DEBUG: Loss 4: {'policy_loss': 0.010346033848278022, 'entropy_loss': -0.03403467684984207, 'vf_loss': 0.0011684962530622805, 'total_loss': -0.022520146748501772, 'approx_kl': 0.03727436065673828, 'clip_fraction': 0.3177083358168602, 'grad_norm': 18.82473373413086}
2022-12-29 17:46:00.682 DEBUG: Early stopping at step 5 for reaching max KL.
2022-12-29 17:46:00.682 INFO: Optimization: policy loss=0.010, vf loss=0.001, entropy loss=-0.034, total loss=-0.023, num steps=5
2022-12-29 17:46:00.683 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:46:02.141 INFO: Evaluation rollout: return=0.645 (0.0), episode length=5.0
2022-12-29 17:46:02.142 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:46:02.145 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-6696.model
2022-12-29 17:46:02.149 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-8856.model
2022-12-29 17:46:02.179 INFO: Iteration: 41/137, steps: 8856
2022-12-29 17:46:50.569 DEBUG: Atoms are too close
2022-12-29 17:46:50.570 DEBUG: Atoms are too close
2022-12-29 17:47:00.839 INFO: Training rollout: return=-0.587 (4.6), episode length=5.0
2022-12-29 17:47:00.840 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:47:00.843 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-8856_train.pkl
2022-12-29 17:47:01.677 DEBUG: Taking gradient step
2022-12-29 17:47:01.687 DEBUG: Loss 0: {'policy_loss': 0.059900690272271795, 'entropy_loss': -0.03452111408114433, 'vf_loss': 0.02250412491874889, 'total_loss': 0.04788370110987636, 'approx_kl': -1.459072151988039e-08, 'clip_fraction': 0.0, 'grad_norm': 21.351457595825195}
2022-12-29 17:47:02.514 DEBUG: Taking gradient step
2022-12-29 17:47:02.523 DEBUG: Loss 1: {'policy_loss': -0.0398271758388279, 'entropy_loss': -0.03442742209881544, 'vf_loss': 0.015330263190603415, 'total_loss': -0.058924334747039925, 'approx_kl': 0.000945923151448369, 'clip_fraction': 0.07291666697710752, 'grad_norm': 8.791937828063965}
2022-12-29 17:47:03.417 DEBUG: Taking gradient step
2022-12-29 17:47:03.427 DEBUG: Loss 2: {'policy_loss': -0.01949984893007263, 'entropy_loss': -0.03498735558241606, 'vf_loss': 0.017850553266985477, 'total_loss': -0.03663665124550321, 'approx_kl': 0.02479403931647539, 'clip_fraction': 0.2434895858168602, 'grad_norm': 9.465326309204102}
2022-12-29 17:47:04.239 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 17:47:04.239 INFO: Optimization: policy loss=-0.019, vf loss=0.018, entropy loss=-0.035, total loss=-0.037, num steps=3
2022-12-29 17:47:04.240 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:47:05.703 INFO: Evaluation rollout: return=0.656 (0.0), episode length=5.0
2022-12-29 17:47:05.704 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:47:05.707 INFO: Iteration: 42/137, steps: 9072
2022-12-29 17:47:17.170 DEBUG: Atoms are too close
2022-12-29 17:47:51.631 DEBUG: Atoms are too close
2022-12-29 17:48:03.598 INFO: Training rollout: return=-0.583 (4.6), episode length=5.0
2022-12-29 17:48:03.600 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:48:03.602 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-9072_train.pkl
2022-12-29 17:48:04.455 DEBUG: Taking gradient step
2022-12-29 17:48:04.465 DEBUG: Loss 0: {'policy_loss': -0.03204878210719329, 'entropy_loss': -0.035372556652873755, 'vf_loss': 0.013828353434456805, 'total_loss': -0.05359298532561024, 'approx_kl': -1.9208528101444244e-08, 'clip_fraction': 0.0, 'grad_norm': 21.62188148498535}
2022-12-29 17:48:05.272 DEBUG: Taking gradient step
2022-12-29 17:48:05.283 DEBUG: Loss 1: {'policy_loss': 0.022841281075062038, 'entropy_loss': -0.034973474219441414, 'vf_loss': 0.018725954621868768, 'total_loss': 0.006593761477489399, 'approx_kl': 0.019161499571055174, 'clip_fraction': 0.09114583395421505, 'grad_norm': 31.59233283996582}
2022-12-29 17:48:06.176 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 17:48:06.176 INFO: Optimization: policy loss=0.023, vf loss=0.019, entropy loss=-0.035, total loss=0.007, num steps=2
2022-12-29 17:48:06.176 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:48:07.620 INFO: Evaluation rollout: return=0.658 (0.0), episode length=5.0
2022-12-29 17:48:07.621 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:48:07.623 INFO: Iteration: 43/137, steps: 9288
2022-12-29 17:48:38.119 DEBUG: Atoms are too close
2022-12-29 17:48:53.322 DEBUG: Atoms are too close
2022-12-29 17:49:05.513 INFO: Training rollout: return=-0.597 (4.6), episode length=5.0
2022-12-29 17:49:05.514 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:49:05.517 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-9288_train.pkl
2022-12-29 17:49:06.359 DEBUG: Taking gradient step
2022-12-29 17:49:06.369 DEBUG: Loss 0: {'policy_loss': -0.03117118587539416, 'entropy_loss': -0.03455803357064724, 'vf_loss': 0.015311638728791146, 'total_loss': -0.050417580717250256, 'approx_kl': -3.3527612686157227e-08, 'clip_fraction': 0.0, 'grad_norm': 10.780379295349121}
2022-12-29 17:49:07.244 DEBUG: Taking gradient step
2022-12-29 17:49:07.255 DEBUG: Loss 1: {'policy_loss': -0.015203026999771276, 'entropy_loss': -0.03499339334666729, 'vf_loss': 0.017833718205505587, 'total_loss': -0.03236270214093298, 'approx_kl': 0.024356022011488676, 'clip_fraction': 0.1901041716337204, 'grad_norm': 8.358338356018066}
2022-12-29 17:49:08.044 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 17:49:08.044 INFO: Optimization: policy loss=-0.015, vf loss=0.018, entropy loss=-0.035, total loss=-0.032, num steps=2
2022-12-29 17:49:08.044 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:49:09.476 INFO: Evaluation rollout: return=0.661 (0.0), episode length=5.0
2022-12-29 17:49:09.477 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:49:09.479 INFO: Iteration: 44/137, steps: 9504
2022-12-29 17:49:42.199 DEBUG: Atoms are too close
2022-12-29 17:50:08.144 INFO: Training rollout: return=-0.001 (3.3), episode length=5.0
2022-12-29 17:50:08.145 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:50:08.147 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-9504_train.pkl
2022-12-29 17:50:08.964 DEBUG: Taking gradient step
2022-12-29 17:50:08.979 DEBUG: Loss 0: {'policy_loss': 0.013823331304509199, 'entropy_loss': -0.034171346575021744, 'vf_loss': 0.01037437994556658, 'total_loss': -0.009973635324945967, 'approx_kl': -5.8052441076483774e-08, 'clip_fraction': 0.0, 'grad_norm': 4.203898906707764}
2022-12-29 17:50:09.859 DEBUG: Taking gradient step
2022-12-29 17:50:09.869 DEBUG: Loss 1: {'policy_loss': -0.028314049024283958, 'entropy_loss': -0.03451681509613991, 'vf_loss': 0.008092372419940402, 'total_loss': -0.05473849170048346, 'approx_kl': 0.004209258011542261, 'clip_fraction': 0.02734375, 'grad_norm': 5.8371076583862305}
2022-12-29 17:50:10.714 DEBUG: Taking gradient step
2022-12-29 17:50:10.725 DEBUG: Loss 2: {'policy_loss': 0.003264485265411343, 'entropy_loss': -0.03575541079044342, 'vf_loss': 0.010543947113555215, 'total_loss': -0.021946978411476865, 'approx_kl': 0.018624673131853342, 'clip_fraction': 0.2473958358168602, 'grad_norm': 7.97209358215332}
2022-12-29 17:50:11.606 DEBUG: Taking gradient step
2022-12-29 17:50:11.616 DEBUG: Loss 3: {'policy_loss': -0.0362545558185179, 'entropy_loss': -0.03507088776677847, 'vf_loss': 0.008067550972162881, 'total_loss': -0.06325789261313348, 'approx_kl': 0.04473949782550335, 'clip_fraction': 0.4205729216337204, 'grad_norm': 3.02350115776062}
2022-12-29 17:50:12.412 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 17:50:12.412 INFO: Optimization: policy loss=-0.036, vf loss=0.008, entropy loss=-0.035, total loss=-0.063, num steps=4
2022-12-29 17:50:12.413 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:50:13.903 INFO: Evaluation rollout: return=0.691 (0.0), episode length=5.0
2022-12-29 17:50:13.904 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:50:13.906 INFO: Iteration: 45/137, steps: 9720
2022-12-29 17:50:16.926 DEBUG: There is a single atom floating around
2022-12-29 17:50:21.585 DEBUG: There is a single atom floating around
2022-12-29 17:50:24.076 DEBUG: Atoms are too close
2022-12-29 17:50:28.448 DEBUG: Atoms are too close
2022-12-29 17:50:39.205 DEBUG: Atoms are too close
2022-12-29 17:50:41.496 DEBUG: Atoms are too close
2022-12-29 17:51:10.612 INFO: Training rollout: return=-2.653 (7.4), episode length=4.8
2022-12-29 17:51:10.613 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:51:10.616 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-9720_train.pkl
2022-12-29 17:51:11.437 DEBUG: Taking gradient step
2022-12-29 17:51:11.446 DEBUG: Loss 0: {'policy_loss': 0.009921829918454821, 'entropy_loss': -0.03655695542693138, 'vf_loss': 0.04136041041159566, 'total_loss': 0.0147252849031191, 'approx_kl': 1.4474305309875035e-08, 'clip_fraction': 0.0, 'grad_norm': 12.247076988220215}
2022-12-29 17:51:12.253 DEBUG: Taking gradient step
2022-12-29 17:51:12.263 DEBUG: Loss 1: {'policy_loss': -0.014557876804772604, 'entropy_loss': -0.035243318416178226, 'vf_loss': 0.03897984420289173, 'total_loss': -0.010821351018059103, 'approx_kl': 0.013094402616843581, 'clip_fraction': 0.045572916977107525, 'grad_norm': 14.475434303283691}
2022-12-29 17:51:13.095 DEBUG: Taking gradient step
2022-12-29 17:51:13.107 DEBUG: Loss 2: {'policy_loss': 0.013865940033554233, 'entropy_loss': -0.03612060099840164, 'vf_loss': 0.04106759217327891, 'total_loss': 0.0188129312084315, 'approx_kl': 0.037871934939175844, 'clip_fraction': 0.16145833395421505, 'grad_norm': 17.86968231201172}
2022-12-29 17:51:14.007 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 17:51:14.007 INFO: Optimization: policy loss=0.014, vf loss=0.041, entropy loss=-0.036, total loss=0.019, num steps=3
2022-12-29 17:51:14.008 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:51:15.445 INFO: Evaluation rollout: return=0.711 (0.0), episode length=5.0
2022-12-29 17:51:15.446 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:51:15.448 INFO: Iteration: 46/137, steps: 9936
2022-12-29 17:51:29.464 DEBUG: There is a single atom floating around
2022-12-29 17:51:45.118 DEBUG: Atoms are too close
2022-12-29 17:52:03.116 DEBUG: Atoms are too close
2022-12-29 17:52:13.796 INFO: Training rollout: return=-1.099 (5.6), episode length=5.0
2022-12-29 17:52:13.797 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:52:13.800 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-9936_train.pkl
2022-12-29 17:52:14.644 DEBUG: Taking gradient step
2022-12-29 17:52:14.653 DEBUG: Loss 0: {'policy_loss': 0.005437950218397875, 'entropy_loss': -0.03456926019862294, 'vf_loss': 0.027161581484540464, 'total_loss': -0.001969728495684603, 'approx_kl': 2.9103830456733704e-08, 'clip_fraction': 0.0, 'grad_norm': 11.955666542053223}
2022-12-29 17:52:15.466 DEBUG: Taking gradient step
2022-12-29 17:52:15.475 DEBUG: Loss 1: {'policy_loss': 0.022421621550086028, 'entropy_loss': -0.03529610298573971, 'vf_loss': 0.029763884573493375, 'total_loss': 0.016889403137839695, 'approx_kl': 0.006113694398663938, 'clip_fraction': 0.06510416697710752, 'grad_norm': 14.155230522155762}
2022-12-29 17:52:16.280 DEBUG: Taking gradient step
2022-12-29 17:52:16.294 DEBUG: Loss 2: {'policy_loss': -0.05800874419991481, 'entropy_loss': -0.03496107365936041, 'vf_loss': 0.022555707811491663, 'total_loss': -0.07041411004778356, 'approx_kl': 0.027100930456072092, 'clip_fraction': 0.265625, 'grad_norm': 7.534643173217773}
2022-12-29 17:52:17.193 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 17:52:17.193 INFO: Optimization: policy loss=-0.058, vf loss=0.023, entropy loss=-0.035, total loss=-0.070, num steps=3
2022-12-29 17:52:17.194 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:52:18.586 INFO: Evaluation rollout: return=0.733 (0.0), episode length=5.0
2022-12-29 17:52:18.587 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:52:18.590 INFO: Iteration: 47/137, steps: 10152
2022-12-29 17:52:34.681 DEBUG: There is a single atom floating around
2022-12-29 17:53:17.572 INFO: Training rollout: return=0.012 (3.3), episode length=5.0
2022-12-29 17:53:17.573 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:53:17.575 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-10152_train.pkl
2022-12-29 17:53:18.363 DEBUG: Taking gradient step
2022-12-29 17:53:18.373 DEBUG: Loss 0: {'policy_loss': -0.031160151435700726, 'entropy_loss': -0.03397660609334707, 'vf_loss': 0.00810069886371707, 'total_loss': -0.05703605866533073, 'approx_kl': 5.362866062341709e-08, 'clip_fraction': 0.0, 'grad_norm': 15.78420352935791}
2022-12-29 17:53:19.191 DEBUG: Taking gradient step
2022-12-29 17:53:19.201 DEBUG: Loss 1: {'policy_loss': -0.02388774708684107, 'entropy_loss': -0.03297033300623298, 'vf_loss': 0.008054507703983848, 'total_loss': -0.0488035723890902, 'approx_kl': 0.011375210131518543, 'clip_fraction': 0.0234375, 'grad_norm': 15.734941482543945}
2022-12-29 17:53:20.055 DEBUG: Taking gradient step
2022-12-29 17:53:20.064 DEBUG: Loss 2: {'policy_loss': 0.0796141110433868, 'entropy_loss': -0.033131216652691364, 'vf_loss': 0.015256206478928572, 'total_loss': 0.061739100869624, 'approx_kl': 0.04348524799570441, 'clip_fraction': 0.19140625, 'grad_norm': 15.653571128845215}
2022-12-29 17:53:20.895 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 17:53:20.896 INFO: Optimization: policy loss=0.080, vf loss=0.015, entropy loss=-0.033, total loss=0.062, num steps=3
2022-12-29 17:53:20.896 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:53:22.292 INFO: Evaluation rollout: return=0.736 (0.0), episode length=5.0
2022-12-29 17:53:22.293 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:53:22.296 INFO: Iteration: 48/137, steps: 10368
2022-12-29 17:54:18.427 DEBUG: Atoms are too close
2022-12-29 17:54:20.788 INFO: Training rollout: return=0.051 (3.3), episode length=4.9
2022-12-29 17:54:20.789 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:54:20.792 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-10368_train.pkl
2022-12-29 17:54:21.624 DEBUG: Taking gradient step
2022-12-29 17:54:21.633 DEBUG: Loss 0: {'policy_loss': -0.01269961600521629, 'entropy_loss': -0.032016283832490444, 'vf_loss': 0.005293218480395787, 'total_loss': -0.03942268135731095, 'approx_kl': 4.710940260110874e-08, 'clip_fraction': 0.0, 'grad_norm': 11.405579566955566}
2022-12-29 17:54:22.443 DEBUG: Taking gradient step
2022-12-29 17:54:22.457 DEBUG: Loss 1: {'policy_loss': 0.037971312596890266, 'entropy_loss': -0.03341084020212293, 'vf_loss': 0.007725400846036493, 'total_loss': 0.012285873240803831, 'approx_kl': -0.013929091510362923, 'clip_fraction': 0.0807291679084301, 'grad_norm': 27.385820388793945}
2022-12-29 17:54:23.345 DEBUG: Taking gradient step
2022-12-29 17:54:23.358 DEBUG: Loss 2: {'policy_loss': -0.02529998493353601, 'entropy_loss': -0.031194244045764208, 'vf_loss': 0.005324624702817527, 'total_loss': -0.05116960427648269, 'approx_kl': -0.004690137458965182, 'clip_fraction': 0.11588541697710752, 'grad_norm': 1.6006228923797607}
2022-12-29 17:54:24.238 DEBUG: Taking gradient step
2022-12-29 17:54:24.248 DEBUG: Loss 3: {'policy_loss': -0.03091219963209913, 'entropy_loss': -0.030116799287497997, 'vf_loss': 0.0053156827799332815, 'total_loss': -0.05571331613966385, 'approx_kl': -0.0016954573802649975, 'clip_fraction': 0.2018229216337204, 'grad_norm': 1.6048851013183594}
2022-12-29 17:54:25.097 DEBUG: Taking gradient step
2022-12-29 17:54:25.106 DEBUG: Loss 4: {'policy_loss': 0.022777097488672203, 'entropy_loss': -0.03021911671385169, 'vf_loss': 0.007671148957950302, 'total_loss': 0.00022912973277081083, 'approx_kl': -0.011914010159671307, 'clip_fraction': 0.2122395858168602, 'grad_norm': 1.6611759662628174}
2022-12-29 17:54:25.880 DEBUG: Taking gradient step
2022-12-29 17:54:25.891 DEBUG: Loss 5: {'policy_loss': -0.030712130466092023, 'entropy_loss': -0.030155392363667488, 'vf_loss': 0.005189695430553552, 'total_loss': -0.05567782739920596, 'approx_kl': 0.00896622589789331, 'clip_fraction': 0.2330729179084301, 'grad_norm': 1.284962773323059}
2022-12-29 17:54:26.737 DEBUG: Taking gradient step
2022-12-29 17:54:26.750 DEBUG: Loss 6: {'policy_loss': -0.029450475506178624, 'entropy_loss': -0.03141578659415245, 'vf_loss': 0.005091780108149979, 'total_loss': -0.05577448199218109, 'approx_kl': 0.026641446631401777, 'clip_fraction': 0.2395833358168602, 'grad_norm': 1.7024279832839966}
2022-12-29 17:54:27.685 DEBUG: Taking gradient step
2022-12-29 17:54:27.695 DEBUG: Loss 7: {'policy_loss': -0.032271493170378446, 'entropy_loss': -0.03192595764994621, 'vf_loss': 0.0050423567461799, 'total_loss': -0.05915509407414476, 'approx_kl': 0.039729442447423935, 'clip_fraction': 0.20703125, 'grad_norm': 1.3882979154586792}
2022-12-29 17:54:28.478 DEBUG: Early stopping at step 8 for reaching max KL.
2022-12-29 17:54:28.478 INFO: Optimization: policy loss=-0.032, vf loss=0.005, entropy loss=-0.032, total loss=-0.059, num steps=8
2022-12-29 17:54:28.478 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:54:29.965 INFO: Evaluation rollout: return=0.741 (0.0), episode length=5.0
2022-12-29 17:54:29.967 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:54:29.970 INFO: Iteration: 49/137, steps: 10584
2022-12-29 17:55:00.537 DEBUG: Atoms are too close
2022-12-29 17:55:15.487 DEBUG: Atoms are too close
2022-12-29 17:55:28.032 INFO: Training rollout: return=-0.507 (4.6), episode length=5.0
2022-12-29 17:55:28.033 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:55:28.037 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-10584_train.pkl
2022-12-29 17:55:28.859 DEBUG: Taking gradient step
2022-12-29 17:55:28.868 DEBUG: Loss 0: {'policy_loss': 0.000785753899039986, 'entropy_loss': -0.03241351107135415, 'vf_loss': 0.016599909449360118, 'total_loss': -0.015027847722954046, 'approx_kl': 3.111199475824833e-08, 'clip_fraction': 0.0, 'grad_norm': 19.542945861816406}
2022-12-29 17:55:29.730 DEBUG: Taking gradient step
2022-12-29 17:55:29.743 DEBUG: Loss 1: {'policy_loss': 0.02526759912802811, 'entropy_loss': -0.03322614869102836, 'vf_loss': 0.01896045581703561, 'total_loss': 0.011001906254035368, 'approx_kl': 0.01396892499178648, 'clip_fraction': 0.1393229179084301, 'grad_norm': 4.1478800773620605}
2022-12-29 17:55:30.613 DEBUG: Taking gradient step
2022-12-29 17:55:30.626 DEBUG: Loss 2: {'policy_loss': -0.019947282800667984, 'entropy_loss': -0.032424263656139374, 'vf_loss': 0.016742894671188707, 'total_loss': -0.03562865178561865, 'approx_kl': 0.026851407019421458, 'clip_fraction': 0.2721354216337204, 'grad_norm': 6.456012725830078}
2022-12-29 17:55:31.464 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 17:55:31.465 INFO: Optimization: policy loss=-0.020, vf loss=0.017, entropy loss=-0.032, total loss=-0.036, num steps=3
2022-12-29 17:55:31.465 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:55:32.936 INFO: Evaluation rollout: return=0.741 (0.0), episode length=5.0
2022-12-29 17:55:32.937 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:55:32.940 INFO: Iteration: 50/137, steps: 10800
2022-12-29 17:55:45.745 DEBUG: There is a single atom floating around
2022-12-29 17:55:47.514 DEBUG: There is a single atom floating around
2022-12-29 17:56:17.587 DEBUG: There is a single atom floating around
2022-12-29 17:56:30.852 INFO: Training rollout: return=-1.104 (5.6), episode length=5.0
2022-12-29 17:56:30.854 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:56:30.856 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-10800_train.pkl
2022-12-29 17:56:31.692 DEBUG: Taking gradient step
2022-12-29 17:56:31.704 DEBUG: Loss 0: {'policy_loss': -0.04561682372982414, 'entropy_loss': -0.03160927724093199, 'vf_loss': 0.021648122176620162, 'total_loss': -0.055577978794135965, 'approx_kl': -1.955777406692505e-08, 'clip_fraction': 0.0, 'grad_norm': 15.873025894165039}
2022-12-29 17:56:32.528 DEBUG: Taking gradient step
2022-12-29 17:56:32.539 DEBUG: Loss 1: {'policy_loss': 0.008241093440726205, 'entropy_loss': -0.03290037391707301, 'vf_loss': 0.026737105008265274, 'total_loss': 0.0020778245319184718, 'approx_kl': 0.0020389208802953362, 'clip_fraction': 0.033854166977107525, 'grad_norm': 20.92952537536621}
2022-12-29 17:56:33.366 DEBUG: Taking gradient step
2022-12-29 17:56:33.377 DEBUG: Loss 2: {'policy_loss': 0.03631866128905322, 'entropy_loss': -0.031293417792767286, 'vf_loss': 0.029230360055169404, 'total_loss': 0.03425560355145534, 'approx_kl': 0.02117703959811479, 'clip_fraction': 0.1575520858168602, 'grad_norm': 9.096158027648926}
2022-12-29 17:56:34.177 DEBUG: Taking gradient step
2022-12-29 17:56:34.186 DEBUG: Loss 3: {'policy_loss': -0.020733164305993215, 'entropy_loss': -0.0314391003921628, 'vf_loss': 0.024266380572844807, 'total_loss': -0.027905884125311208, 'approx_kl': 0.03466004366055131, 'clip_fraction': 0.2213541679084301, 'grad_norm': 8.570365905761719}
2022-12-29 17:56:34.990 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 17:56:34.990 INFO: Optimization: policy loss=-0.021, vf loss=0.024, entropy loss=-0.031, total loss=-0.028, num steps=4
2022-12-29 17:56:34.991 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:56:36.461 INFO: Evaluation rollout: return=0.748 (0.0), episode length=5.0
2022-12-29 17:56:36.462 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:56:36.464 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-8856.model
2022-12-29 17:56:36.466 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-11016.model
2022-12-29 17:56:36.495 INFO: Iteration: 51/137, steps: 11016
2022-12-29 17:57:35.408 INFO: Training rollout: return=0.567 (0.1), episode length=5.0
2022-12-29 17:57:35.410 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:57:35.413 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-11016_train.pkl
2022-12-29 17:57:36.240 DEBUG: Taking gradient step
2022-12-29 17:57:36.249 DEBUG: Loss 0: {'policy_loss': -0.021988589460454053, 'entropy_loss': -0.03104071319103241, 'vf_loss': 0.00044737787908882435, 'total_loss': -0.052581924772397634, 'approx_kl': 2.773595042526722e-08, 'clip_fraction': 0.0, 'grad_norm': 15.411225318908691}
2022-12-29 17:57:37.091 DEBUG: Taking gradient step
2022-12-29 17:57:37.105 DEBUG: Loss 1: {'policy_loss': -0.027872688401016414, 'entropy_loss': -0.032117506954818964, 'vf_loss': 0.0004505897445572946, 'total_loss': -0.05953960561127809, 'approx_kl': 0.019088810600806028, 'clip_fraction': 0.08984375, 'grad_norm': 6.517941951751709}
2022-12-29 17:57:37.984 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 17:57:37.984 INFO: Optimization: policy loss=-0.028, vf loss=0.000, entropy loss=-0.032, total loss=-0.060, num steps=2
2022-12-29 17:57:37.985 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:57:39.413 INFO: Evaluation rollout: return=0.756 (0.0), episode length=5.0
2022-12-29 17:57:39.414 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:57:39.417 INFO: Iteration: 52/137, steps: 11232
2022-12-29 17:57:52.088 DEBUG: There is a single atom floating around
2022-12-29 17:58:36.994 INFO: Training rollout: return=0.055 (3.3), episode length=5.0
2022-12-29 17:58:36.998 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:58:37.000 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-11232_train.pkl
2022-12-29 17:58:37.842 DEBUG: Taking gradient step
2022-12-29 17:58:37.852 DEBUG: Loss 0: {'policy_loss': 0.019454046407459246, 'entropy_loss': -0.03197291633114219, 'vf_loss': 0.010854821968279239, 'total_loss': -0.0016640479554037094, 'approx_kl': -1.9111517701730918e-08, 'clip_fraction': 0.0, 'grad_norm': 20.173324584960938}
2022-12-29 17:58:38.675 DEBUG: Taking gradient step
2022-12-29 17:58:38.686 DEBUG: Loss 1: {'policy_loss': -0.04291686129388389, 'entropy_loss': -0.030370984692126513, 'vf_loss': 0.008185791487378679, 'total_loss': -0.06510205449863173, 'approx_kl': 0.0022533524315804243, 'clip_fraction': 0.03125, 'grad_norm': 10.548562049865723}
2022-12-29 17:58:39.524 DEBUG: Taking gradient step
2022-12-29 17:58:39.534 DEBUG: Loss 2: {'policy_loss': -0.04612020100176868, 'entropy_loss': -0.02995141362771392, 'vf_loss': 0.008176074209439062, 'total_loss': -0.06789554042004353, 'approx_kl': 0.017345317639410496, 'clip_fraction': 0.2161458358168602, 'grad_norm': 1.6318937540054321}
2022-12-29 17:58:40.412 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 17:58:40.412 INFO: Optimization: policy loss=-0.046, vf loss=0.008, entropy loss=-0.030, total loss=-0.068, num steps=3
2022-12-29 17:58:40.413 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:58:41.839 INFO: Evaluation rollout: return=0.768 (0.0), episode length=5.0
2022-12-29 17:58:41.840 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:58:41.842 INFO: Iteration: 53/137, steps: 11448
2022-12-29 17:59:29.771 DEBUG: Atoms are too close
2022-12-29 17:59:40.909 INFO: Training rollout: return=0.057 (3.3), episode length=5.0
2022-12-29 17:59:40.911 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 17:59:40.913 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-11448_train.pkl
2022-12-29 17:59:41.863 DEBUG: Taking gradient step
2022-12-29 17:59:41.873 DEBUG: Loss 0: {'policy_loss': 0.018126793461849373, 'entropy_loss': -0.03012930741533637, 'vf_loss': 0.010444749730117619, 'total_loss': -0.0015577642233693817, 'approx_kl': -5.300777772276888e-08, 'clip_fraction': 0.0, 'grad_norm': 16.827627182006836}
2022-12-29 17:59:42.740 DEBUG: Taking gradient step
2022-12-29 17:59:42.749 DEBUG: Loss 1: {'policy_loss': -0.029948688543109978, 'entropy_loss': -0.029520473908632994, 'vf_loss': 0.007933347652381113, 'total_loss': -0.05153581479936186, 'approx_kl': 0.008976586163043976, 'clip_fraction': 0.1497395858168602, 'grad_norm': 3.458759307861328}
2022-12-29 17:59:43.527 DEBUG: Taking gradient step
2022-12-29 17:59:43.536 DEBUG: Loss 2: {'policy_loss': -0.0411307429865111, 'entropy_loss': -0.027017555199563503, 'vf_loss': 0.007941348915574827, 'total_loss': -0.06020694927049978, 'approx_kl': 0.04432979738339782, 'clip_fraction': 0.2799479179084301, 'grad_norm': 3.315544843673706}
2022-12-29 17:59:44.409 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 17:59:44.409 INFO: Optimization: policy loss=-0.041, vf loss=0.008, entropy loss=-0.027, total loss=-0.060, num steps=3
2022-12-29 17:59:44.410 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 17:59:45.891 INFO: Evaluation rollout: return=0.772 (0.0), episode length=5.0
2022-12-29 17:59:45.892 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 17:59:45.895 INFO: Iteration: 54/137, steps: 11664
2022-12-29 18:00:01.523 DEBUG: Atoms are too close
2022-12-29 18:00:44.643 INFO: Training rollout: return=0.074 (3.3), episode length=5.0
2022-12-29 18:00:44.645 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:00:44.648 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-11664_train.pkl
2022-12-29 18:00:45.480 DEBUG: Taking gradient step
2022-12-29 18:00:45.489 DEBUG: Loss 0: {'policy_loss': -0.019495172210759432, 'entropy_loss': -0.029086988419294357, 'vf_loss': 0.007893140850796317, 'total_loss': -0.04068901977925747, 'approx_kl': 2.3166649043560028e-08, 'clip_fraction': 0.0, 'grad_norm': 18.531967163085938}
2022-12-29 18:00:46.303 DEBUG: Taking gradient step
2022-12-29 18:00:46.313 DEBUG: Loss 1: {'policy_loss': 0.02142699028200545, 'entropy_loss': -0.027041921857744455, 'vf_loss': 0.010495895054627311, 'total_loss': 0.004880963478888313, 'approx_kl': -0.0055668237619102, 'clip_fraction': 0.03255208395421505, 'grad_norm': 30.597322463989258}
2022-12-29 18:00:47.140 DEBUG: Taking gradient step
2022-12-29 18:00:47.149 DEBUG: Loss 2: {'policy_loss': 0.005131649260691642, 'entropy_loss': -0.02789776399731636, 'vf_loss': 0.01047055552284417, 'total_loss': -0.01229555921378055, 'approx_kl': 0.01178235630504787, 'clip_fraction': 0.053385416977107525, 'grad_norm': 15.464797973632812}
2022-12-29 18:00:47.962 DEBUG: Taking gradient step
2022-12-29 18:00:47.971 DEBUG: Loss 3: {'policy_loss': -0.03277023696229537, 'entropy_loss': -0.02806322928518057, 'vf_loss': 0.007850534680507332, 'total_loss': -0.05298293156696861, 'approx_kl': 0.0187196132610552, 'clip_fraction': 0.14453125, 'grad_norm': 7.866633892059326}
2022-12-29 18:00:48.787 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 18:00:48.787 INFO: Optimization: policy loss=-0.033, vf loss=0.008, entropy loss=-0.028, total loss=-0.053, num steps=4
2022-12-29 18:00:48.788 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:00:50.218 INFO: Evaluation rollout: return=0.765 (0.0), episode length=5.0
2022-12-29 18:00:50.219 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:00:50.223 INFO: Iteration: 55/137, steps: 11880
2022-12-29 18:01:49.621 INFO: Training rollout: return=0.618 (0.1), episode length=5.0
2022-12-29 18:01:49.622 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:01:49.624 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-11880_train.pkl
2022-12-29 18:01:50.457 DEBUG: Taking gradient step
2022-12-29 18:01:50.467 DEBUG: Loss 0: {'policy_loss': 0.032017184985712745, 'entropy_loss': -0.029270693194121122, 'vf_loss': 0.0003357154532216581, 'total_loss': 0.0030822072448132842, 'approx_kl': -1.0426932561813373e-07, 'clip_fraction': 0.0, 'grad_norm': 13.501391410827637}
2022-12-29 18:01:51.300 DEBUG: Taking gradient step
2022-12-29 18:01:51.310 DEBUG: Loss 1: {'policy_loss': 0.043186790732855425, 'entropy_loss': -0.02768968092277646, 'vf_loss': 0.000337144029063358, 'total_loss': 0.01583425383914233, 'approx_kl': 0.03410611767321825, 'clip_fraction': 0.15755208395421505, 'grad_norm': 8.06407356262207}
2022-12-29 18:01:52.106 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 18:01:52.106 INFO: Optimization: policy loss=0.043, vf loss=0.000, entropy loss=-0.028, total loss=0.016, num steps=2
2022-12-29 18:01:52.106 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:01:53.540 INFO: Evaluation rollout: return=0.754 (0.0), episode length=5.0
2022-12-29 18:01:53.541 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:01:53.543 INFO: Iteration: 56/137, steps: 12096
2022-12-29 18:02:26.393 DEBUG: Atoms are too close
2022-12-29 18:02:52.374 INFO: Training rollout: return=0.016 (3.3), episode length=5.0
2022-12-29 18:02:52.375 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:02:52.378 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-12096_train.pkl
2022-12-29 18:02:53.241 DEBUG: Taking gradient step
2022-12-29 18:02:53.255 DEBUG: Loss 0: {'policy_loss': -0.027088163828143094, 'entropy_loss': -0.02913876809179783, 'vf_loss': 0.007845211549064554, 'total_loss': -0.048381720370876374, 'approx_kl': -2.1953990625434017e-08, 'clip_fraction': 0.0, 'grad_norm': 7.364472389221191}
2022-12-29 18:02:54.163 DEBUG: Taking gradient step
2022-12-29 18:02:54.177 DEBUG: Loss 1: {'policy_loss': 0.019620789489908787, 'entropy_loss': -0.029494324699044228, 'vf_loss': 0.010360287678343781, 'total_loss': 0.00048675246920833887, 'approx_kl': 0.0107553256675601, 'clip_fraction': 0.2565104216337204, 'grad_norm': 15.970587730407715}
2022-12-29 18:02:55.018 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 18:02:55.019 INFO: Optimization: policy loss=0.020, vf loss=0.010, entropy loss=-0.029, total loss=0.000, num steps=2
2022-12-29 18:02:55.019 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:02:56.428 INFO: Evaluation rollout: return=0.734 (0.0), episode length=5.0
2022-12-29 18:02:56.429 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:02:56.432 INFO: Iteration: 57/137, steps: 12312
2022-12-29 18:03:10.147 DEBUG: Atoms are too close
2022-12-29 18:03:25.556 DEBUG: Atoms are too close
2022-12-29 18:03:27.368 DEBUG: Atoms are too close
2022-12-29 18:03:54.092 INFO: Training rollout: return=-1.138 (5.6), episode length=5.0
2022-12-29 18:03:54.093 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:03:54.096 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-12312_train.pkl
2022-12-29 18:03:54.984 DEBUG: Taking gradient step
2022-12-29 18:03:54.998 DEBUG: Loss 0: {'policy_loss': -0.04093270723204605, 'entropy_loss': -0.028034218586981297, 'vf_loss': 0.0227133726173167, 'total_loss': -0.04625355320171065, 'approx_kl': -5.201824659906151e-08, 'clip_fraction': 0.0, 'grad_norm': 7.235925674438477}
2022-12-29 18:03:55.847 DEBUG: Taking gradient step
2022-12-29 18:03:55.857 DEBUG: Loss 1: {'policy_loss': -0.02241571805553045, 'entropy_loss': -0.029093258548527956, 'vf_loss': 0.025202758384387267, 'total_loss': -0.02630621821967113, 'approx_kl': 0.04324147867737338, 'clip_fraction': 0.2239583358168602, 'grad_norm': 9.114476203918457}
2022-12-29 18:03:56.720 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 18:03:56.720 INFO: Optimization: policy loss=-0.022, vf loss=0.025, entropy loss=-0.029, total loss=-0.026, num steps=2
2022-12-29 18:03:56.720 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:03:58.189 INFO: Evaluation rollout: return=0.708 (0.0), episode length=5.0
2022-12-29 18:03:58.190 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:03:58.192 INFO: Iteration: 58/137, steps: 12528
2022-12-29 18:04:08.945 DEBUG: Atoms are too close
2022-12-29 18:04:43.002 DEBUG: Atoms are too close
2022-12-29 18:04:56.183 INFO: Training rollout: return=-0.584 (4.6), episode length=4.9
2022-12-29 18:04:56.184 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:04:56.187 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-12528_train.pkl
2022-12-29 18:04:57.018 DEBUG: Taking gradient step
2022-12-29 18:04:57.028 DEBUG: Loss 0: {'policy_loss': 0.002259966145610299, 'entropy_loss': -0.0274931313470006, 'vf_loss': 0.014908513324312044, 'total_loss': -0.010324651877078251, 'approx_kl': -1.1526572052389383e-07, 'clip_fraction': 0.0, 'grad_norm': 19.417722702026367}
2022-12-29 18:04:57.851 DEBUG: Taking gradient step
2022-12-29 18:04:57.862 DEBUG: Loss 1: {'policy_loss': -0.010874726855174007, 'entropy_loss': -0.026723672170192003, 'vf_loss': 0.015019951431133622, 'total_loss': -0.022578447594232394, 'approx_kl': 0.015913414012175053, 'clip_fraction': 0.05598958395421505, 'grad_norm': 11.181638717651367}
2022-12-29 18:04:58.766 DEBUG: Taking gradient step
2022-12-29 18:04:58.778 DEBUG: Loss 2: {'policy_loss': 0.020591042415375738, 'entropy_loss': -0.02611836465075612, 'vf_loss': 0.01759327106346917, 'total_loss': 0.012065948828088792, 'approx_kl': 0.03017925377935171, 'clip_fraction': 0.1158854179084301, 'grad_norm': 21.562236785888672}
2022-12-29 18:04:59.709 DEBUG: Taking gradient step
2022-12-29 18:04:59.719 DEBUG: Loss 3: {'policy_loss': 0.016187650545627985, 'entropy_loss': -0.025833413004875183, 'vf_loss': 0.01756929655873773, 'total_loss': 0.007923534099490537, 'approx_kl': 0.037668123783078045, 'clip_fraction': 0.2174479179084301, 'grad_norm': 8.82211971282959}
2022-12-29 18:05:00.553 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 18:05:00.553 INFO: Optimization: policy loss=0.016, vf loss=0.018, entropy loss=-0.026, total loss=0.008, num steps=4
2022-12-29 18:05:00.553 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:05:02.002 INFO: Evaluation rollout: return=0.727 (0.0), episode length=5.0
2022-12-29 18:05:02.003 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:05:02.006 INFO: Iteration: 59/137, steps: 12744
2022-12-29 18:05:48.949 DEBUG: Atoms are too close
2022-12-29 18:05:59.608 INFO: Training rollout: return=0.038 (3.3), episode length=5.0
2022-12-29 18:05:59.609 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:05:59.612 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-12744_train.pkl
2022-12-29 18:06:00.418 DEBUG: Taking gradient step
2022-12-29 18:06:00.432 DEBUG: Loss 0: {'policy_loss': -0.020191509698417043, 'entropy_loss': -0.024567211978137493, 'vf_loss': 0.007836111356204812, 'total_loss': -0.03692261032034973, 'approx_kl': 6.072999525486011e-08, 'clip_fraction': 0.0, 'grad_norm': 9.234060287475586}
2022-12-29 18:06:01.256 DEBUG: Taking gradient step
2022-12-29 18:06:01.268 DEBUG: Loss 1: {'policy_loss': -0.035223059029444566, 'entropy_loss': -0.02511863224208355, 'vf_loss': 0.007863478006015185, 'total_loss': -0.05247821326551293, 'approx_kl': 0.0054174140095710754, 'clip_fraction': 0.18098958395421505, 'grad_norm': 6.769989490509033}
2022-12-29 18:06:02.107 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 18:06:02.107 INFO: Optimization: policy loss=-0.035, vf loss=0.008, entropy loss=-0.025, total loss=-0.052, num steps=2
2022-12-29 18:06:02.107 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:06:03.595 INFO: Evaluation rollout: return=0.735 (0.0), episode length=5.0
2022-12-29 18:06:03.596 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:06:03.599 INFO: Iteration: 60/137, steps: 12960
2022-12-29 18:06:51.952 DEBUG: Atoms are too close
2022-12-29 18:07:01.790 INFO: Training rollout: return=0.049 (3.3), episode length=5.0
2022-12-29 18:07:01.791 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:07:01.794 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-12960_train.pkl
2022-12-29 18:07:02.650 DEBUG: Taking gradient step
2022-12-29 18:07:02.661 DEBUG: Loss 0: {'policy_loss': 0.01670617185071576, 'entropy_loss': -0.024551805574446917, 'vf_loss': 0.010175079197937625, 'total_loss': 0.002329445474206465, 'approx_kl': 7.644608501777839e-09, 'clip_fraction': 0.0, 'grad_norm': 13.023736000061035}
2022-12-29 18:07:03.497 DEBUG: Taking gradient step
2022-12-29 18:07:03.511 DEBUG: Loss 1: {'policy_loss': 0.00897328862173085, 'entropy_loss': -0.023293808102607727, 'vf_loss': 0.010256273221354698, 'total_loss': -0.004064246259522179, 'approx_kl': 0.016761123668402433, 'clip_fraction': 0.1953125, 'grad_norm': 5.901985168457031}
2022-12-29 18:07:04.355 DEBUG: Taking gradient step
2022-12-29 18:07:04.364 DEBUG: Loss 2: {'policy_loss': 0.005695571072073344, 'entropy_loss': -0.023243135772645473, 'vf_loss': 0.010261668653652821, 'total_loss': -0.007285896046919309, 'approx_kl': 0.033385282615199685, 'clip_fraction': 0.3411458358168602, 'grad_norm': 5.3073649406433105}
2022-12-29 18:07:05.145 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 18:07:05.145 INFO: Optimization: policy loss=0.006, vf loss=0.010, entropy loss=-0.023, total loss=-0.007, num steps=3
2022-12-29 18:07:05.146 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:07:06.589 INFO: Evaluation rollout: return=0.725 (0.0), episode length=5.0
2022-12-29 18:07:06.590 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:07:06.593 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-11016.model
2022-12-29 18:07:06.598 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-13176.model
2022-12-29 18:07:06.629 INFO: Iteration: 61/137, steps: 13176
2022-12-29 18:08:05.569 INFO: Training rollout: return=0.613 (0.1), episode length=5.0
2022-12-29 18:08:05.571 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:08:05.573 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-13176_train.pkl
2022-12-29 18:08:06.424 DEBUG: Taking gradient step
2022-12-29 18:08:06.434 DEBUG: Loss 0: {'policy_loss': 0.012503118024160053, 'entropy_loss': -0.022401090245693922, 'vf_loss': 0.00045135398185222207, 'total_loss': -0.00944661823968165, 'approx_kl': 1.317045352777768e-07, 'clip_fraction': 0.0, 'grad_norm': 21.29058837890625}
2022-12-29 18:08:07.242 DEBUG: Taking gradient step
2022-12-29 18:08:07.252 DEBUG: Loss 1: {'policy_loss': -0.0011329033564182293, 'entropy_loss': -0.022290233988314867, 'vf_loss': 0.0004299705273429081, 'total_loss': -0.02299316681739019, 'approx_kl': -0.005622191907605156, 'clip_fraction': 0.07161458395421505, 'grad_norm': 5.2342095375061035}
2022-12-29 18:08:08.150 DEBUG: Taking gradient step
2022-12-29 18:08:08.163 DEBUG: Loss 2: {'policy_loss': 0.03455720030322993, 'entropy_loss': -0.021609578747302294, 'vf_loss': 0.0003936014282002932, 'total_loss': 0.013341222984127932, 'approx_kl': -0.004263992886990309, 'clip_fraction': 0.2630208358168602, 'grad_norm': 18.22974967956543}
2022-12-29 18:08:08.987 DEBUG: Taking gradient step
2022-12-29 18:08:08.996 DEBUG: Loss 3: {'policy_loss': -0.032302755144759845, 'entropy_loss': -0.0218858839944005, 'vf_loss': 0.00039095159925253687, 'total_loss': -0.05379768753990781, 'approx_kl': 0.006158618023619056, 'clip_fraction': 0.3033854216337204, 'grad_norm': 17.448410034179688}
2022-12-29 18:08:09.801 DEBUG: Taking gradient step
2022-12-29 18:08:09.810 DEBUG: Loss 4: {'policy_loss': 0.017066871172718934, 'entropy_loss': -0.020736321806907654, 'vf_loss': 0.00036910837566201884, 'total_loss': -0.003300342258526698, 'approx_kl': 0.005514763295650482, 'clip_fraction': 0.2734375, 'grad_norm': 17.1498966217041}
2022-12-29 18:08:10.639 DEBUG: Taking gradient step
2022-12-29 18:08:10.650 DEBUG: Loss 5: {'policy_loss': -0.003901487515666259, 'entropy_loss': -0.021164349745959044, 'vf_loss': 0.0003402443613435459, 'total_loss': -0.024725592900281756, 'approx_kl': -0.008089217124506831, 'clip_fraction': 0.1575520858168602, 'grad_norm': 29.459959030151367}
2022-12-29 18:08:11.448 DEBUG: Taking gradient step
2022-12-29 18:08:11.457 DEBUG: Loss 6: {'policy_loss': 0.016555858061000527, 'entropy_loss': -0.020376690197736025, 'vf_loss': 0.00031996059470331697, 'total_loss': -0.0035008715420321787, 'approx_kl': 0.003797472221776843, 'clip_fraction': 0.10286458395421505, 'grad_norm': 17.770971298217773}
2022-12-29 18:08:12.283 DEBUG: Taking gradient step
2022-12-29 18:08:12.293 DEBUG: Loss 7: {'policy_loss': -0.001629357243876169, 'entropy_loss': -0.02065342850983143, 'vf_loss': 0.0003113810330727582, 'total_loss': -0.02197140472063484, 'approx_kl': -0.001079503446817398, 'clip_fraction': 0.16796875, 'grad_norm': 5.0068583488464355}
2022-12-29 18:08:13.097 DEBUG: Taking gradient step
2022-12-29 18:08:13.106 DEBUG: Loss 8: {'policy_loss': -0.04298395991394166, 'entropy_loss': -0.020827511325478554, 'vf_loss': 0.0003023886908107203, 'total_loss': -0.06350908254860949, 'approx_kl': 0.004525193013250828, 'clip_fraction': 0.14713541697710752, 'grad_norm': 11.605570793151855}
2022-12-29 18:08:13.930 DEBUG: Taking gradient step
2022-12-29 18:08:13.940 DEBUG: Loss 9: {'policy_loss': -0.021765365074410545, 'entropy_loss': -0.020661087706685066, 'vf_loss': 0.0002797851862527238, 'total_loss': -0.04214666759484289, 'approx_kl': 0.0016308019403368235, 'clip_fraction': 0.1940104179084301, 'grad_norm': 5.459526538848877}
2022-12-29 18:08:13.940 INFO: Optimization: policy loss=-0.022, vf loss=0.000, entropy loss=-0.021, total loss=-0.042, num steps=10
2022-12-29 18:08:13.940 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:08:15.314 INFO: Evaluation rollout: return=0.734 (0.0), episode length=5.0
2022-12-29 18:08:15.316 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:08:15.318 INFO: Iteration: 62/137, steps: 13392
2022-12-29 18:08:29.134 DEBUG: Atoms are too close
2022-12-29 18:08:44.507 DEBUG: Atoms are too close
2022-12-29 18:09:13.029 INFO: Training rollout: return=-0.513 (4.6), episode length=5.0
2022-12-29 18:09:13.031 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:09:13.034 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-13392_train.pkl
2022-12-29 18:09:13.861 DEBUG: Taking gradient step
2022-12-29 18:09:13.872 DEBUG: Loss 0: {'policy_loss': 0.02773617202632376, 'entropy_loss': -0.020981641486287117, 'vf_loss': 0.02041316503508034, 'total_loss': 0.027167695575116977, 'approx_kl': -1.4338482401399233e-08, 'clip_fraction': 0.0, 'grad_norm': 13.936137199401855}
2022-12-29 18:09:14.666 DEBUG: Taking gradient step
2022-12-29 18:09:14.678 DEBUG: Loss 1: {'policy_loss': 0.0417680551048861, 'entropy_loss': -0.021335281897336245, 'vf_loss': 0.02288560166049305, 'total_loss': 0.04331837486804291, 'approx_kl': 0.0038675404503010213, 'clip_fraction': 0.05598958395421505, 'grad_norm': 15.290179252624512}
2022-12-29 18:09:15.504 DEBUG: Taking gradient step
2022-12-29 18:09:15.514 DEBUG: Loss 2: {'policy_loss': -0.052553743902907506, 'entropy_loss': -0.02035201247781515, 'vf_loss': 0.015348284744972329, 'total_loss': -0.057557471635750326, 'approx_kl': 0.01715192649862729, 'clip_fraction': 0.2278645858168602, 'grad_norm': 3.554643154144287}
2022-12-29 18:09:16.371 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 18:09:16.372 INFO: Optimization: policy loss=-0.053, vf loss=0.015, entropy loss=-0.020, total loss=-0.058, num steps=3
2022-12-29 18:09:16.372 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:09:17.771 INFO: Evaluation rollout: return=0.738 (0.0), episode length=5.0
2022-12-29 18:09:17.772 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:09:17.774 INFO: Iteration: 63/137, steps: 13608
2022-12-29 18:10:03.569 DEBUG: Atoms are too close
2022-12-29 18:10:16.138 INFO: Training rollout: return=0.079 (3.3), episode length=5.0
2022-12-29 18:10:16.140 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:10:16.142 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-13608_train.pkl
2022-12-29 18:10:17.005 DEBUG: Taking gradient step
2022-12-29 18:10:17.017 DEBUG: Loss 0: {'policy_loss': 0.018949739573677273, 'entropy_loss': -0.019971095025539398, 'vf_loss': 0.010259640876924962, 'total_loss': 0.009238285425062841, 'approx_kl': -7.318643469034214e-08, 'clip_fraction': 0.0, 'grad_norm': 14.06187629699707}
2022-12-29 18:10:17.876 DEBUG: Taking gradient step
2022-12-29 18:10:17.891 DEBUG: Loss 1: {'policy_loss': 0.0212903407991714, 'entropy_loss': -0.020388109609484673, 'vf_loss': 0.010259588116500205, 'total_loss': 0.01116181930618693, 'approx_kl': 0.004191399086266756, 'clip_fraction': 0.02734375, 'grad_norm': 13.83395767211914}
2022-12-29 18:10:18.752 DEBUG: Taking gradient step
2022-12-29 18:10:18.762 DEBUG: Loss 2: {'policy_loss': -0.026678329324512067, 'entropy_loss': -0.02068406529724598, 'vf_loss': 0.007819750664695136, 'total_loss': -0.03954264395706291, 'approx_kl': 0.017533966747578233, 'clip_fraction': 0.30859375, 'grad_norm': 9.026386260986328}
2022-12-29 18:10:19.590 DEBUG: Taking gradient step
2022-12-29 18:10:19.599 DEBUG: Loss 3: {'policy_loss': -0.026830216707403397, 'entropy_loss': -0.01967960875481367, 'vf_loss': 0.00781295118504449, 'total_loss': -0.03869687427717257, 'approx_kl': 0.02694689342752099, 'clip_fraction': 0.3802083358168602, 'grad_norm': 7.151098251342773}
2022-12-29 18:10:20.428 DEBUG: Taking gradient step
2022-12-29 18:10:20.442 DEBUG: Loss 4: {'policy_loss': -0.03253047840465558, 'entropy_loss': -0.01960077742114663, 'vf_loss': 0.007823952521956896, 'total_loss': -0.04430730330384532, 'approx_kl': 0.013952067121863365, 'clip_fraction': 0.3515625, 'grad_norm': 6.413811683654785}
2022-12-29 18:10:21.263 DEBUG: Taking gradient step
2022-12-29 18:10:21.274 DEBUG: Loss 5: {'policy_loss': -0.03454573178808399, 'entropy_loss': -0.01972903683781624, 'vf_loss': 0.007820119402836254, 'total_loss': -0.04645464922306398, 'approx_kl': 0.00613704533316195, 'clip_fraction': 0.2799479216337204, 'grad_norm': 3.3499834537506104}
2022-12-29 18:10:22.090 DEBUG: Taking gradient step
2022-12-29 18:10:22.099 DEBUG: Loss 6: {'policy_loss': -0.03868996112672202, 'entropy_loss': -0.019301271066069603, 'vf_loss': 0.007824527086871872, 'total_loss': -0.05016670510591975, 'approx_kl': -0.0005874768830835819, 'clip_fraction': 0.2096354179084301, 'grad_norm': 3.3357062339782715}
2022-12-29 18:10:22.969 DEBUG: Taking gradient step
2022-12-29 18:10:22.978 DEBUG: Loss 7: {'policy_loss': -0.03684431751078991, 'entropy_loss': -0.018594767432659864, 'vf_loss': 0.00781504103602011, 'total_loss': -0.04762404390742967, 'approx_kl': -0.003304907586425543, 'clip_fraction': 0.18880208395421505, 'grad_norm': 3.170320510864258}
2022-12-29 18:10:23.795 DEBUG: Taking gradient step
2022-12-29 18:10:23.805 DEBUG: Loss 8: {'policy_loss': -0.037645200481350734, 'entropy_loss': -0.019521609414368868, 'vf_loss': 0.00781990643949659, 'total_loss': -0.049346903456223015, 'approx_kl': 0.01894380711019039, 'clip_fraction': 0.2734375, 'grad_norm': 4.489193916320801}
2022-12-29 18:10:24.610 DEBUG: Early stopping at step 9 for reaching max KL.
2022-12-29 18:10:24.611 INFO: Optimization: policy loss=-0.038, vf loss=0.008, entropy loss=-0.020, total loss=-0.049, num steps=9
2022-12-29 18:10:24.611 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:10:26.159 INFO: Evaluation rollout: return=0.727 (0.0), episode length=5.0
2022-12-29 18:10:26.160 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:10:26.162 INFO: Iteration: 64/137, steps: 13824
2022-12-29 18:11:12.958 DEBUG: Atoms are too close
2022-12-29 18:11:25.352 INFO: Training rollout: return=0.068 (3.3), episode length=5.0
2022-12-29 18:11:25.354 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:11:25.357 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-13824_train.pkl
2022-12-29 18:11:26.226 DEBUG: Taking gradient step
2022-12-29 18:11:26.235 DEBUG: Loss 0: {'policy_loss': -0.024174149081083035, 'entropy_loss': -0.018713124562054873, 'vf_loss': 0.007811171867972235, 'total_loss': -0.03507610177516568, 'approx_kl': 6.829701248989295e-09, 'clip_fraction': 0.0, 'grad_norm': 14.166436195373535}
2022-12-29 18:11:27.011 DEBUG: Taking gradient step
2022-12-29 18:11:27.022 DEBUG: Loss 1: {'policy_loss': 0.0025355091413655877, 'entropy_loss': -0.0190008282661438, 'vf_loss': 0.01037440867686101, 'total_loss': -0.006090910447917205, 'approx_kl': -0.0023637550184503198, 'clip_fraction': 0.03645833395421505, 'grad_norm': 21.67473030090332}
2022-12-29 18:11:27.805 DEBUG: Taking gradient step
2022-12-29 18:11:27.816 DEBUG: Loss 2: {'policy_loss': 0.004437962405812287, 'entropy_loss': -0.018915909808129072, 'vf_loss': 0.010332705495625323, 'total_loss': -0.004145241906691459, 'approx_kl': -0.003691157791763544, 'clip_fraction': 0.1549479179084301, 'grad_norm': 8.217001914978027}
2022-12-29 18:11:28.615 DEBUG: Taking gradient step
2022-12-29 18:11:28.632 DEBUG: Loss 3: {'policy_loss': -0.04004256303805636, 'entropy_loss': -0.0189174460247159, 'vf_loss': 0.00780426044364336, 'total_loss': -0.051155748619128895, 'approx_kl': -0.0026950270403176546, 'clip_fraction': 0.2682291716337204, 'grad_norm': 4.297210693359375}
2022-12-29 18:11:29.444 DEBUG: Taking gradient step
2022-12-29 18:11:29.454 DEBUG: Loss 4: {'policy_loss': -0.039108820740709344, 'entropy_loss': -0.019763431046158075, 'vf_loss': 0.007798153572825597, 'total_loss': -0.051074098214041824, 'approx_kl': 0.004280930035747588, 'clip_fraction': 0.3580729216337204, 'grad_norm': 2.0005862712860107}
2022-12-29 18:11:30.239 DEBUG: Taking gradient step
2022-12-29 18:11:30.248 DEBUG: Loss 5: {'policy_loss': -0.035253807692581185, 'entropy_loss': -0.019384934101253748, 'vf_loss': 0.007789341396134224, 'total_loss': -0.046849400397700705, 'approx_kl': 0.017390844877809286, 'clip_fraction': 0.3828125, 'grad_norm': 2.076730966567993}
2022-12-29 18:11:31.032 DEBUG: Taking gradient step
2022-12-29 18:11:31.042 DEBUG: Loss 6: {'policy_loss': 0.0009157830268916659, 'entropy_loss': -0.019538116175681353, 'vf_loss': 0.010269208683875066, 'total_loss': -0.008353124464914614, 'approx_kl': 0.03236381383612752, 'clip_fraction': 0.33984375, 'grad_norm': 2.0156641006469727}
2022-12-29 18:11:31.864 DEBUG: Taking gradient step
2022-12-29 18:11:31.874 DEBUG: Loss 7: {'policy_loss': -0.04002041995461822, 'entropy_loss': -0.01998226437717676, 'vf_loss': 0.0077967770228650635, 'total_loss': -0.05220590730892992, 'approx_kl': 0.030740500893443823, 'clip_fraction': 0.37109375, 'grad_norm': 3.399812936782837}
2022-12-29 18:11:32.663 DEBUG: Early stopping at step 8 for reaching max KL.
2022-12-29 18:11:32.664 INFO: Optimization: policy loss=-0.040, vf loss=0.008, entropy loss=-0.020, total loss=-0.052, num steps=8
2022-12-29 18:11:32.664 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:11:34.155 INFO: Evaluation rollout: return=0.723 (0.0), episode length=5.0
2022-12-29 18:11:34.156 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:11:34.159 INFO: Iteration: 65/137, steps: 14040
2022-12-29 18:11:56.912 DEBUG: There is a single atom floating around
2022-12-29 18:12:14.641 DEBUG: There is a single atom floating around
2022-12-29 18:12:32.647 INFO: Training rollout: return=-0.448 (4.6), episode length=4.9
2022-12-29 18:12:32.648 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:12:32.651 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-14040_train.pkl
2022-12-29 18:12:33.511 DEBUG: Taking gradient step
2022-12-29 18:12:33.524 DEBUG: Loss 0: {'policy_loss': 0.052773732198369186, 'entropy_loss': -0.020400984212756157, 'vf_loss': 0.014689983787558909, 'total_loss': 0.04706273177317195, 'approx_kl': 1.777273794800749e-08, 'clip_fraction': 0.0, 'grad_norm': 23.49837303161621}
2022-12-29 18:12:34.377 DEBUG: Taking gradient step
2022-12-29 18:12:34.388 DEBUG: Loss 1: {'policy_loss': -0.03083329299339594, 'entropy_loss': -0.0207478697411716, 'vf_loss': 0.00951497286761718, 'total_loss': -0.04206618986695036, 'approx_kl': 0.003970055724494159, 'clip_fraction': 0.0, 'grad_norm': 13.654109954833984}
2022-12-29 18:12:35.212 DEBUG: Taking gradient step
2022-12-29 18:12:35.221 DEBUG: Loss 2: {'policy_loss': 0.0026176908372190395, 'entropy_loss': -0.0209468393586576, 'vf_loss': 0.01205640373465659, 'total_loss': -0.006272744786781963, 'approx_kl': 0.008098318823613226, 'clip_fraction': 0.029947916977107525, 'grad_norm': 6.546919345855713}
2022-12-29 18:12:36.041 DEBUG: Taking gradient step
2022-12-29 18:12:36.051 DEBUG: Loss 3: {'policy_loss': -0.03751679468710994, 'entropy_loss': -0.021519419737160206, 'vf_loss': 0.009509487789492801, 'total_loss': -0.049526726634777335, 'approx_kl': 0.011401971569284797, 'clip_fraction': 0.11067708395421505, 'grad_norm': 1.7826337814331055}
2022-12-29 18:12:36.872 DEBUG: Taking gradient step
2022-12-29 18:12:36.881 DEBUG: Loss 4: {'policy_loss': -0.005048672128200912, 'entropy_loss': -0.02083774795755744, 'vf_loss': 0.012082839571300198, 'total_loss': -0.013803580514458154, 'approx_kl': 0.008673040196299553, 'clip_fraction': 0.1263020858168602, 'grad_norm': 1.9414018392562866}
2022-12-29 18:12:37.689 DEBUG: Taking gradient step
2022-12-29 18:12:37.699 DEBUG: Loss 5: {'policy_loss': 0.0013718433986809782, 'entropy_loss': -0.021188274957239628, 'vf_loss': 0.012055872733543284, 'total_loss': -0.007760558825015366, 'approx_kl': 0.003319695999380201, 'clip_fraction': 0.1171875, 'grad_norm': 1.7933331727981567}
2022-12-29 18:12:38.530 DEBUG: Taking gradient step
2022-12-29 18:12:38.539 DEBUG: Loss 6: {'policy_loss': -0.007094739096151185, 'entropy_loss': -0.021781868301331997, 'vf_loss': 0.012050257882429832, 'total_loss': -0.01682634951505335, 'approx_kl': 0.00934375636279583, 'clip_fraction': 0.23046875, 'grad_norm': 3.5742974281311035}
2022-12-29 18:12:39.418 DEBUG: Taking gradient step
2022-12-29 18:12:39.428 DEBUG: Loss 7: {'policy_loss': -0.005939031530587443, 'entropy_loss': -0.021769308019429445, 'vf_loss': 0.012023962822621189, 'total_loss': -0.015684376727395703, 'approx_kl': 0.01135905459523201, 'clip_fraction': 0.19921875, 'grad_norm': 2.1255459785461426}
2022-12-29 18:12:40.228 DEBUG: Taking gradient step
2022-12-29 18:12:40.237 DEBUG: Loss 8: {'policy_loss': -0.0421712327390186, 'entropy_loss': -0.02301110839471221, 'vf_loss': 0.009435357388994911, 'total_loss': -0.0557469837447359, 'approx_kl': 0.0032793356804177165, 'clip_fraction': 0.140625, 'grad_norm': 1.4429506063461304}
2022-12-29 18:12:41.044 DEBUG: Taking gradient step
2022-12-29 18:12:41.054 DEBUG: Loss 9: {'policy_loss': -0.040890498540515144, 'entropy_loss': -0.02349823759868741, 'vf_loss': 0.00941750401907542, 'total_loss': -0.05497123212012714, 'approx_kl': 0.006720445118844509, 'clip_fraction': 0.2630208358168602, 'grad_norm': 0.6871411800384521}
2022-12-29 18:12:41.054 INFO: Optimization: policy loss=-0.041, vf loss=0.009, entropy loss=-0.023, total loss=-0.055, num steps=10
2022-12-29 18:12:41.055 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:12:42.462 INFO: Evaluation rollout: return=0.704 (0.0), episode length=5.0
2022-12-29 18:12:42.463 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:12:42.466 INFO: Iteration: 66/137, steps: 14256
2022-12-29 18:13:41.251 INFO: Training rollout: return=0.635 (0.1), episode length=5.0
2022-12-29 18:13:41.253 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:13:41.256 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-14256_train.pkl
2022-12-29 18:13:42.134 DEBUG: Taking gradient step
2022-12-29 18:13:42.148 DEBUG: Loss 0: {'policy_loss': 0.031775525381048476, 'entropy_loss': -0.024618492927402258, 'vf_loss': 0.0002633581675471038, 'total_loss': 0.007420390621193325, 'approx_kl': 7.136259227991104e-08, 'clip_fraction': 0.0, 'grad_norm': 27.106842041015625}
2022-12-29 18:13:42.985 DEBUG: Taking gradient step
2022-12-29 18:13:42.999 DEBUG: Loss 1: {'policy_loss': 0.007831377659652541, 'entropy_loss': -0.025406126398593187, 'vf_loss': 0.00026970479776760426, 'total_loss': -0.017305043941173038, 'approx_kl': -0.0014284189091995358, 'clip_fraction': 0.041666666977107525, 'grad_norm': 13.338855743408203}
2022-12-29 18:13:43.807 DEBUG: Taking gradient step
2022-12-29 18:13:43.816 DEBUG: Loss 2: {'policy_loss': -0.045783205403244785, 'entropy_loss': -0.02448986331000924, 'vf_loss': 0.0002830891880689072, 'total_loss': -0.06998997952518511, 'approx_kl': -9.69739630818367e-06, 'clip_fraction': 0.2174479179084301, 'grad_norm': 10.150691032409668}
2022-12-29 18:13:44.658 DEBUG: Taking gradient step
2022-12-29 18:13:44.672 DEBUG: Loss 3: {'policy_loss': -0.02390659554921602, 'entropy_loss': -0.02559050777927041, 'vf_loss': 0.00028552683150469396, 'total_loss': -0.04921157649698174, 'approx_kl': 0.01121006312314421, 'clip_fraction': 0.33984375, 'grad_norm': 12.693819999694824}
2022-12-29 18:13:45.515 DEBUG: Taking gradient step
2022-12-29 18:13:45.528 DEBUG: Loss 4: {'policy_loss': -0.005856568201113185, 'entropy_loss': -0.02646646276116371, 'vf_loss': 0.00028345214165337827, 'total_loss': -0.03203957882062352, 'approx_kl': 0.0262999051483348, 'clip_fraction': 0.3567708358168602, 'grad_norm': 10.977027893066406}
2022-12-29 18:13:46.335 DEBUG: Taking gradient step
2022-12-29 18:13:46.344 DEBUG: Loss 5: {'policy_loss': -0.030011784835744588, 'entropy_loss': -0.02619015146046877, 'vf_loss': 0.0002934576355117907, 'total_loss': -0.055908478660701574, 'approx_kl': 0.02586514875292778, 'clip_fraction': 0.33203125, 'grad_norm': 10.361331939697266}
2022-12-29 18:13:47.149 DEBUG: Taking gradient step
2022-12-29 18:13:47.160 DEBUG: Loss 6: {'policy_loss': -0.06522917455563745, 'entropy_loss': -0.02716315258294344, 'vf_loss': 0.0003168927028822958, 'total_loss': -0.0920754344356986, 'approx_kl': 0.031872404273599386, 'clip_fraction': 0.3138020858168602, 'grad_norm': 9.025017738342285}
2022-12-29 18:13:48.006 DEBUG: Taking gradient step
2022-12-29 18:13:48.017 DEBUG: Loss 7: {'policy_loss': -0.07208052465230785, 'entropy_loss': -0.027247545775026083, 'vf_loss': 0.0003149840046881182, 'total_loss': -0.09901308642264581, 'approx_kl': 0.008671492338180542, 'clip_fraction': 0.24609375, 'grad_norm': 10.831562042236328}
2022-12-29 18:13:48.839 DEBUG: Taking gradient step
2022-12-29 18:13:48.849 DEBUG: Loss 8: {'policy_loss': -0.06695464901639475, 'entropy_loss': -0.02771903108805418, 'vf_loss': 0.000310645037001422, 'total_loss': -0.0943630350674475, 'approx_kl': 0.029776329640299082, 'clip_fraction': 0.203125, 'grad_norm': 15.138015747070312}
2022-12-29 18:13:49.684 DEBUG: Early stopping at step 9 for reaching max KL.
2022-12-29 18:13:49.684 INFO: Optimization: policy loss=-0.067, vf loss=0.000, entropy loss=-0.028, total loss=-0.094, num steps=9
2022-12-29 18:13:49.685 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:13:51.124 INFO: Evaluation rollout: return=0.697 (0.0), episode length=5.0
2022-12-29 18:13:51.125 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:13:51.128 INFO: Iteration: 67/137, steps: 14472
2022-12-29 18:14:20.316 DEBUG: Atoms are too close
2022-12-29 18:14:20.904 DEBUG: There is a single atom floating around
2022-12-29 18:14:48.837 INFO: Training rollout: return=-0.492 (4.6), episode length=5.0
2022-12-29 18:14:48.839 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:14:48.841 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-14472_train.pkl
2022-12-29 18:14:49.693 DEBUG: Taking gradient step
2022-12-29 18:14:49.702 DEBUG: Loss 0: {'policy_loss': -0.002459345844076568, 'entropy_loss': -0.028734589461237192, 'vf_loss': 0.018298726355814962, 'total_loss': -0.012895208949498794, 'approx_kl': 5.436595529317856e-08, 'clip_fraction': 0.0, 'grad_norm': 11.560093879699707}
2022-12-29 18:14:50.521 DEBUG: Taking gradient step
2022-12-29 18:14:50.531 DEBUG: Loss 1: {'policy_loss': -0.04717428069775652, 'entropy_loss': -0.029624024406075478, 'vf_loss': 0.015681297925962498, 'total_loss': -0.0611170071778695, 'approx_kl': -0.0027729476569220424, 'clip_fraction': 0.07682291697710752, 'grad_norm': 3.5356757640838623}
2022-12-29 18:14:51.355 DEBUG: Taking gradient step
2022-12-29 18:14:51.364 DEBUG: Loss 2: {'policy_loss': 0.00805342656590528, 'entropy_loss': -0.030144182965159416, 'vf_loss': 0.020779828944636466, 'total_loss': -0.0013109274546176733, 'approx_kl': 0.017208917066454887, 'clip_fraction': 0.29296875, 'grad_norm': 7.325489044189453}
2022-12-29 18:14:52.159 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 18:14:52.160 INFO: Optimization: policy loss=0.008, vf loss=0.021, entropy loss=-0.030, total loss=-0.001, num steps=3
2022-12-29 18:14:52.160 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:14:53.655 INFO: Evaluation rollout: return=0.655 (0.0), episode length=5.0
2022-12-29 18:14:53.656 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:14:53.659 INFO: Iteration: 68/137, steps: 14688
2022-12-29 18:15:09.304 DEBUG: Atoms are too close
2022-12-29 18:15:12.191 DEBUG: There is a single atom floating around
2022-12-29 18:15:19.573 DEBUG: There is a single atom floating around
2022-12-29 18:15:35.849 DEBUG: Atoms are too close
2022-12-29 18:15:50.568 INFO: Training rollout: return=-1.641 (6.3), episode length=4.9
2022-12-29 18:15:50.569 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:15:50.572 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-14688_train.pkl
2022-12-29 18:15:51.460 DEBUG: Taking gradient step
2022-12-29 18:15:51.470 DEBUG: Loss 0: {'policy_loss': 0.02928660217672467, 'entropy_loss': -0.03056706255301833, 'vf_loss': 0.03237138939812298, 'total_loss': 0.03109092902182932, 'approx_kl': 4.889443516731262e-09, 'clip_fraction': 0.0, 'grad_norm': 17.11065101623535}
2022-12-29 18:15:52.293 DEBUG: Taking gradient step
2022-12-29 18:15:52.303 DEBUG: Loss 1: {'policy_loss': 0.09152318324037068, 'entropy_loss': -0.03162440238520503, 'vf_loss': 0.03765303984244751, 'total_loss': 0.09755182069761315, 'approx_kl': 0.008206033613532782, 'clip_fraction': 0.09895833395421505, 'grad_norm': 13.847208023071289}
2022-12-29 18:15:53.142 DEBUG: Taking gradient step
2022-12-29 18:15:53.151 DEBUG: Loss 2: {'policy_loss': -0.010171770800453082, 'entropy_loss': -0.03254548832774162, 'vf_loss': 0.027206363379680694, 'total_loss': -0.015510895748514008, 'approx_kl': 0.01804998889565468, 'clip_fraction': 0.1822916679084301, 'grad_norm': 12.126519203186035}
2022-12-29 18:15:53.987 DEBUG: Taking gradient step
2022-12-29 18:15:53.997 DEBUG: Loss 3: {'policy_loss': 0.018785137835271453, 'entropy_loss': -0.032686860766261816, 'vf_loss': 0.02962486782917447, 'total_loss': 0.015723144898184112, 'approx_kl': 0.021729809930548072, 'clip_fraction': 0.1901041679084301, 'grad_norm': 17.112411499023438}
2022-12-29 18:15:54.813 DEBUG: Taking gradient step
2022-12-29 18:15:54.822 DEBUG: Loss 4: {'policy_loss': -0.015940194508133512, 'entropy_loss': -0.031012901104986668, 'vf_loss': 0.0271022560055899, 'total_loss': -0.01985083960753028, 'approx_kl': 0.02818448282778263, 'clip_fraction': 0.1901041679084301, 'grad_norm': 15.232217788696289}
2022-12-29 18:15:55.615 DEBUG: Taking gradient step
2022-12-29 18:15:55.624 DEBUG: Loss 5: {'policy_loss': -0.0007235460428822612, 'entropy_loss': -0.03216254338622093, 'vf_loss': 0.0295729255015997, 'total_loss': -0.0033131639275034907, 'approx_kl': 0.03453502175398171, 'clip_fraction': 0.2734375, 'grad_norm': 10.873698234558105}
2022-12-29 18:15:56.447 DEBUG: Taking gradient step
2022-12-29 18:15:56.456 DEBUG: Loss 6: {'policy_loss': 0.08084102570113606, 'entropy_loss': -0.03267062548547983, 'vf_loss': 0.037310641198331836, 'total_loss': 0.08548104141398807, 'approx_kl': 0.04426041059195995, 'clip_fraction': 0.3736979216337204, 'grad_norm': 18.709651947021484}
2022-12-29 18:15:57.273 DEBUG: Early stopping at step 7 for reaching max KL.
2022-12-29 18:15:57.273 INFO: Optimization: policy loss=0.081, vf loss=0.037, entropy loss=-0.033, total loss=0.085, num steps=7
2022-12-29 18:15:57.274 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:15:58.733 INFO: Evaluation rollout: return=0.676 (0.0), episode length=5.0
2022-12-29 18:15:58.734 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:15:58.737 INFO: Iteration: 69/137, steps: 14904
2022-12-29 18:16:08.318 DEBUG: There is a single atom floating around
2022-12-29 18:16:25.570 DEBUG: There is a single atom floating around
2022-12-29 18:16:56.895 INFO: Training rollout: return=-0.557 (4.6), episode length=4.9
2022-12-29 18:16:56.897 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:16:56.900 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-14904_train.pkl
2022-12-29 18:16:57.705 DEBUG: Taking gradient step
2022-12-29 18:16:57.715 DEBUG: Loss 0: {'policy_loss': 0.03864984951416664, 'entropy_loss': -0.03274601045995951, 'vf_loss': 0.017491369629096566, 'total_loss': 0.023395208683303706, 'approx_kl': -7.031485438346863e-08, 'clip_fraction': 0.0, 'grad_norm': 27.434040069580078}
2022-12-29 18:16:58.515 DEBUG: Taking gradient step
2022-12-29 18:16:58.530 DEBUG: Loss 1: {'policy_loss': 0.0035663746829808366, 'entropy_loss': -0.03372018365189433, 'vf_loss': 0.014949053905370463, 'total_loss': -0.015204755063543032, 'approx_kl': 0.006567054777406156, 'clip_fraction': 0.041666666977107525, 'grad_norm': 30.255178451538086}
2022-12-29 18:16:59.377 DEBUG: Taking gradient step
2022-12-29 18:16:59.388 DEBUG: Loss 2: {'policy_loss': 0.06753013419391825, 'entropy_loss': -0.032428459729999304, 'vf_loss': 0.019861457244584026, 'total_loss': 0.05496313170850298, 'approx_kl': 0.011214467856916599, 'clip_fraction': 0.12630208395421505, 'grad_norm': 19.559608459472656}
2022-12-29 18:17:00.191 DEBUG: Taking gradient step
2022-12-29 18:17:00.200 DEBUG: Loss 3: {'policy_loss': -0.031177951011244986, 'entropy_loss': -0.033002354204654694, 'vf_loss': 0.01234870503471102, 'total_loss': -0.05183160018118865, 'approx_kl': 0.014496796764433384, 'clip_fraction': 0.1666666679084301, 'grad_norm': 7.880303859710693}
2022-12-29 18:17:01.040 DEBUG: Taking gradient step
2022-12-29 18:17:01.051 DEBUG: Loss 4: {'policy_loss': -0.03313439900438939, 'entropy_loss': -0.03295016381889582, 'vf_loss': 0.012344153335073492, 'total_loss': -0.05374040948821171, 'approx_kl': 0.028375979512929916, 'clip_fraction': 0.2044270858168602, 'grad_norm': 7.926519393920898}
2022-12-29 18:17:01.932 DEBUG: Taking gradient step
2022-12-29 18:17:01.946 DEBUG: Loss 5: {'policy_loss': 0.0233842405613335, 'entropy_loss': -0.03181331604719162, 'vf_loss': 0.017296924931462947, 'total_loss': 0.008867849445604828, 'approx_kl': 0.03795471368357539, 'clip_fraction': 0.2369791679084301, 'grad_norm': 12.875106811523438}
2022-12-29 18:17:02.827 DEBUG: Early stopping at step 6 for reaching max KL.
2022-12-29 18:17:02.827 INFO: Optimization: policy loss=0.023, vf loss=0.017, entropy loss=-0.032, total loss=0.009, num steps=6
2022-12-29 18:17:02.827 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:17:04.221 INFO: Evaluation rollout: return=0.680 (0.0), episode length=5.0
2022-12-29 18:17:04.222 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:17:04.225 INFO: Iteration: 70/137, steps: 15120
2022-12-29 18:18:02.542 INFO: Training rollout: return=0.582 (0.0), episode length=5.0
2022-12-29 18:18:02.544 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:18:02.546 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-15120_train.pkl
2022-12-29 18:18:03.381 DEBUG: Taking gradient step
2022-12-29 18:18:03.390 DEBUG: Loss 0: {'policy_loss': -0.04428578090635696, 'entropy_loss': -0.03229957353323698, 'vf_loss': 0.00047474491022669434, 'total_loss': -0.07611060952936725, 'approx_kl': -6.457170087514896e-08, 'clip_fraction': 0.0, 'grad_norm': 9.487602233886719}
2022-12-29 18:18:04.289 DEBUG: Taking gradient step
2022-12-29 18:18:04.305 DEBUG: Loss 1: {'policy_loss': -0.05143074415031598, 'entropy_loss': -0.03199576586484909, 'vf_loss': 0.0004944224978796767, 'total_loss': -0.0829320875172854, 'approx_kl': 8.04470619186759e-05, 'clip_fraction': 0.08723958395421505, 'grad_norm': 15.247889518737793}
2022-12-29 18:18:05.200 DEBUG: Taking gradient step
2022-12-29 18:18:05.210 DEBUG: Loss 2: {'policy_loss': 0.007848639666814762, 'entropy_loss': -0.03335509728640318, 'vf_loss': 0.00048483634747061136, 'total_loss': -0.02502162127211781, 'approx_kl': 0.01875192462466657, 'clip_fraction': 0.23828125, 'grad_norm': 16.231843948364258}
2022-12-29 18:18:06.029 DEBUG: Taking gradient step
2022-12-29 18:18:06.040 DEBUG: Loss 3: {'policy_loss': 0.02147633405324114, 'entropy_loss': -0.033210073597729206, 'vf_loss': 0.000486588464593475, 'total_loss': -0.011247151079894595, 'approx_kl': 0.019798720721155405, 'clip_fraction': 0.3138020858168602, 'grad_norm': 14.821718215942383}
2022-12-29 18:18:06.917 DEBUG: Taking gradient step
2022-12-29 18:18:06.926 DEBUG: Loss 4: {'policy_loss': -0.033520127238841006, 'entropy_loss': -0.03340728813782334, 'vf_loss': 0.0005023070395404293, 'total_loss': -0.06642510833712392, 'approx_kl': 0.03627088316716254, 'clip_fraction': 0.36328125, 'grad_norm': 13.135162353515625}
2022-12-29 18:18:07.767 DEBUG: Early stopping at step 5 for reaching max KL.
2022-12-29 18:18:07.768 INFO: Optimization: policy loss=-0.034, vf loss=0.001, entropy loss=-0.033, total loss=-0.066, num steps=5
2022-12-29 18:18:07.768 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:18:09.227 INFO: Evaluation rollout: return=0.693 (0.0), episode length=5.0
2022-12-29 18:18:09.228 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:18:09.230 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-13176.model
2022-12-29 18:18:09.232 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-15336.model
2022-12-29 18:18:09.262 INFO: Iteration: 71/137, steps: 15336
2022-12-29 18:18:39.392 DEBUG: There is a single atom floating around
2022-12-29 18:18:40.561 DEBUG: Atoms are too close
2022-12-29 18:18:55.382 DEBUG: Atoms are too close
2022-12-29 18:19:06.591 INFO: Training rollout: return=-1.108 (5.6), episode length=5.0
2022-12-29 18:19:06.592 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:19:06.595 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-15336_train.pkl
2022-12-29 18:19:07.394 DEBUG: Taking gradient step
2022-12-29 18:19:07.404 DEBUG: Loss 0: {'policy_loss': -0.017106461585380973, 'entropy_loss': -0.033360767643898726, 'vf_loss': 0.02490857378713191, 'total_loss': -0.02555865544214779, 'approx_kl': -4.147295840084553e-08, 'clip_fraction': 0.0, 'grad_norm': 11.639257431030273}
2022-12-29 18:19:08.278 DEBUG: Taking gradient step
2022-12-29 18:19:08.290 DEBUG: Loss 1: {'policy_loss': -0.041690551469897946, 'entropy_loss': -0.03236208809539676, 'vf_loss': 0.022407987470773737, 'total_loss': -0.051644652094520974, 'approx_kl': 0.004068902984727174, 'clip_fraction': 0.01953125, 'grad_norm': 10.984445571899414}
2022-12-29 18:19:09.068 DEBUG: Taking gradient step
2022-12-29 18:19:09.077 DEBUG: Loss 2: {'policy_loss': -0.0426806006366878, 'entropy_loss': -0.03295811591669917, 'vf_loss': 0.022404288501047153, 'total_loss': -0.053234428052339806, 'approx_kl': 0.019599355524405837, 'clip_fraction': 0.057291666977107525, 'grad_norm': 9.372179985046387}
2022-12-29 18:19:09.869 DEBUG: Taking gradient step
2022-12-29 18:19:09.879 DEBUG: Loss 3: {'policy_loss': -0.025536975929072107, 'entropy_loss': -0.033095710445195436, 'vf_loss': 0.024901812169290303, 'total_loss': -0.03373087420497724, 'approx_kl': 0.032789163873530924, 'clip_fraction': 0.0859375, 'grad_norm': 8.52070426940918}
2022-12-29 18:19:10.714 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 18:19:10.714 INFO: Optimization: policy loss=-0.026, vf loss=0.025, entropy loss=-0.033, total loss=-0.034, num steps=4
2022-12-29 18:19:10.715 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:19:12.192 INFO: Evaluation rollout: return=0.688 (0.0), episode length=5.0
2022-12-29 18:19:12.193 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:19:12.195 INFO: Iteration: 72/137, steps: 15552
2022-12-29 18:19:42.858 DEBUG: Atoms are too close
2022-12-29 18:19:59.150 DEBUG: Atoms are too close
2022-12-29 18:20:10.613 INFO: Training rollout: return=-0.552 (4.6), episode length=5.0
2022-12-29 18:20:10.615 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:20:10.618 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-15552_train.pkl
2022-12-29 18:20:11.457 DEBUG: Taking gradient step
2022-12-29 18:20:11.466 DEBUG: Loss 0: {'policy_loss': 0.028980623565114137, 'entropy_loss': -0.03222130052745342, 'vf_loss': 0.020275299585133064, 'total_loss': 0.01703462262279378, 'approx_kl': 3.4924596548080444e-09, 'clip_fraction': 0.0, 'grad_norm': 20.222177505493164}
2022-12-29 18:20:12.308 DEBUG: Taking gradient step
2022-12-29 18:20:12.319 DEBUG: Loss 1: {'policy_loss': -0.03720011106581154, 'entropy_loss': -0.03201771108433604, 'vf_loss': 0.015288915198527266, 'total_loss': -0.053928906951620316, 'approx_kl': -0.008155881194397807, 'clip_fraction': 0.1041666679084301, 'grad_norm': 11.321513175964355}
2022-12-29 18:20:13.102 DEBUG: Taking gradient step
2022-12-29 18:20:13.111 DEBUG: Loss 2: {'policy_loss': -0.04665095975344569, 'entropy_loss': -0.03139985213056207, 'vf_loss': 0.015312328665802774, 'total_loss': -0.06273848321820498, 'approx_kl': -0.001007231418043375, 'clip_fraction': 0.2174479179084301, 'grad_norm': 7.215357303619385}
2022-12-29 18:20:13.908 DEBUG: Taking gradient step
2022-12-29 18:20:13.917 DEBUG: Loss 3: {'policy_loss': 0.015433788587095265, 'entropy_loss': -0.031286625657230616, 'vf_loss': 0.020122675068744024, 'total_loss': 0.00426983799860868, 'approx_kl': -0.005349583458155394, 'clip_fraction': 0.2864583358168602, 'grad_norm': 10.090907096862793}
2022-12-29 18:20:14.737 DEBUG: Taking gradient step
2022-12-29 18:20:14.746 DEBUG: Loss 4: {'policy_loss': 0.012523154179889952, 'entropy_loss': -0.031650871969759464, 'vf_loss': 0.02009882412428742, 'total_loss': 0.0009711063344179094, 'approx_kl': -0.005651498329825699, 'clip_fraction': 0.34765625, 'grad_norm': 7.399169921875}
2022-12-29 18:20:15.579 DEBUG: Taking gradient step
2022-12-29 18:20:15.589 DEBUG: Loss 5: {'policy_loss': -0.019533435003460574, 'entropy_loss': -0.03165145171806216, 'vf_loss': 0.01772263074851667, 'total_loss': -0.03346225597300606, 'approx_kl': 0.001578683266416192, 'clip_fraction': 0.3893229216337204, 'grad_norm': 6.015717506408691}
2022-12-29 18:20:16.471 DEBUG: Taking gradient step
2022-12-29 18:20:16.482 DEBUG: Loss 6: {'policy_loss': 0.03780015785948449, 'entropy_loss': -0.030687162186950445, 'vf_loss': 0.022499387272163232, 'total_loss': 0.02961238294469728, 'approx_kl': -0.008087358437478542, 'clip_fraction': 0.3736979216337204, 'grad_norm': 7.726998805999756}
2022-12-29 18:20:17.306 DEBUG: Taking gradient step
2022-12-29 18:20:17.318 DEBUG: Loss 7: {'policy_loss': -0.02575797741233359, 'entropy_loss': -0.030897274147719145, 'vf_loss': 0.017714293550739602, 'total_loss': -0.03894095800931313, 'approx_kl': 0.009842781815677881, 'clip_fraction': 0.4036458358168602, 'grad_norm': 4.935940265655518}
2022-12-29 18:20:18.146 DEBUG: Taking gradient step
2022-12-29 18:20:18.156 DEBUG: Loss 8: {'policy_loss': -0.0485719877439016, 'entropy_loss': -0.030929110012948513, 'vf_loss': 0.015264394893111379, 'total_loss': -0.06423670286373873, 'approx_kl': 0.006133389193564653, 'clip_fraction': 0.3606770858168602, 'grad_norm': 4.974583148956299}
2022-12-29 18:20:18.953 DEBUG: Taking gradient step
2022-12-29 18:20:18.962 DEBUG: Loss 9: {'policy_loss': 0.011622847803576454, 'entropy_loss': -0.03154808096587658, 'vf_loss': 0.020166533182824176, 'total_loss': 0.0002413000205240476, 'approx_kl': -0.00744272000156343, 'clip_fraction': 0.2799479216337204, 'grad_norm': 4.778972625732422}
2022-12-29 18:20:18.963 INFO: Optimization: policy loss=0.012, vf loss=0.020, entropy loss=-0.032, total loss=0.000, num steps=10
2022-12-29 18:20:18.963 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:20:20.439 INFO: Evaluation rollout: return=0.694 (0.0), episode length=5.0
2022-12-29 18:20:20.440 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:20:20.443 INFO: Iteration: 73/137, steps: 15768
2022-12-29 18:21:08.811 DEBUG: Atoms are too close
2022-12-29 18:21:19.699 INFO: Training rollout: return=0.019 (3.4), episode length=5.0
2022-12-29 18:21:19.700 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:21:19.702 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-15768_train.pkl
2022-12-29 18:21:20.547 DEBUG: Taking gradient step
2022-12-29 18:21:20.557 DEBUG: Loss 0: {'policy_loss': 0.01381874218768207, 'entropy_loss': -0.030444203410297632, 'vf_loss': 0.010478271385001951, 'total_loss': -0.006147189837613615, 'approx_kl': 3.360522349282746e-08, 'clip_fraction': 0.0, 'grad_norm': 18.443655014038086}
2022-12-29 18:21:21.434 DEBUG: Taking gradient step
2022-12-29 18:21:21.449 DEBUG: Loss 1: {'policy_loss': 0.04487230293413204, 'entropy_loss': -0.029886215459555387, 'vf_loss': 0.012855448819996566, 'total_loss': 0.02784153629457322, 'approx_kl': 0.009381783660501242, 'clip_fraction': 0.061197916977107525, 'grad_norm': 23.466228485107422}
2022-12-29 18:21:22.299 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 18:21:22.299 INFO: Optimization: policy loss=0.045, vf loss=0.013, entropy loss=-0.030, total loss=0.028, num steps=2
2022-12-29 18:21:22.299 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:21:23.780 INFO: Evaluation rollout: return=0.693 (0.0), episode length=5.0
2022-12-29 18:21:23.781 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:21:23.784 INFO: Iteration: 74/137, steps: 15984
2022-12-29 18:22:07.305 DEBUG: Atoms are too close
2022-12-29 18:22:22.609 INFO: Training rollout: return=0.018 (3.3), episode length=5.0
2022-12-29 18:22:22.612 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:22:22.615 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-15984_train.pkl
2022-12-29 18:22:23.464 DEBUG: Taking gradient step
2022-12-29 18:22:23.478 DEBUG: Loss 0: {'policy_loss': -0.021732119119209824, 'entropy_loss': -0.03144001169130206, 'vf_loss': 0.006556909418657179, 'total_loss': -0.0466152213918547, 'approx_kl': -4.268564168796729e-09, 'clip_fraction': 0.0, 'grad_norm': 5.123614311218262}
2022-12-29 18:22:24.339 DEBUG: Taking gradient step
2022-12-29 18:22:24.350 DEBUG: Loss 1: {'policy_loss': -0.021541376713654428, 'entropy_loss': -0.02941158600151539, 'vf_loss': 0.006552566691277137, 'total_loss': -0.04440039602389268, 'approx_kl': 0.011581807862967253, 'clip_fraction': 0.1028645858168602, 'grad_norm': 5.723796367645264}
2022-12-29 18:22:25.176 DEBUG: Taking gradient step
2022-12-29 18:22:25.185 DEBUG: Loss 2: {'policy_loss': 0.054337427581718806, 'entropy_loss': -0.029196900315582752, 'vf_loss': 0.01134414076788701, 'total_loss': 0.03648466803402306, 'approx_kl': 0.03406387520954013, 'clip_fraction': 0.2265625, 'grad_norm': 8.89797306060791}
2022-12-29 18:22:26.022 DEBUG: Taking gradient step
2022-12-29 18:22:26.036 DEBUG: Loss 3: {'policy_loss': 0.008487325262309046, 'entropy_loss': -0.02809775434434414, 'vf_loss': 0.008971499903236278, 'total_loss': -0.010638929178798821, 'approx_kl': 0.029489061329513788, 'clip_fraction': 0.24609375, 'grad_norm': 7.287709712982178}
2022-12-29 18:22:26.880 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 18:22:26.880 INFO: Optimization: policy loss=0.008, vf loss=0.009, entropy loss=-0.028, total loss=-0.011, num steps=4
2022-12-29 18:22:26.881 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:22:28.314 INFO: Evaluation rollout: return=0.701 (0.0), episode length=5.0
2022-12-29 18:22:28.315 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:22:28.318 INFO: Iteration: 75/137, steps: 16200
2022-12-29 18:22:32.048 DEBUG: There is a single atom floating around
2022-12-29 18:23:26.400 INFO: Training rollout: return=0.055 (3.3), episode length=4.9
2022-12-29 18:23:26.402 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:23:26.405 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-16200_train.pkl
2022-12-29 18:23:27.220 DEBUG: Taking gradient step
2022-12-29 18:23:27.229 DEBUG: Loss 0: {'policy_loss': -0.009074968762844783, 'entropy_loss': -0.02913917088881135, 'vf_loss': 0.003847382336912679, 'total_loss': -0.03436675731474345, 'approx_kl': 5.626741739206409e-09, 'clip_fraction': 0.0, 'grad_norm': 2.7152669429779053}
2022-12-29 18:23:28.056 DEBUG: Taking gradient step
2022-12-29 18:23:28.066 DEBUG: Loss 1: {'policy_loss': -0.027873781664651775, 'entropy_loss': -0.028279459103941917, 'vf_loss': 0.0038364046923668447, 'total_loss': -0.05231683607622685, 'approx_kl': 0.0014088493771851063, 'clip_fraction': 0.037760416977107525, 'grad_norm': 2.7033095359802246}
2022-12-29 18:23:28.871 DEBUG: Taking gradient step
2022-12-29 18:23:28.882 DEBUG: Loss 2: {'policy_loss': 0.04231395464123798, 'entropy_loss': -0.026351011358201504, 'vf_loss': 0.006325113219045657, 'total_loss': 0.02228805650208214, 'approx_kl': 0.004697268479503691, 'clip_fraction': 0.109375, 'grad_norm': 2.887389659881592}
2022-12-29 18:23:29.667 DEBUG: Taking gradient step
2022-12-29 18:23:29.676 DEBUG: Loss 3: {'policy_loss': -0.03131051070293707, 'entropy_loss': -0.025666439440101385, 'vf_loss': 0.0037227681454510813, 'total_loss': -0.05325418199758737, 'approx_kl': 0.012248887214809656, 'clip_fraction': 0.1236979179084301, 'grad_norm': 2.7085959911346436}
2022-12-29 18:23:30.466 DEBUG: Taking gradient step
2022-12-29 18:23:30.475 DEBUG: Loss 4: {'policy_loss': -0.02925788236984894, 'entropy_loss': -0.025458343792706728, 'vf_loss': 0.0036459444946123206, 'total_loss': -0.05107028166794334, 'approx_kl': 0.005290464032441378, 'clip_fraction': 0.0703125, 'grad_norm': 3.33070707321167}
2022-12-29 18:23:31.285 DEBUG: Taking gradient step
2022-12-29 18:23:31.294 DEBUG: Loss 5: {'policy_loss': -0.0258185165269235, 'entropy_loss': -0.024873896036297083, 'vf_loss': 0.003571635421420597, 'total_loss': -0.04712077714179999, 'approx_kl': 0.0015885501634329557, 'clip_fraction': 0.05859375, 'grad_norm': 1.5886814594268799}
2022-12-29 18:23:32.092 DEBUG: Taking gradient step
2022-12-29 18:23:32.101 DEBUG: Loss 6: {'policy_loss': 0.03549243120192191, 'entropy_loss': -0.02372320555150509, 'vf_loss': 0.006055238509108168, 'total_loss': 0.01782446415952499, 'approx_kl': 0.011124933953396976, 'clip_fraction': 0.1458333358168602, 'grad_norm': 1.766440749168396}
2022-12-29 18:23:32.900 DEBUG: Taking gradient step
2022-12-29 18:23:32.911 DEBUG: Loss 7: {'policy_loss': -0.02298593621295647, 'entropy_loss': -0.024528607726097107, 'vf_loss': 0.0034440430272552024, 'total_loss': -0.04407050091179837, 'approx_kl': 0.008416418335400522, 'clip_fraction': 0.1484375, 'grad_norm': 1.6574807167053223}
2022-12-29 18:23:33.725 DEBUG: Taking gradient step
2022-12-29 18:23:33.735 DEBUG: Loss 8: {'policy_loss': -0.028546298456833107, 'entropy_loss': -0.02497497107833624, 'vf_loss': 0.003394576075057165, 'total_loss': -0.050126693460112184, 'approx_kl': 0.01270282338373363, 'clip_fraction': 0.13411458395421505, 'grad_norm': 2.570338249206543}
2022-12-29 18:23:34.553 DEBUG: Taking gradient step
2022-12-29 18:23:34.562 DEBUG: Loss 9: {'policy_loss': -0.03162809149507154, 'entropy_loss': -0.024568037129938602, 'vf_loss': 0.003371139548242747, 'total_loss': -0.0528249890767674, 'approx_kl': 0.018646207405254245, 'clip_fraction': 0.1731770858168602, 'grad_norm': 3.444998264312744}
2022-12-29 18:23:34.562 INFO: Optimization: policy loss=-0.032, vf loss=0.003, entropy loss=-0.025, total loss=-0.053, num steps=10
2022-12-29 18:23:34.563 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:23:36.057 INFO: Evaluation rollout: return=0.694 (0.0), episode length=5.0
2022-12-29 18:23:36.058 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:23:36.061 INFO: Iteration: 76/137, steps: 16416
2022-12-29 18:24:34.439 INFO: Training rollout: return=0.625 (0.1), episode length=5.0
2022-12-29 18:24:34.440 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:24:34.443 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-16416_train.pkl
2022-12-29 18:24:35.280 DEBUG: Taking gradient step
2022-12-29 18:24:35.289 DEBUG: Loss 0: {'policy_loss': 0.01005662535985839, 'entropy_loss': -0.023984671104699373, 'vf_loss': 0.0003104595428030927, 'total_loss': -0.013617586202037901, 'approx_kl': -2.9554939828813076e-08, 'clip_fraction': 0.0, 'grad_norm': 13.821253776550293}
2022-12-29 18:24:36.101 DEBUG: Taking gradient step
2022-12-29 18:24:36.111 DEBUG: Loss 1: {'policy_loss': -6.870766866100969e-05, 'entropy_loss': -0.024752008263021708, 'vf_loss': 0.0002945192516806782, 'total_loss': -0.024526196680002034, 'approx_kl': 0.008552791085094213, 'clip_fraction': 0.018229166977107525, 'grad_norm': 7.602695465087891}
2022-12-29 18:24:36.928 DEBUG: Taking gradient step
2022-12-29 18:24:36.937 DEBUG: Loss 2: {'policy_loss': 0.030549844849963457, 'entropy_loss': -0.024257106240838766, 'vf_loss': 0.00026728507228632853, 'total_loss': 0.006560023681411023, 'approx_kl': 0.021947807166725397, 'clip_fraction': 0.1705729179084301, 'grad_norm': 20.546798706054688}
2022-12-29 18:24:37.768 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 18:24:37.769 INFO: Optimization: policy loss=0.031, vf loss=0.000, entropy loss=-0.024, total loss=0.007, num steps=3
2022-12-29 18:24:37.769 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:24:39.183 INFO: Evaluation rollout: return=0.684 (0.0), episode length=5.0
2022-12-29 18:24:39.184 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:24:39.187 INFO: Iteration: 77/137, steps: 16632
2022-12-29 18:25:38.490 INFO: Training rollout: return=0.592 (0.1), episode length=5.0
2022-12-29 18:25:38.491 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:25:38.495 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-16632_train.pkl
2022-12-29 18:25:39.396 DEBUG: Taking gradient step
2022-12-29 18:25:39.406 DEBUG: Loss 0: {'policy_loss': -0.04306714991385068, 'entropy_loss': -0.024212009739130735, 'vf_loss': 0.00030689436556175463, 'total_loss': -0.06697226528741967, 'approx_kl': -2.109057639643197e-08, 'clip_fraction': 0.0, 'grad_norm': 19.846784591674805}
2022-12-29 18:25:40.220 DEBUG: Taking gradient step
2022-12-29 18:25:40.230 DEBUG: Loss 1: {'policy_loss': -0.023684587972775624, 'entropy_loss': -0.02491883747279644, 'vf_loss': 0.0003059278956224423, 'total_loss': -0.04829749754994962, 'approx_kl': 0.00914536218624562, 'clip_fraction': 0.01953125, 'grad_norm': 24.443984985351562}
2022-12-29 18:25:41.057 DEBUG: Taking gradient step
2022-12-29 18:25:41.067 DEBUG: Loss 2: {'policy_loss': 0.016281841419928347, 'entropy_loss': -0.025561027228832245, 'vf_loss': 0.0003009349968925192, 'total_loss': -0.008978250812011382, 'approx_kl': 0.03044252167455852, 'clip_fraction': 0.19140625, 'grad_norm': 18.204917907714844}
2022-12-29 18:25:41.875 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 18:25:41.875 INFO: Optimization: policy loss=0.016, vf loss=0.000, entropy loss=-0.026, total loss=-0.009, num steps=3
2022-12-29 18:25:41.876 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:25:43.326 INFO: Evaluation rollout: return=0.687 (0.0), episode length=5.0
2022-12-29 18:25:43.327 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:25:43.330 INFO: Iteration: 78/137, steps: 16848
2022-12-29 18:26:42.321 INFO: Training rollout: return=0.592 (0.1), episode length=5.0
2022-12-29 18:26:42.323 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:26:42.325 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-16848_train.pkl
2022-12-29 18:26:43.161 DEBUG: Taking gradient step
2022-12-29 18:26:43.171 DEBUG: Loss 0: {'policy_loss': -0.015733340004276196, 'entropy_loss': -0.024724374525249004, 'vf_loss': 0.000333501935803064, 'total_loss': -0.04012421259372213, 'approx_kl': -1.4590720409657365e-08, 'clip_fraction': 0.0, 'grad_norm': 14.821455001831055}
2022-12-29 18:26:44.055 DEBUG: Taking gradient step
2022-12-29 18:26:44.064 DEBUG: Loss 1: {'policy_loss': -0.05675934681527085, 'entropy_loss': -0.024488063529133797, 'vf_loss': 0.00034378609086886355, 'total_loss': -0.08090362425353578, 'approx_kl': 0.00034449660597601905, 'clip_fraction': 0.0, 'grad_norm': 14.201183319091797}
2022-12-29 18:26:44.862 DEBUG: Taking gradient step
2022-12-29 18:26:44.872 DEBUG: Loss 2: {'policy_loss': -0.022853973714959175, 'entropy_loss': -0.024619029369205236, 'vf_loss': 0.00034505664948566967, 'total_loss': -0.04712794643467874, 'approx_kl': 0.013056684751063585, 'clip_fraction': 0.1015625, 'grad_norm': 6.956263065338135}
2022-12-29 18:26:45.642 DEBUG: Taking gradient step
2022-12-29 18:26:45.653 DEBUG: Loss 3: {'policy_loss': 0.03780377660609979, 'entropy_loss': -0.025268208235502243, 'vf_loss': 0.0003519972706420689, 'total_loss': 0.01288756564123962, 'approx_kl': 0.01277886750176549, 'clip_fraction': 0.1822916716337204, 'grad_norm': 28.52141761779785}
2022-12-29 18:26:46.461 DEBUG: Taking gradient step
2022-12-29 18:26:46.470 DEBUG: Loss 4: {'policy_loss': 0.012072974875154036, 'entropy_loss': -0.024591473396867514, 'vf_loss': 0.0003543628446663358, 'total_loss': -0.01216413567704714, 'approx_kl': 0.03435438917949796, 'clip_fraction': 0.1536458358168602, 'grad_norm': 9.307032585144043}
2022-12-29 18:26:47.296 DEBUG: Taking gradient step
2022-12-29 18:26:47.306 DEBUG: Loss 5: {'policy_loss': -0.01582713687915174, 'entropy_loss': -0.023656435310840607, 'vf_loss': 0.00035312697577042024, 'total_loss': -0.03913044521422192, 'approx_kl': 0.025836284272372723, 'clip_fraction': 0.11067708395421505, 'grad_norm': 5.823383808135986}
2022-12-29 18:26:48.158 DEBUG: Taking gradient step
2022-12-29 18:26:48.168 DEBUG: Loss 6: {'policy_loss': -0.016224083312889076, 'entropy_loss': -0.024561371188610792, 'vf_loss': 0.00036577636840901104, 'total_loss': -0.04041967813309086, 'approx_kl': 0.03375321254134178, 'clip_fraction': 0.1419270858168602, 'grad_norm': 8.765787124633789}
2022-12-29 18:26:48.992 DEBUG: Early stopping at step 7 for reaching max KL.
2022-12-29 18:26:48.992 INFO: Optimization: policy loss=-0.016, vf loss=0.000, entropy loss=-0.025, total loss=-0.040, num steps=7
2022-12-29 18:26:48.993 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:26:50.497 INFO: Evaluation rollout: return=0.679 (0.0), episode length=5.0
2022-12-29 18:26:50.498 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:26:50.501 INFO: Iteration: 79/137, steps: 17064
2022-12-29 18:27:22.998 DEBUG: Atoms are too close
2022-12-29 18:27:48.992 INFO: Training rollout: return=0.038 (3.3), episode length=5.0
2022-12-29 18:27:48.994 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:27:48.997 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-17064_train.pkl
2022-12-29 18:27:49.846 DEBUG: Taking gradient step
2022-12-29 18:27:49.856 DEBUG: Loss 0: {'policy_loss': -0.03328562806447416, 'entropy_loss': -0.023808450903743505, 'vf_loss': 0.00837657509938587, 'total_loss': -0.04871750386883179, 'approx_kl': 2.1381616477356147e-08, 'clip_fraction': 0.0, 'grad_norm': 8.120030403137207}
2022-12-29 18:27:50.698 DEBUG: Taking gradient step
2022-12-29 18:27:50.708 DEBUG: Loss 1: {'policy_loss': 0.05651814835301764, 'entropy_loss': -0.02415182627737522, 'vf_loss': 0.013538903164859364, 'total_loss': 0.045905225240501786, 'approx_kl': 0.004740677773952484, 'clip_fraction': 0.01953125, 'grad_norm': 9.284597396850586}
2022-12-29 18:27:51.529 DEBUG: Taking gradient step
2022-12-29 18:27:51.538 DEBUG: Loss 2: {'policy_loss': -0.029010308309834015, 'entropy_loss': -0.023121079429984093, 'vf_loss': 0.008351844767735837, 'total_loss': -0.043779542972082267, 'approx_kl': 0.01927760592661798, 'clip_fraction': 0.2096354179084301, 'grad_norm': 1.974516749382019}
2022-12-29 18:27:52.358 DEBUG: Taking gradient step
2022-12-29 18:27:52.368 DEBUG: Loss 3: {'policy_loss': -0.04382396812778687, 'entropy_loss': -0.02343118702992797, 'vf_loss': 0.00835139517471462, 'total_loss': -0.05890375998300022, 'approx_kl': 0.03890377248171717, 'clip_fraction': 0.3177083358168602, 'grad_norm': 3.7212042808532715}
2022-12-29 18:27:53.184 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 18:27:53.184 INFO: Optimization: policy loss=-0.044, vf loss=0.008, entropy loss=-0.023, total loss=-0.059, num steps=4
2022-12-29 18:27:53.185 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:27:54.703 INFO: Evaluation rollout: return=0.685 (0.0), episode length=5.0
2022-12-29 18:27:54.704 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:27:54.707 INFO: Iteration: 80/137, steps: 17280
2022-12-29 18:28:53.338 INFO: Training rollout: return=0.591 (0.1), episode length=5.0
2022-12-29 18:28:53.340 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:28:53.342 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-17280_train.pkl
2022-12-29 18:28:54.234 DEBUG: Taking gradient step
2022-12-29 18:28:54.243 DEBUG: Loss 0: {'policy_loss': 0.04365311312659993, 'entropy_loss': -0.023511101491749287, 'vf_loss': 0.0003305300582135639, 'total_loss': 0.020472541693064214, 'approx_kl': -1.3380001462337532e-07, 'clip_fraction': 0.0, 'grad_norm': 13.991509437561035}
2022-12-29 18:28:55.114 DEBUG: Taking gradient step
2022-12-29 18:28:55.123 DEBUG: Loss 1: {'policy_loss': -0.005157717116798216, 'entropy_loss': -0.024355981033295393, 'vf_loss': 0.00033321608221697666, 'total_loss': -0.029180482067876636, 'approx_kl': 0.00723862717859447, 'clip_fraction': 0.05989583395421505, 'grad_norm': 3.9962832927703857}
2022-12-29 18:28:55.939 DEBUG: Taking gradient step
2022-12-29 18:28:55.948 DEBUG: Loss 2: {'policy_loss': 0.011380839171379626, 'entropy_loss': -0.02409585425630212, 'vf_loss': 0.00030604796675566775, 'total_loss': -0.01240896711816683, 'approx_kl': 0.014405793976038694, 'clip_fraction': 0.1966145858168602, 'grad_norm': 6.632877349853516}
2022-12-29 18:28:56.760 DEBUG: Taking gradient step
2022-12-29 18:28:56.780 DEBUG: Loss 3: {'policy_loss': -0.05308406301490068, 'entropy_loss': -0.02299433248117566, 'vf_loss': 0.00028373579445785824, 'total_loss': -0.07579465970161849, 'approx_kl': 0.024371226551011205, 'clip_fraction': 0.2682291716337204, 'grad_norm': 8.887109756469727}
2022-12-29 18:28:57.595 DEBUG: Taking gradient step
2022-12-29 18:28:57.605 DEBUG: Loss 4: {'policy_loss': -0.012184889171406418, 'entropy_loss': -0.022793889977037907, 'vf_loss': 0.000253289753430441, 'total_loss': -0.03472548939501388, 'approx_kl': 0.04457945376634598, 'clip_fraction': 0.2721354179084301, 'grad_norm': 8.300126075744629}
2022-12-29 18:28:58.410 DEBUG: Taking gradient step
2022-12-29 18:28:58.421 DEBUG: Loss 5: {'policy_loss': -0.03885549875461905, 'entropy_loss': -0.022823185194283724, 'vf_loss': 0.00024997062296360867, 'total_loss': -0.06142871332593917, 'approx_kl': 0.029054452275886433, 'clip_fraction': 0.19791666697710752, 'grad_norm': 8.95137882232666}
2022-12-29 18:28:59.238 DEBUG: Taking gradient step
2022-12-29 18:28:59.247 DEBUG: Loss 6: {'policy_loss': 0.016846042682609655, 'entropy_loss': -0.02344301948323846, 'vf_loss': 0.00023461511216272456, 'total_loss': -0.00636236168846608, 'approx_kl': 0.030596058699302375, 'clip_fraction': 0.2096354216337204, 'grad_norm': 14.057442665100098}
2022-12-29 18:29:00.061 DEBUG: Taking gradient step
2022-12-29 18:29:00.071 DEBUG: Loss 7: {'policy_loss': 0.004117299519984331, 'entropy_loss': -0.02383861504495144, 'vf_loss': 0.0002295351804625529, 'total_loss': -0.019491780344504554, 'approx_kl': 0.018219779478386045, 'clip_fraction': 0.08984375, 'grad_norm': 13.588568687438965}
2022-12-29 18:29:00.900 DEBUG: Taking gradient step
2022-12-29 18:29:00.910 DEBUG: Loss 8: {'policy_loss': 0.009458380739665477, 'entropy_loss': -0.023641555570065975, 'vf_loss': 0.00021410884290078911, 'total_loss': -0.01396906598749971, 'approx_kl': 0.026461688452400267, 'clip_fraction': 0.1653645858168602, 'grad_norm': 14.036458969116211}
2022-12-29 18:29:01.736 DEBUG: Taking gradient step
2022-12-29 18:29:01.745 DEBUG: Loss 9: {'policy_loss': 0.03182833524056083, 'entropy_loss': -0.02369317226111889, 'vf_loss': 0.00020532430796877733, 'total_loss': 0.008340487287410714, 'approx_kl': 0.019157496746629477, 'clip_fraction': 0.1861979179084301, 'grad_norm': 15.719621658325195}
2022-12-29 18:29:01.745 INFO: Optimization: policy loss=0.032, vf loss=0.000, entropy loss=-0.024, total loss=0.008, num steps=10
2022-12-29 18:29:01.746 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:29:03.151 INFO: Evaluation rollout: return=0.706 (0.0), episode length=5.0
2022-12-29 18:29:03.152 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:29:03.155 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-15336.model
2022-12-29 18:29:03.160 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-17496.model
2022-12-29 18:29:03.190 INFO: Iteration: 81/137, steps: 17496
2022-12-29 18:29:34.987 DEBUG: There is a single atom floating around
2022-12-29 18:30:02.080 INFO: Training rollout: return=0.072 (3.3), episode length=5.0
2022-12-29 18:30:02.081 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:30:02.084 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-17496_train.pkl
2022-12-29 18:30:02.921 DEBUG: Taking gradient step
2022-12-29 18:30:02.930 DEBUG: Loss 0: {'policy_loss': 0.02030057834499338, 'entropy_loss': -0.022612077184021473, 'vf_loss': 0.01063564940746219, 'total_loss': 0.008324150568434094, 'approx_kl': -2.7532223612070084e-08, 'clip_fraction': 0.0, 'grad_norm': 36.41449737548828}
2022-12-29 18:30:03.760 DEBUG: Taking gradient step
2022-12-29 18:30:03.774 DEBUG: Loss 1: {'policy_loss': 0.06072526754969482, 'entropy_loss': -0.02266289573162794, 'vf_loss': 0.012870938896808435, 'total_loss': 0.05093331071487531, 'approx_kl': -0.000775107357185334, 'clip_fraction': 0.00390625, 'grad_norm': 7.760799407958984}
2022-12-29 18:30:04.624 DEBUG: Taking gradient step
2022-12-29 18:30:04.633 DEBUG: Loss 2: {'policy_loss': -0.03403537797043843, 'entropy_loss': -0.02305017225444317, 'vf_loss': 0.00789715305526915, 'total_loss': -0.04918839716961246, 'approx_kl': 0.001496080425567925, 'clip_fraction': 0.01171875, 'grad_norm': 5.9532999992370605}
2022-12-29 18:30:05.510 DEBUG: Taking gradient step
2022-12-29 18:30:05.520 DEBUG: Loss 3: {'policy_loss': -0.04025507339260636, 'entropy_loss': -0.02214553626254201, 'vf_loss': 0.00787827255489936, 'total_loss': -0.05452233710024901, 'approx_kl': 0.017418623669072986, 'clip_fraction': 0.07682291697710752, 'grad_norm': 5.078080654144287}
2022-12-29 18:30:06.356 DEBUG: Taking gradient step
2022-12-29 18:30:06.365 DEBUG: Loss 4: {'policy_loss': -0.0422842966229069, 'entropy_loss': -0.022377417888492346, 'vf_loss': 0.007867273571093902, 'total_loss': -0.05679444094030535, 'approx_kl': 0.03925904352217913, 'clip_fraction': 0.30859375, 'grad_norm': 3.393561601638794}
2022-12-29 18:30:07.181 DEBUG: Early stopping at step 5 for reaching max KL.
2022-12-29 18:30:07.181 INFO: Optimization: policy loss=-0.042, vf loss=0.008, entropy loss=-0.022, total loss=-0.057, num steps=5
2022-12-29 18:30:07.181 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:30:08.616 INFO: Evaluation rollout: return=0.734 (0.0), episode length=5.0
2022-12-29 18:30:08.617 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:30:08.620 INFO: Iteration: 82/137, steps: 17712
2022-12-29 18:31:07.527 INFO: Training rollout: return=0.616 (0.1), episode length=5.0
2022-12-29 18:31:07.528 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:31:07.530 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-17712_train.pkl
2022-12-29 18:31:08.383 DEBUG: Taking gradient step
2022-12-29 18:31:08.393 DEBUG: Loss 0: {'policy_loss': 0.0383750766240188, 'entropy_loss': -0.023067144211381674, 'vf_loss': 0.00012341091517116342, 'total_loss': 0.01543134332780828, 'approx_kl': -2.716357805354619e-09, 'clip_fraction': 0.0, 'grad_norm': 11.83806037902832}
2022-12-29 18:31:09.277 DEBUG: Taking gradient step
2022-12-29 18:31:09.288 DEBUG: Loss 1: {'policy_loss': 0.057483006677245346, 'entropy_loss': -0.02327040070667863, 'vf_loss': 0.00012409403166412778, 'total_loss': 0.03433670000223085, 'approx_kl': 0.007851537608075887, 'clip_fraction': 0.03515625, 'grad_norm': 14.434942245483398}
2022-12-29 18:31:10.105 DEBUG: Taking gradient step
2022-12-29 18:31:10.115 DEBUG: Loss 2: {'policy_loss': -0.029307312498812427, 'entropy_loss': -0.022589026018977165, 'vf_loss': 0.00013461466822753236, 'total_loss': -0.051761723849562055, 'approx_kl': 0.02348299417644739, 'clip_fraction': 0.23046875, 'grad_norm': 13.105177879333496}
2022-12-29 18:31:10.962 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 18:31:10.963 INFO: Optimization: policy loss=-0.029, vf loss=0.000, entropy loss=-0.023, total loss=-0.052, num steps=3
2022-12-29 18:31:10.963 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:31:12.422 INFO: Evaluation rollout: return=0.748 (0.0), episode length=5.0
2022-12-29 18:31:12.423 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:31:12.426 INFO: Iteration: 83/137, steps: 17928
2022-12-29 18:32:11.307 INFO: Training rollout: return=0.634 (0.1), episode length=5.0
2022-12-29 18:32:11.309 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:32:11.311 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-17928_train.pkl
2022-12-29 18:32:12.144 DEBUG: Taking gradient step
2022-12-29 18:32:12.154 DEBUG: Loss 0: {'policy_loss': 0.016323649391184667, 'entropy_loss': -0.0231741308234632, 'vf_loss': 0.00011254724572720217, 'total_loss': -0.006737934186551334, 'approx_kl': -1.482355038717742e-08, 'clip_fraction': 0.0, 'grad_norm': 5.367452144622803}
2022-12-29 18:32:12.945 DEBUG: Taking gradient step
2022-12-29 18:32:12.954 DEBUG: Loss 1: {'policy_loss': -0.044672921462965524, 'entropy_loss': -0.021784676238894463, 'vf_loss': 0.00012110051437527104, 'total_loss': -0.06633649718748472, 'approx_kl': -0.0006034533143974841, 'clip_fraction': 0.0, 'grad_norm': 6.665836811065674}
2022-12-29 18:32:13.753 DEBUG: Taking gradient step
2022-12-29 18:32:13.763 DEBUG: Loss 2: {'policy_loss': -0.0019279547013291778, 'entropy_loss': -0.021905267145484686, 'vf_loss': 0.000109838805012568, 'total_loss': -0.0237233830418013, 'approx_kl': 0.017398539697751403, 'clip_fraction': 0.0703125, 'grad_norm': 5.570722579956055}
2022-12-29 18:32:14.626 DEBUG: Taking gradient step
2022-12-29 18:32:14.635 DEBUG: Loss 3: {'policy_loss': -0.006317158886316888, 'entropy_loss': -0.02234016777947545, 'vf_loss': 0.00011315507752310019, 'total_loss': -0.02854417158826924, 'approx_kl': 0.04211501660756767, 'clip_fraction': 0.2291666716337204, 'grad_norm': 10.967196464538574}
2022-12-29 18:32:15.504 DEBUG: Taking gradient step
2022-12-29 18:32:15.513 DEBUG: Loss 4: {'policy_loss': 0.015016377065837888, 'entropy_loss': -0.021576269529759884, 'vf_loss': 0.00010992099793276391, 'total_loss': -0.006449971465989232, 'approx_kl': 0.03324596071615815, 'clip_fraction': 0.3059895858168602, 'grad_norm': 11.29605770111084}
2022-12-29 18:32:16.400 DEBUG: Early stopping at step 5 for reaching max KL.
2022-12-29 18:32:16.401 INFO: Optimization: policy loss=0.015, vf loss=0.000, entropy loss=-0.022, total loss=-0.006, num steps=5
2022-12-29 18:32:16.401 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:32:17.836 INFO: Evaluation rollout: return=0.763 (0.0), episode length=5.0
2022-12-29 18:32:17.837 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:32:17.839 INFO: Iteration: 84/137, steps: 18144
2022-12-29 18:32:56.638 DEBUG: Atoms are too close
2022-12-29 18:33:05.133 DEBUG: Atoms are too close
2022-12-29 18:33:15.564 INFO: Training rollout: return=-0.451 (4.6), episode length=4.9
2022-12-29 18:33:15.566 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:33:15.568 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-18144_train.pkl
2022-12-29 18:33:16.396 DEBUG: Taking gradient step
2022-12-29 18:33:16.407 DEBUG: Loss 0: {'policy_loss': 0.006259053956539057, 'entropy_loss': -0.02132103592157364, 'vf_loss': 0.015010367651922594, 'total_loss': -5.161431311199137e-05, 'approx_kl': 1.909211277961731e-08, 'clip_fraction': 0.0, 'grad_norm': 38.98100662231445}
2022-12-29 18:33:17.214 DEBUG: Taking gradient step
2022-12-29 18:33:17.224 DEBUG: Loss 1: {'policy_loss': 0.0011884426869225116, 'entropy_loss': -0.020646105520427227, 'vf_loss': 0.014974420307093742, 'total_loss': -0.004483242526410972, 'approx_kl': -0.0017275689751841128, 'clip_fraction': 0.015625, 'grad_norm': 5.726210117340088}
2022-12-29 18:33:18.019 DEBUG: Taking gradient step
2022-12-29 18:33:18.029 DEBUG: Loss 2: {'policy_loss': 0.05693884809225257, 'entropy_loss': -0.020194513257592916, 'vf_loss': 0.020220058239920964, 'total_loss': 0.0569643930745806, 'approx_kl': 0.007355774519965053, 'clip_fraction': 0.04817708395421505, 'grad_norm': 11.911173820495605}
2022-12-29 18:33:18.839 DEBUG: Taking gradient step
2022-12-29 18:33:18.849 DEBUG: Loss 3: {'policy_loss': -0.04131454285300226, 'entropy_loss': -0.020560569129884243, 'vf_loss': 0.0123728529472444, 'total_loss': -0.04950225903564211, 'approx_kl': -0.003230274887755513, 'clip_fraction': 0.06380208395421505, 'grad_norm': 5.549940586090088}
2022-12-29 18:33:19.652 DEBUG: Taking gradient step
2022-12-29 18:33:19.663 DEBUG: Loss 4: {'policy_loss': -0.018413459566593522, 'entropy_loss': -0.019380562007427216, 'vf_loss': 0.014970190263805128, 'total_loss': -0.022823831310215613, 'approx_kl': 0.0036729080602526665, 'clip_fraction': 0.14583333395421505, 'grad_norm': 4.629271030426025}
2022-12-29 18:33:20.491 DEBUG: Taking gradient step
2022-12-29 18:33:20.502 DEBUG: Loss 5: {'policy_loss': -0.04743982512345078, 'entropy_loss': -0.02000727178528905, 'vf_loss': 0.012346539023021378, 'total_loss': -0.05510055788571846, 'approx_kl': 0.014923489186912775, 'clip_fraction': 0.3489583358168602, 'grad_norm': 5.844191551208496}
2022-12-29 18:33:21.378 DEBUG: Taking gradient step
2022-12-29 18:33:21.392 DEBUG: Loss 6: {'policy_loss': -0.04871774257114858, 'entropy_loss': -0.020160941407084465, 'vf_loss': 0.012338014319012783, 'total_loss': -0.05654066965922026, 'approx_kl': 0.03492890018969774, 'clip_fraction': 0.4036458358168602, 'grad_norm': 5.603122234344482}
2022-12-29 18:33:22.267 DEBUG: Taking gradient step
2022-12-29 18:33:22.277 DEBUG: Loss 7: {'policy_loss': -0.02346767763612611, 'entropy_loss': -0.01992572983726859, 'vf_loss': 0.014839949776580479, 'total_loss': -0.02855345769681422, 'approx_kl': 0.035495639545843005, 'clip_fraction': 0.4348958358168602, 'grad_norm': 4.7356181144714355}
2022-12-29 18:33:23.087 DEBUG: Taking gradient step
2022-12-29 18:33:23.097 DEBUG: Loss 8: {'policy_loss': -0.05330103077080425, 'entropy_loss': -0.01980959391221404, 'vf_loss': 0.012316282976827247, 'total_loss': -0.06079434170619104, 'approx_kl': 0.04144139285199344, 'clip_fraction': 0.4453125, 'grad_norm': 4.460260391235352}
2022-12-29 18:33:23.908 DEBUG: Taking gradient step
2022-12-29 18:33:23.918 DEBUG: Loss 9: {'policy_loss': -0.033669907848479175, 'entropy_loss': -0.020401242654770613, 'vf_loss': 0.01491486642029164, 'total_loss': -0.039156284082958154, 'approx_kl': 0.017349601723253727, 'clip_fraction': 0.3997395858168602, 'grad_norm': 3.695258378982544}
2022-12-29 18:33:23.918 INFO: Optimization: policy loss=-0.034, vf loss=0.015, entropy loss=-0.020, total loss=-0.039, num steps=10
2022-12-29 18:33:23.918 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:33:25.352 INFO: Evaluation rollout: return=0.757 (0.0), episode length=5.0
2022-12-29 18:33:25.353 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:33:25.355 INFO: Iteration: 85/137, steps: 18360
2022-12-29 18:34:14.378 DEBUG: Atoms are too close
2022-12-29 18:34:23.579 INFO: Training rollout: return=0.087 (3.3), episode length=5.0
2022-12-29 18:34:23.580 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:34:23.582 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-18360_train.pkl
2022-12-29 18:34:24.409 DEBUG: Taking gradient step
2022-12-29 18:34:24.422 DEBUG: Loss 0: {'policy_loss': -0.024923324148334217, 'entropy_loss': -0.019252901431173086, 'vf_loss': 0.00785965229766853, 'total_loss': -0.03631657328183878, 'approx_kl': -7.528189627237225e-09, 'clip_fraction': 0.0, 'grad_norm': 8.984039306640625}
2022-12-29 18:34:25.216 DEBUG: Taking gradient step
2022-12-29 18:34:25.227 DEBUG: Loss 1: {'policy_loss': -0.027462455620191072, 'entropy_loss': -0.0192848052829504, 'vf_loss': 0.007854284224853193, 'total_loss': -0.038892976678288275, 'approx_kl': 0.004217866284307092, 'clip_fraction': 0.041666666977107525, 'grad_norm': 8.810372352600098}
2022-12-29 18:34:26.098 DEBUG: Taking gradient step
2022-12-29 18:34:26.112 DEBUG: Loss 2: {'policy_loss': -0.03563056539791773, 'entropy_loss': -0.01980369444936514, 'vf_loss': 0.007861696394152354, 'total_loss': -0.047572563453130504, 'approx_kl': 0.020531818503513932, 'clip_fraction': 0.2421875, 'grad_norm': 3.3864176273345947}
2022-12-29 18:34:26.938 DEBUG: Taking gradient step
2022-12-29 18:34:26.948 DEBUG: Loss 3: {'policy_loss': 0.010535061124654254, 'entropy_loss': -0.01872681500390172, 'vf_loss': 0.0104808084715504, 'total_loss': 0.0022890545923029415, 'approx_kl': 0.031163354171440005, 'clip_fraction': 0.3177083358168602, 'grad_norm': 2.575962781906128}
2022-12-29 18:34:27.766 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 18:34:27.766 INFO: Optimization: policy loss=0.011, vf loss=0.010, entropy loss=-0.019, total loss=0.002, num steps=4
2022-12-29 18:34:27.766 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:34:29.475 INFO: Evaluation rollout: return=0.775 (0.0), episode length=5.0
2022-12-29 18:34:29.476 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:34:29.478 INFO: Iteration: 86/137, steps: 18576
2022-12-29 18:35:01.835 DEBUG: Atoms are too close
2022-12-29 18:35:28.189 INFO: Training rollout: return=0.095 (3.4), episode length=5.0
2022-12-29 18:35:28.191 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:35:28.194 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-18576_train.pkl
2022-12-29 18:35:29.035 DEBUG: Taking gradient step
2022-12-29 18:35:29.044 DEBUG: Loss 0: {'policy_loss': -0.025470965101582248, 'entropy_loss': -0.01881513837724924, 'vf_loss': 0.007948234257668847, 'total_loss': -0.036337869221162646, 'approx_kl': 1.0710209608078003e-08, 'clip_fraction': 0.0, 'grad_norm': 11.40245246887207}
2022-12-29 18:35:29.879 DEBUG: Taking gradient step
2022-12-29 18:35:29.889 DEBUG: Loss 1: {'policy_loss': 0.01431988474013643, 'entropy_loss': -0.019396537449210882, 'vf_loss': 0.010498977970586795, 'total_loss': 0.005422325261512345, 'approx_kl': -0.002101914258673787, 'clip_fraction': 0.022135416977107525, 'grad_norm': 12.810453414916992}
2022-12-29 18:35:30.702 DEBUG: Taking gradient step
2022-12-29 18:35:30.712 DEBUG: Loss 2: {'policy_loss': -0.03161074854209195, 'entropy_loss': -0.01853898586705327, 'vf_loss': 0.00795089710607097, 'total_loss': -0.04219883730307425, 'approx_kl': -0.0039689207915216684, 'clip_fraction': 0.19921875, 'grad_norm': 9.307827949523926}
2022-12-29 18:35:31.550 DEBUG: Taking gradient step
2022-12-29 18:35:31.560 DEBUG: Loss 3: {'policy_loss': -0.03640436828611908, 'entropy_loss': -0.018306230194866657, 'vf_loss': 0.007948831381329964, 'total_loss': -0.04676176709965578, 'approx_kl': 0.013141525909304619, 'clip_fraction': 0.2747395858168602, 'grad_norm': 7.248185157775879}
2022-12-29 18:35:32.358 DEBUG: Taking gradient step
2022-12-29 18:35:32.367 DEBUG: Loss 4: {'policy_loss': -0.033320809840973825, 'entropy_loss': -0.01756625296548009, 'vf_loss': 0.00794476834040463, 'total_loss': -0.04294229446604929, 'approx_kl': 0.021326410118490458, 'clip_fraction': 0.3125, 'grad_norm': 6.969141006469727}
2022-12-29 18:35:33.186 DEBUG: Taking gradient step
2022-12-29 18:35:33.196 DEBUG: Loss 5: {'policy_loss': -0.036558453508728085, 'entropy_loss': -0.0184486648067832, 'vf_loss': 0.007947226946155192, 'total_loss': -0.04705989136935609, 'approx_kl': 0.030029035871848464, 'clip_fraction': 0.2434895858168602, 'grad_norm': 3.572587728500366}
2022-12-29 18:35:33.998 DEBUG: Taking gradient step
2022-12-29 18:35:34.009 DEBUG: Loss 6: {'policy_loss': -0.04182729577740454, 'entropy_loss': -0.018066419754177332, 'vf_loss': 0.007952365095078129, 'total_loss': -0.05194135043650375, 'approx_kl': 0.023067129077389836, 'clip_fraction': 0.2109375, 'grad_norm': 4.19946813583374}
2022-12-29 18:35:34.851 DEBUG: Taking gradient step
2022-12-29 18:35:34.865 DEBUG: Loss 7: {'policy_loss': -0.04038696287287467, 'entropy_loss': -0.01828576810657978, 'vf_loss': 0.007954227522940768, 'total_loss': -0.05071850345651368, 'approx_kl': 0.028995963046327233, 'clip_fraction': 0.2252604216337204, 'grad_norm': 4.255570888519287}
2022-12-29 18:35:35.756 DEBUG: Taking gradient step
2022-12-29 18:35:35.772 DEBUG: Loss 8: {'policy_loss': 0.003391409119057519, 'entropy_loss': -0.01839501829817891, 'vf_loss': 0.010488267198709246, 'total_loss': -0.004515341980412146, 'approx_kl': 0.023762251250445843, 'clip_fraction': 0.23828125, 'grad_norm': 4.90952205657959}
2022-12-29 18:35:36.623 DEBUG: Early stopping at step 9 for reaching max KL.
2022-12-29 18:35:36.623 INFO: Optimization: policy loss=0.003, vf loss=0.010, entropy loss=-0.018, total loss=-0.005, num steps=9
2022-12-29 18:35:36.624 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:35:38.111 INFO: Evaluation rollout: return=0.775 (0.0), episode length=5.0
2022-12-29 18:35:38.112 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:35:38.115 INFO: Iteration: 87/137, steps: 18792
2022-12-29 18:36:36.540 INFO: Training rollout: return=0.652 (0.1), episode length=5.0
2022-12-29 18:36:36.541 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:36:36.544 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-18792_train.pkl
2022-12-29 18:36:37.392 DEBUG: Taking gradient step
2022-12-29 18:36:37.402 DEBUG: Loss 0: {'policy_loss': -0.027230156927511694, 'entropy_loss': -0.018668875563889742, 'vf_loss': 0.00030321318130466213, 'total_loss': -0.04559581931009678, 'approx_kl': -3.023887984454632e-08, 'clip_fraction': 0.0, 'grad_norm': 12.569924354553223}
2022-12-29 18:36:38.237 DEBUG: Taking gradient step
2022-12-29 18:36:38.251 DEBUG: Loss 1: {'policy_loss': -0.0003624709567615486, 'entropy_loss': -0.018744153436273336, 'vf_loss': 0.0002906919768832705, 'total_loss': -0.018815932416151614, 'approx_kl': -0.002024299814365804, 'clip_fraction': 0.05208333395421505, 'grad_norm': 8.490423202514648}
2022-12-29 18:36:39.096 DEBUG: Taking gradient step
2022-12-29 18:36:39.105 DEBUG: Loss 2: {'policy_loss': -0.014091753721454471, 'entropy_loss': -0.018366414587944746, 'vf_loss': 0.0002938491972734649, 'total_loss': -0.03216431911212575, 'approx_kl': -0.0048844070406630635, 'clip_fraction': 0.2395833358168602, 'grad_norm': 18.09573745727539}
2022-12-29 18:36:39.989 DEBUG: Taking gradient step
2022-12-29 18:36:40.005 DEBUG: Loss 3: {'policy_loss': 0.014506620424955045, 'entropy_loss': -0.01865123910829425, 'vf_loss': 0.0002881086625806527, 'total_loss': -0.0038565100207585516, 'approx_kl': -0.004793328698724508, 'clip_fraction': 0.28515625, 'grad_norm': 18.731292724609375}
2022-12-29 18:36:40.858 DEBUG: Taking gradient step
2022-12-29 18:36:40.873 DEBUG: Loss 4: {'policy_loss': -0.03922402073553108, 'entropy_loss': -0.018686403520405293, 'vf_loss': 0.0002874256235417182, 'total_loss': -0.05762299863239465, 'approx_kl': 0.011863217689096928, 'clip_fraction': 0.3190104216337204, 'grad_norm': 15.202447891235352}
2022-12-29 18:36:41.784 DEBUG: Taking gradient step
2022-12-29 18:36:41.795 DEBUG: Loss 5: {'policy_loss': 0.02824036573978663, 'entropy_loss': -0.01806898834183812, 'vf_loss': 0.0002769159806676545, 'total_loss': 0.010448293378616164, 'approx_kl': 0.0018383683636784554, 'clip_fraction': 0.2369791679084301, 'grad_norm': 11.60920524597168}
2022-12-29 18:36:42.641 DEBUG: Taking gradient step
2022-12-29 18:36:42.650 DEBUG: Loss 6: {'policy_loss': 0.06649576524105645, 'entropy_loss': -0.016991882352158427, 'vf_loss': 0.0002587850091571759, 'total_loss': 0.049762667898055196, 'approx_kl': 0.02714841766282916, 'clip_fraction': 0.23828125, 'grad_norm': 12.7304105758667}
2022-12-29 18:36:43.485 DEBUG: Taking gradient step
2022-12-29 18:36:43.495 DEBUG: Loss 7: {'policy_loss': 0.022970025771321827, 'entropy_loss': -0.017395447241142392, 'vf_loss': 0.0002569705512649362, 'total_loss': 0.005831549081444377, 'approx_kl': 0.01994799729436636, 'clip_fraction': 0.2122395858168602, 'grad_norm': 13.593716621398926}
2022-12-29 18:36:44.317 DEBUG: Taking gradient step
2022-12-29 18:36:44.326 DEBUG: Loss 8: {'policy_loss': -0.03782470479402027, 'entropy_loss': -0.017080523539334536, 'vf_loss': 0.00026083666447537703, 'total_loss': -0.05464439166887943, 'approx_kl': 0.017398178344592452, 'clip_fraction': 0.1822916716337204, 'grad_norm': 15.456144332885742}
2022-12-29 18:36:45.174 DEBUG: Taking gradient step
2022-12-29 18:36:45.184 DEBUG: Loss 9: {'policy_loss': -0.003915094448472786, 'entropy_loss': -0.01677341526374221, 'vf_loss': 0.00024273843783115994, 'total_loss': -0.02044577127438384, 'approx_kl': -0.0023849825374782085, 'clip_fraction': 0.19661458395421505, 'grad_norm': 10.830523490905762}
2022-12-29 18:36:45.184 INFO: Optimization: policy loss=-0.004, vf loss=0.000, entropy loss=-0.017, total loss=-0.020, num steps=10
2022-12-29 18:36:45.184 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:36:46.627 INFO: Evaluation rollout: return=0.776 (0.0), episode length=5.0
2022-12-29 18:36:46.629 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:36:46.632 INFO: Iteration: 88/137, steps: 19008
2022-12-29 18:37:45.191 INFO: Training rollout: return=0.672 (0.1), episode length=5.0
2022-12-29 18:37:45.192 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:37:45.195 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-19008_train.pkl
2022-12-29 18:37:46.027 DEBUG: Taking gradient step
2022-12-29 18:37:46.037 DEBUG: Loss 0: {'policy_loss': 0.007231308401266947, 'entropy_loss': -0.016992913791909814, 'vf_loss': 0.00023742577430494886, 'total_loss': -0.009524179616337912, 'approx_kl': 4.9612330244031e-08, 'clip_fraction': 0.0, 'grad_norm': 12.873758316040039}
2022-12-29 18:37:46.833 DEBUG: Taking gradient step
2022-12-29 18:37:46.843 DEBUG: Loss 1: {'policy_loss': -0.043813857154779715, 'entropy_loss': -0.017070992616936564, 'vf_loss': 0.00023525014435720192, 'total_loss': -0.06064959962735908, 'approx_kl': -0.0011087946113548242, 'clip_fraction': 0.0, 'grad_norm': 13.604480743408203}
2022-12-29 18:37:47.659 DEBUG: Taking gradient step
2022-12-29 18:37:47.670 DEBUG: Loss 2: {'policy_loss': -0.01946108437924263, 'entropy_loss': -0.016130319330841303, 'vf_loss': 0.0002204240055398578, 'total_loss': -0.03537097970454408, 'approx_kl': 0.0008169710636138916, 'clip_fraction': 0.053385416977107525, 'grad_norm': 9.384177207946777}
2022-12-29 18:37:48.489 DEBUG: Taking gradient step
2022-12-29 18:37:48.498 DEBUG: Loss 3: {'policy_loss': 0.013075075125871041, 'entropy_loss': -0.015380017459392548, 'vf_loss': 0.0002021691833833649, 'total_loss': -0.0021027731501381403, 'approx_kl': 0.022691073187161237, 'clip_fraction': 0.3098958358168602, 'grad_norm': 10.55711841583252}
2022-12-29 18:37:49.333 DEBUG: Taking gradient step
2022-12-29 18:37:49.344 DEBUG: Loss 4: {'policy_loss': -0.004855550442272448, 'entropy_loss': -0.015248786192387342, 'vf_loss': 0.0001968486851307521, 'total_loss': -0.01990748794952904, 'approx_kl': 0.025779436924494803, 'clip_fraction': 0.3893229216337204, 'grad_norm': 10.9774808883667}
2022-12-29 18:37:50.213 DEBUG: Taking gradient step
2022-12-29 18:37:50.223 DEBUG: Loss 5: {'policy_loss': -0.008531812758004057, 'entropy_loss': -0.014974565478041768, 'vf_loss': 0.00018567307808556443, 'total_loss': -0.02332070515796026, 'approx_kl': 0.023985491134226322, 'clip_fraction': 0.39453125, 'grad_norm': 9.849186897277832}
2022-12-29 18:37:51.069 DEBUG: Taking gradient step
2022-12-29 18:37:51.080 DEBUG: Loss 6: {'policy_loss': -0.005529192191031821, 'entropy_loss': -0.014147407142445445, 'vf_loss': 0.000181260278739865, 'total_loss': -0.0194953390547374, 'approx_kl': 0.023314769845455885, 'clip_fraction': 0.3190104216337204, 'grad_norm': 4.340851306915283}
2022-12-29 18:37:51.888 DEBUG: Taking gradient step
2022-12-29 18:37:51.898 DEBUG: Loss 7: {'policy_loss': -0.02977549588381249, 'entropy_loss': -0.013892176793888211, 'vf_loss': 0.0001700572584523407, 'total_loss': -0.04349761541924836, 'approx_kl': 0.014581106370314956, 'clip_fraction': 0.27734375, 'grad_norm': 11.43097972869873}
2022-12-29 18:37:52.704 DEBUG: Taking gradient step
2022-12-29 18:37:52.714 DEBUG: Loss 8: {'policy_loss': 0.009010346649349243, 'entropy_loss': -0.014275796944275498, 'vf_loss': 0.00015347951500826638, 'total_loss': -0.005111970779917992, 'approx_kl': 0.021435723640024662, 'clip_fraction': 0.2408854216337204, 'grad_norm': 14.010595321655273}
2022-12-29 18:37:53.544 DEBUG: Taking gradient step
2022-12-29 18:37:53.558 DEBUG: Loss 9: {'policy_loss': 0.005730887444029116, 'entropy_loss': -0.013588342582806945, 'vf_loss': 0.00014290643939515865, 'total_loss': -0.0077145486993826715, 'approx_kl': 0.012565685901790857, 'clip_fraction': 0.18359375, 'grad_norm': 12.298314094543457}
2022-12-29 18:37:53.558 INFO: Optimization: policy loss=0.006, vf loss=0.000, entropy loss=-0.014, total loss=-0.008, num steps=10
2022-12-29 18:37:53.559 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:37:55.069 INFO: Evaluation rollout: return=0.782 (0.0), episode length=5.0
2022-12-29 18:37:55.070 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:37:55.073 INFO: Iteration: 89/137, steps: 19224
2022-12-29 18:38:53.663 INFO: Training rollout: return=0.687 (0.1), episode length=5.0
2022-12-29 18:38:53.665 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:38:53.667 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-19224_train.pkl
2022-12-29 18:38:54.577 DEBUG: Taking gradient step
2022-12-29 18:38:54.587 DEBUG: Loss 0: {'policy_loss': 0.039242764111468605, 'entropy_loss': -0.013768806587904692, 'vf_loss': 0.00014032927709203353, 'total_loss': 0.02561428680065595, 'approx_kl': 1.2320621678441057e-07, 'clip_fraction': 0.0, 'grad_norm': 16.058143615722656}
2022-12-29 18:38:55.434 DEBUG: Taking gradient step
2022-12-29 18:38:55.444 DEBUG: Loss 1: {'policy_loss': -0.011483477534190354, 'entropy_loss': -0.013173416955396533, 'vf_loss': 0.0001392815990553349, 'total_loss': -0.024517612890531552, 'approx_kl': 0.01718836184591055, 'clip_fraction': 0.049479166977107525, 'grad_norm': 15.972236633300781}
2022-12-29 18:38:56.276 DEBUG: Taking gradient step
2022-12-29 18:38:56.286 DEBUG: Loss 2: {'policy_loss': -0.014822548059246274, 'entropy_loss': -0.01305157388560474, 'vf_loss': 0.00012805564945885244, 'total_loss': -0.027746066295392155, 'approx_kl': 0.03599990252405405, 'clip_fraction': 0.15755208395421505, 'grad_norm': 13.064302444458008}
2022-12-29 18:38:57.149 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 18:38:57.149 INFO: Optimization: policy loss=-0.015, vf loss=0.000, entropy loss=-0.013, total loss=-0.028, num steps=3
2022-12-29 18:38:57.150 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:38:58.599 INFO: Evaluation rollout: return=0.770 (0.0), episode length=5.0
2022-12-29 18:38:58.600 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:38:58.602 INFO: Iteration: 90/137, steps: 19440
2022-12-29 18:39:57.634 INFO: Training rollout: return=0.668 (0.1), episode length=5.0
2022-12-29 18:39:57.635 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:39:57.637 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-19440_train.pkl
2022-12-29 18:39:58.454 DEBUG: Taking gradient step
2022-12-29 18:39:58.463 DEBUG: Loss 0: {'policy_loss': 0.014158434298215297, 'entropy_loss': -0.012937608640640974, 'vf_loss': 0.00010581392358069514, 'total_loss': 0.001326639581155014, 'approx_kl': 4.516914486885071e-08, 'clip_fraction': 0.0, 'grad_norm': 20.608108520507812}
2022-12-29 18:39:59.264 DEBUG: Taking gradient step
2022-12-29 18:39:59.273 DEBUG: Loss 1: {'policy_loss': 0.0075646007841665, 'entropy_loss': -0.012278617592528462, 'vf_loss': 9.948697685085748e-05, 'total_loss': -0.004614529831511107, 'approx_kl': 0.01675630093086511, 'clip_fraction': 0.10026041697710752, 'grad_norm': 13.011868476867676}
2022-12-29 18:40:00.169 DEBUG: Taking gradient step
2022-12-29 18:40:00.179 DEBUG: Loss 2: {'policy_loss': 0.009940574113286802, 'entropy_loss': -0.012279137969017029, 'vf_loss': 9.30839176585845e-05, 'total_loss': -0.0022454799380716393, 'approx_kl': 0.038327408488839865, 'clip_fraction': 0.2421875, 'grad_norm': 19.4337158203125}
2022-12-29 18:40:01.044 DEBUG: Taking gradient step
2022-12-29 18:40:01.059 DEBUG: Loss 3: {'policy_loss': -0.0751706303423296, 'entropy_loss': -0.012430687202140689, 'vf_loss': 9.45484644313931e-05, 'total_loss': -0.0875067690800389, 'approx_kl': 0.03317169088404626, 'clip_fraction': 0.2877604216337204, 'grad_norm': 17.548015594482422}
2022-12-29 18:40:01.910 DEBUG: Taking gradient step
2022-12-29 18:40:01.920 DEBUG: Loss 4: {'policy_loss': 0.00043185751089451843, 'entropy_loss': -0.01233838452026248, 'vf_loss': 8.035661027992592e-05, 'total_loss': -0.011826170399088039, 'approx_kl': 0.0433882144279778, 'clip_fraction': 0.2981770858168602, 'grad_norm': 10.959075927734375}
2022-12-29 18:40:02.736 DEBUG: Taking gradient step
2022-12-29 18:40:02.745 DEBUG: Loss 5: {'policy_loss': -0.0416141687031621, 'entropy_loss': -0.012094598030671477, 'vf_loss': 7.851132178949793e-05, 'total_loss': -0.05363025541204408, 'approx_kl': 0.043350502382963896, 'clip_fraction': 0.25, 'grad_norm': 12.5009126663208}
2022-12-29 18:40:03.573 DEBUG: Taking gradient step
2022-12-29 18:40:03.582 DEBUG: Loss 6: {'policy_loss': -0.0320304888345685, 'entropy_loss': -0.011986776487901807, 'vf_loss': 7.078394962627016e-05, 'total_loss': -0.04394648137284404, 'approx_kl': 0.04074347298592329, 'clip_fraction': 0.2096354216337204, 'grad_norm': 8.645894050598145}
2022-12-29 18:40:04.407 DEBUG: Early stopping at step 7 for reaching max KL.
2022-12-29 18:40:04.408 INFO: Optimization: policy loss=-0.032, vf loss=0.000, entropy loss=-0.012, total loss=-0.044, num steps=7
2022-12-29 18:40:04.408 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:40:05.916 INFO: Evaluation rollout: return=0.780 (0.0), episode length=5.0
2022-12-29 18:40:05.917 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:40:05.919 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-17496.model
2022-12-29 18:40:05.922 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-19656.model
2022-12-29 18:40:05.951 INFO: Iteration: 91/137, steps: 19656
2022-12-29 18:41:04.065 INFO: Training rollout: return=0.683 (0.1), episode length=5.0
2022-12-29 18:41:04.067 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:41:04.069 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-19656_train.pkl
2022-12-29 18:41:04.935 DEBUG: Taking gradient step
2022-12-29 18:41:04.948 DEBUG: Loss 0: {'policy_loss': -0.03505099518653414, 'entropy_loss': -0.011770462384447455, 'vf_loss': 7.526347929527993e-05, 'total_loss': -0.046746194091686316, 'approx_kl': -1.4901161193847656e-08, 'clip_fraction': 0.0, 'grad_norm': 10.07470703125}
2022-12-29 18:41:05.845 DEBUG: Taking gradient step
2022-12-29 18:41:05.855 DEBUG: Loss 1: {'policy_loss': -0.01369013477766999, 'entropy_loss': -0.011446604505181313, 'vf_loss': 6.8537294427519e-05, 'total_loss': -0.02506820198842379, 'approx_kl': 0.005056275287643075, 'clip_fraction': 0.015625, 'grad_norm': 9.87170696258545}
2022-12-29 18:41:06.690 DEBUG: Taking gradient step
2022-12-29 18:41:06.700 DEBUG: Loss 2: {'policy_loss': -0.10090583850726002, 'entropy_loss': -0.011334068374708295, 'vf_loss': 6.942399082772822e-05, 'total_loss': -0.11217048289114057, 'approx_kl': 0.018683608854189515, 'clip_fraction': 0.07552083395421505, 'grad_norm': 7.8490495681762695}
2022-12-29 18:41:07.519 DEBUG: Taking gradient step
2022-12-29 18:41:07.530 DEBUG: Loss 3: {'policy_loss': -0.029717194928778966, 'entropy_loss': -0.011150921927765012, 'vf_loss': 5.741588360129384e-05, 'total_loss': -0.04081070097294268, 'approx_kl': 0.04039995651692152, 'clip_fraction': 0.1328125, 'grad_norm': 5.80546236038208}
2022-12-29 18:41:08.354 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 18:41:08.355 INFO: Optimization: policy loss=-0.030, vf loss=0.000, entropy loss=-0.011, total loss=-0.041, num steps=4
2022-12-29 18:41:08.355 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:41:09.807 INFO: Evaluation rollout: return=0.778 (0.0), episode length=5.0
2022-12-29 18:41:09.808 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:41:09.810 INFO: Iteration: 92/137, steps: 19872
2022-12-29 18:42:08.540 INFO: Training rollout: return=0.693 (0.1), episode length=5.0
2022-12-29 18:42:08.542 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:42:08.545 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-19872_train.pkl
2022-12-29 18:42:09.379 DEBUG: Taking gradient step
2022-12-29 18:42:09.388 DEBUG: Loss 0: {'policy_loss': 0.026901221014817882, 'entropy_loss': -0.010847960598766804, 'vf_loss': 6.222772140819654e-05, 'total_loss': 0.01611548813745927, 'approx_kl': -6.845220923423767e-08, 'clip_fraction': 0.0, 'grad_norm': 8.047285079956055}
2022-12-29 18:42:10.221 DEBUG: Taking gradient step
2022-12-29 18:42:10.231 DEBUG: Loss 1: {'policy_loss': 0.002975759726610382, 'entropy_loss': -0.011004033731296659, 'vf_loss': 5.845172114547208e-05, 'total_loss': -0.00796982228354081, 'approx_kl': 0.0018811915069818497, 'clip_fraction': 0.1510416679084301, 'grad_norm': 17.126346588134766}
2022-12-29 18:42:11.046 DEBUG: Taking gradient step
2022-12-29 18:42:11.057 DEBUG: Loss 2: {'policy_loss': 0.02476012248652546, 'entropy_loss': -0.010524372104555368, 'vf_loss': 5.3769172537481406e-05, 'total_loss': 0.014289519554507572, 'approx_kl': 0.02160563739016652, 'clip_fraction': 0.2526041716337204, 'grad_norm': 24.0773868560791}
2022-12-29 18:42:11.861 DEBUG: Taking gradient step
2022-12-29 18:42:11.870 DEBUG: Loss 3: {'policy_loss': -0.019406795256039076, 'entropy_loss': -0.010568771744146943, 'vf_loss': 5.936827341373804e-05, 'total_loss': -0.02991619872677228, 'approx_kl': 0.025010440731421113, 'clip_fraction': 0.2669270858168602, 'grad_norm': 27.9970703125}
2022-12-29 18:42:12.658 DEBUG: Taking gradient step
2022-12-29 18:42:12.667 DEBUG: Loss 4: {'policy_loss': 0.01618094032323963, 'entropy_loss': -0.010223472025245428, 'vf_loss': 4.498539279861123e-05, 'total_loss': 0.006002453690792813, 'approx_kl': 0.03329399740323424, 'clip_fraction': 0.2330729216337204, 'grad_norm': 11.972043991088867}
2022-12-29 18:42:13.447 DEBUG: Taking gradient step
2022-12-29 18:42:13.457 DEBUG: Loss 5: {'policy_loss': -0.003647615930071524, 'entropy_loss': -0.010077377781271935, 'vf_loss': 5.066393571605286e-05, 'total_loss': -0.013674329775627407, 'approx_kl': 0.03502630884759128, 'clip_fraction': 0.1822916679084301, 'grad_norm': 15.614006042480469}
2022-12-29 18:42:14.284 DEBUG: Taking gradient step
2022-12-29 18:42:14.294 DEBUG: Loss 6: {'policy_loss': 0.05201223667715633, 'entropy_loss': -0.009980735834687948, 'vf_loss': 3.945186455060625e-05, 'total_loss': 0.04207095270701898, 'approx_kl': 0.02879531914368272, 'clip_fraction': 0.16015625, 'grad_norm': 13.990275382995605}
2022-12-29 18:42:15.156 DEBUG: Taking gradient step
2022-12-29 18:42:15.170 DEBUG: Loss 7: {'policy_loss': -0.015047802141190877, 'entropy_loss': -0.010758022079244256, 'vf_loss': 3.9205233568084085e-05, 'total_loss': -0.02576661898686705, 'approx_kl': 0.04144823341630399, 'clip_fraction': 0.1783854179084301, 'grad_norm': 18.476978302001953}
2022-12-29 18:42:16.100 DEBUG: Taking gradient step
2022-12-29 18:42:16.114 DEBUG: Loss 8: {'policy_loss': 0.007823155677839968, 'entropy_loss': -0.0101207853294909, 'vf_loss': 3.674691499965846e-05, 'total_loss': -0.0022608827366512735, 'approx_kl': 0.04263452300801873, 'clip_fraction': 0.10286458395421505, 'grad_norm': 6.073756694793701}
2022-12-29 18:42:16.985 DEBUG: Early stopping at step 9 for reaching max KL.
2022-12-29 18:42:16.986 INFO: Optimization: policy loss=0.008, vf loss=0.000, entropy loss=-0.010, total loss=-0.002, num steps=9
2022-12-29 18:42:16.986 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:42:18.449 INFO: Evaluation rollout: return=0.779 (0.0), episode length=5.0
2022-12-29 18:42:18.451 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:42:18.454 INFO: Iteration: 93/137, steps: 20088
2022-12-29 18:43:17.639 INFO: Training rollout: return=0.674 (0.1), episode length=5.0
2022-12-29 18:43:17.642 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:43:17.645 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-20088_train.pkl
2022-12-29 18:43:18.568 DEBUG: Taking gradient step
2022-12-29 18:43:18.577 DEBUG: Loss 0: {'policy_loss': -0.05754379635318354, 'entropy_loss': -0.010803196812048554, 'vf_loss': 3.964685012341671e-05, 'total_loss': -0.06830734631510868, 'approx_kl': -1.2960905682035673e-08, 'clip_fraction': 0.0, 'grad_norm': 13.23652172088623}
2022-12-29 18:43:19.421 DEBUG: Taking gradient step
2022-12-29 18:43:19.435 DEBUG: Loss 1: {'policy_loss': 0.003050476120828415, 'entropy_loss': -0.01090598525479436, 'vf_loss': 4.044462566324193e-05, 'total_loss': -0.0078150645083027, 'approx_kl': 0.0012391242198646069, 'clip_fraction': 0.0234375, 'grad_norm': 12.486623764038086}
2022-12-29 18:43:20.376 DEBUG: Taking gradient step
2022-12-29 18:43:20.386 DEBUG: Loss 2: {'policy_loss': -0.023125582216233884, 'entropy_loss': -0.011046773754060268, 'vf_loss': 3.6359697546967354e-05, 'total_loss': -0.03413599627274719, 'approx_kl': 0.014854511246085167, 'clip_fraction': 0.1315104179084301, 'grad_norm': 9.950271606445312}
2022-12-29 18:43:21.205 DEBUG: Taking gradient step
2022-12-29 18:43:21.216 DEBUG: Loss 3: {'policy_loss': -0.004247986745674687, 'entropy_loss': -0.010747331194579601, 'vf_loss': 3.412946085814677e-05, 'total_loss': -0.014961188479396141, 'approx_kl': 0.020005654310807586, 'clip_fraction': 0.16927083395421505, 'grad_norm': 9.123461723327637}
2022-12-29 18:43:22.024 DEBUG: Taking gradient step
2022-12-29 18:43:22.034 DEBUG: Loss 4: {'policy_loss': 0.009479992286376394, 'entropy_loss': -0.010418630670756102, 'vf_loss': 3.6030987504922566e-05, 'total_loss': -0.0009026073968747862, 'approx_kl': 0.023933073971420527, 'clip_fraction': 0.26171875, 'grad_norm': 17.39202117919922}
2022-12-29 18:43:22.879 DEBUG: Taking gradient step
2022-12-29 18:43:22.890 DEBUG: Loss 5: {'policy_loss': -0.03970503913388036, 'entropy_loss': -0.010770673397928476, 'vf_loss': 3.973758933223836e-05, 'total_loss': -0.050435974942476605, 'approx_kl': 0.03754792036488652, 'clip_fraction': 0.2981770858168602, 'grad_norm': 25.80815315246582}
2022-12-29 18:43:23.741 DEBUG: Taking gradient step
2022-12-29 18:43:23.750 DEBUG: Loss 6: {'policy_loss': 0.003166562271894019, 'entropy_loss': -0.010637229075655341, 'vf_loss': 3.248346080562682e-05, 'total_loss': -0.007438183342955691, 'approx_kl': 0.026887810323387384, 'clip_fraction': 0.2252604179084301, 'grad_norm': 21.888078689575195}
2022-12-29 18:43:24.566 DEBUG: Early stopping at step 7 for reaching max KL.
2022-12-29 18:43:24.567 INFO: Optimization: policy loss=0.003, vf loss=0.000, entropy loss=-0.011, total loss=-0.007, num steps=7
2022-12-29 18:43:24.567 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:43:26.024 INFO: Evaluation rollout: return=0.775 (0.0), episode length=5.0
2022-12-29 18:43:26.025 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:43:26.029 INFO: Iteration: 94/137, steps: 20304
2022-12-29 18:44:24.740 INFO: Training rollout: return=0.698 (0.1), episode length=5.0
2022-12-29 18:44:24.741 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:44:24.744 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-20304_train.pkl
2022-12-29 18:44:25.576 DEBUG: Taking gradient step
2022-12-29 18:44:25.586 DEBUG: Loss 0: {'policy_loss': -0.028607995970899233, 'entropy_loss': -0.010377232916653156, 'vf_loss': 3.175652106864521e-05, 'total_loss': -0.03895347236648375, 'approx_kl': -9.809931356130619e-08, 'clip_fraction': 0.0, 'grad_norm': 5.074200630187988}
2022-12-29 18:44:26.401 DEBUG: Taking gradient step
2022-12-29 18:44:26.411 DEBUG: Loss 1: {'policy_loss': -0.03932779655746449, 'entropy_loss': -0.010620721150189638, 'vf_loss': 3.0973593249982854e-05, 'total_loss': -0.04991754411440414, 'approx_kl': 0.0010861027985811234, 'clip_fraction': 0.02473958395421505, 'grad_norm': 12.308205604553223}
2022-12-29 18:44:27.228 DEBUG: Taking gradient step
2022-12-29 18:44:27.237 DEBUG: Loss 2: {'policy_loss': -0.028900342014096376, 'entropy_loss': -0.01120302313938737, 'vf_loss': 3.0596327890311693e-05, 'total_loss': -0.040072768825593436, 'approx_kl': 0.006524211727082729, 'clip_fraction': 0.12369791697710752, 'grad_norm': 12.272298812866211}
2022-12-29 18:44:28.053 DEBUG: Taking gradient step
2022-12-29 18:44:28.062 DEBUG: Loss 3: {'policy_loss': 0.024034215842284032, 'entropy_loss': -0.01087540527805686, 'vf_loss': 2.7678310546476343e-05, 'total_loss': 0.013186488874773646, 'approx_kl': 0.012824940960854292, 'clip_fraction': 0.17578125, 'grad_norm': 20.71674919128418}
2022-12-29 18:44:28.878 DEBUG: Taking gradient step
2022-12-29 18:44:28.887 DEBUG: Loss 4: {'policy_loss': -0.009714400514453726, 'entropy_loss': -0.010724554071202874, 'vf_loss': 2.715922485406935e-05, 'total_loss': -0.02041179536080253, 'approx_kl': 0.002083490020595491, 'clip_fraction': 0.1171875, 'grad_norm': 10.580018043518066}
2022-12-29 18:44:29.659 DEBUG: Taking gradient step
2022-12-29 18:44:29.670 DEBUG: Loss 5: {'policy_loss': -0.07747218898984684, 'entropy_loss': -0.010218738578259945, 'vf_loss': 2.866854984647826e-05, 'total_loss': -0.0876622590182603, 'approx_kl': 0.009081657277420163, 'clip_fraction': 0.07161458395421505, 'grad_norm': 8.98302936553955}
2022-12-29 18:44:30.466 DEBUG: Taking gradient step
2022-12-29 18:44:30.476 DEBUG: Loss 6: {'policy_loss': 0.05883336877259271, 'entropy_loss': -0.010356288868933916, 'vf_loss': 2.5026942750494675e-05, 'total_loss': 0.04850210684640929, 'approx_kl': 0.014277278387453407, 'clip_fraction': 0.07421875, 'grad_norm': 5.986886501312256}
2022-12-29 18:44:31.251 DEBUG: Taking gradient step
2022-12-29 18:44:31.260 DEBUG: Loss 7: {'policy_loss': -0.015962052839343245, 'entropy_loss': -0.010281308088451624, 'vf_loss': 2.4360938571198236e-05, 'total_loss': -0.026218999989223675, 'approx_kl': 0.015761421993374825, 'clip_fraction': 0.078125, 'grad_norm': 10.786481857299805}
2022-12-29 18:44:32.029 DEBUG: Taking gradient step
2022-12-29 18:44:32.039 DEBUG: Loss 8: {'policy_loss': -0.01850321320746466, 'entropy_loss': -0.011102807708084583, 'vf_loss': 2.682238454655429e-05, 'total_loss': -0.029579198531002687, 'approx_kl': 0.027562034665606916, 'clip_fraction': 0.1315104179084301, 'grad_norm': 11.624505996704102}
2022-12-29 18:44:32.883 DEBUG: Taking gradient step
2022-12-29 18:44:32.894 DEBUG: Loss 9: {'policy_loss': 0.05112584173063503, 'entropy_loss': -0.010895165847614408, 'vf_loss': 2.4157096197654188e-05, 'total_loss': 0.04025483297921827, 'approx_kl': 0.024080999661237, 'clip_fraction': 0.19140625, 'grad_norm': 21.247886657714844}
2022-12-29 18:44:32.894 INFO: Optimization: policy loss=0.051, vf loss=0.000, entropy loss=-0.011, total loss=0.040, num steps=10
2022-12-29 18:44:32.895 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:44:34.353 INFO: Evaluation rollout: return=0.769 (0.0), episode length=5.0
2022-12-29 18:44:34.354 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:44:34.357 INFO: Iteration: 95/137, steps: 20520
2022-12-29 18:45:01.764 DEBUG: Atoms are too close
2022-12-29 18:45:32.323 INFO: Training rollout: return=0.137 (3.3), episode length=5.0
2022-12-29 18:45:32.325 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:45:32.327 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-20520_train.pkl
2022-12-29 18:45:33.190 DEBUG: Taking gradient step
2022-12-29 18:45:33.200 DEBUG: Loss 0: {'policy_loss': 0.029096656618275683, 'entropy_loss': -0.011126595316454768, 'vf_loss': 0.008879692327082203, 'total_loss': 0.02684975362890312, 'approx_kl': 4.109460860490799e-08, 'clip_fraction': 0.0, 'grad_norm': 15.204978942871094}
2022-12-29 18:45:34.033 DEBUG: Taking gradient step
2022-12-29 18:45:34.043 DEBUG: Loss 1: {'policy_loss': 0.06807522038821318, 'entropy_loss': -0.01091620558872819, 'vf_loss': 0.01171952046611611, 'total_loss': 0.06887853526560109, 'approx_kl': 0.00029957928927615285, 'clip_fraction': 0.0, 'grad_norm': 35.79624557495117}
2022-12-29 18:45:34.881 DEBUG: Taking gradient step
2022-12-29 18:45:34.892 DEBUG: Loss 2: {'policy_loss': 0.01602699793255585, 'entropy_loss': -0.011395955923944712, 'vf_loss': 0.009053980839559928, 'total_loss': 0.013685022848171066, 'approx_kl': 0.008568953024223447, 'clip_fraction': 0.109375, 'grad_norm': 17.29490089416504}
2022-12-29 18:45:35.743 DEBUG: Taking gradient step
2022-12-29 18:45:35.752 DEBUG: Loss 3: {'policy_loss': -0.037149683750600536, 'entropy_loss': -0.011186705436557531, 'vf_loss': 0.006361599070888992, 'total_loss': -0.041974790116269076, 'approx_kl': 0.013054357375949621, 'clip_fraction': 0.2513020858168602, 'grad_norm': 3.8628311157226562}
2022-12-29 18:45:36.592 DEBUG: Taking gradient step
2022-12-29 18:45:36.601 DEBUG: Loss 4: {'policy_loss': -0.03565658053447428, 'entropy_loss': -0.011299074161797762, 'vf_loss': 0.006356696879205449, 'total_loss': -0.04059895781706659, 'approx_kl': 0.02366337412968278, 'clip_fraction': 0.2877604216337204, 'grad_norm': 2.9095990657806396}
2022-12-29 18:45:37.522 DEBUG: Taking gradient step
2022-12-29 18:45:37.536 DEBUG: Loss 5: {'policy_loss': 0.008650555481752392, 'entropy_loss': -0.011472307844087481, 'vf_loss': 0.009015542121903884, 'total_loss': 0.006193789759568788, 'approx_kl': 0.025088540744036436, 'clip_fraction': 0.2565104179084301, 'grad_norm': 3.0439932346343994}
2022-12-29 18:45:38.454 DEBUG: Taking gradient step
2022-12-29 18:45:38.466 DEBUG: Loss 6: {'policy_loss': -0.037298445233663005, 'entropy_loss': -0.011311267735436559, 'vf_loss': 0.006338191231674797, 'total_loss': -0.042271521737424766, 'approx_kl': 0.014948360621929169, 'clip_fraction': 0.1705729179084301, 'grad_norm': 10.172761917114258}
2022-12-29 18:45:39.326 DEBUG: Taking gradient step
2022-12-29 18:45:39.340 DEBUG: Loss 7: {'policy_loss': -0.03937193763669103, 'entropy_loss': -0.011110965861007571, 'vf_loss': 0.006333943124098545, 'total_loss': -0.04414896037360006, 'approx_kl': 0.02385829179547727, 'clip_fraction': 0.16015625, 'grad_norm': 11.849053382873535}
2022-12-29 18:45:40.244 DEBUG: Taking gradient step
2022-12-29 18:45:40.255 DEBUG: Loss 8: {'policy_loss': 0.0368721582079346, 'entropy_loss': -0.011174640152603388, 'vf_loss': 0.011635702729598577, 'total_loss': 0.037333220784929795, 'approx_kl': 0.032616404350847006, 'clip_fraction': 0.2317708358168602, 'grad_norm': 1.1030237674713135}
2022-12-29 18:45:41.061 DEBUG: Taking gradient step
2022-12-29 18:45:41.071 DEBUG: Loss 9: {'policy_loss': -0.04031167872902789, 'entropy_loss': -0.01094764331355691, 'vf_loss': 0.006309739486793664, 'total_loss': -0.04494958255579114, 'approx_kl': 0.036629398353397846, 'clip_fraction': 0.2421875, 'grad_norm': 2.1171154975891113}
2022-12-29 18:45:41.071 INFO: Optimization: policy loss=-0.040, vf loss=0.006, entropy loss=-0.011, total loss=-0.045, num steps=10
2022-12-29 18:45:41.071 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:45:42.511 INFO: Evaluation rollout: return=0.772 (0.0), episode length=5.0
2022-12-29 18:45:42.512 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:45:42.515 INFO: Iteration: 96/137, steps: 20736
2022-12-29 18:46:41.489 INFO: Training rollout: return=0.681 (0.1), episode length=5.0
2022-12-29 18:46:41.490 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:46:41.493 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-20736_train.pkl
2022-12-29 18:46:42.336 DEBUG: Taking gradient step
2022-12-29 18:46:42.345 DEBUG: Loss 0: {'policy_loss': 0.03802256659341764, 'entropy_loss': -0.011517730541527271, 'vf_loss': 3.068967562739406e-05, 'total_loss': 0.026535525727517762, 'approx_kl': -1.0236787417738924e-07, 'clip_fraction': 0.0, 'grad_norm': 14.566414833068848}
2022-12-29 18:46:43.162 DEBUG: Taking gradient step
2022-12-29 18:46:43.172 DEBUG: Loss 1: {'policy_loss': 0.01751694532710639, 'entropy_loss': -0.011432085884734988, 'vf_loss': 3.962783607790527e-05, 'total_loss': 0.00612448727844931, 'approx_kl': 0.0028748544864356518, 'clip_fraction': 0.02864583395421505, 'grad_norm': 18.160526275634766}
2022-12-29 18:46:44.003 DEBUG: Taking gradient step
2022-12-29 18:46:44.014 DEBUG: Loss 2: {'policy_loss': 0.014184458635424071, 'entropy_loss': -0.01128183351829648, 'vf_loss': 4.9253640129601125e-05, 'total_loss': 0.002951878757257189, 'approx_kl': 0.018573153764009476, 'clip_fraction': 0.2421875, 'grad_norm': 9.696090698242188}
2022-12-29 18:46:44.849 DEBUG: Taking gradient step
2022-12-29 18:46:44.858 DEBUG: Loss 3: {'policy_loss': -0.03003847490106846, 'entropy_loss': -0.01098957471549511, 'vf_loss': 6.317481500880392e-05, 'total_loss': -0.040964874801554765, 'approx_kl': 0.04336208966560662, 'clip_fraction': 0.3645833358168602, 'grad_norm': 16.06560516357422}
2022-12-29 18:46:45.670 DEBUG: Taking gradient step
2022-12-29 18:46:45.680 DEBUG: Loss 4: {'policy_loss': 0.02207919392957826, 'entropy_loss': -0.010510444641113281, 'vf_loss': 6.751212721530936e-05, 'total_loss': 0.01163626141568029, 'approx_kl': 0.027445399668067694, 'clip_fraction': 0.3450520858168602, 'grad_norm': 17.681137084960938}
2022-12-29 18:46:46.575 DEBUG: Taking gradient step
2022-12-29 18:46:46.589 DEBUG: Loss 5: {'policy_loss': 0.005237469059840728, 'entropy_loss': -0.010743593331426382, 'vf_loss': 7.691203421145269e-05, 'total_loss': -0.005429212237374199, 'approx_kl': 0.041201282758265734, 'clip_fraction': 0.2890625, 'grad_norm': 9.885149002075195}
2022-12-29 18:46:47.493 DEBUG: Taking gradient step
2022-12-29 18:46:47.502 DEBUG: Loss 6: {'policy_loss': -0.025321317737409577, 'entropy_loss': -0.011229288531467319, 'vf_loss': 9.059667406138223e-05, 'total_loss': -0.03646000959481552, 'approx_kl': 0.02273868676275015, 'clip_fraction': 0.10677083395421505, 'grad_norm': 8.980554580688477}
2022-12-29 18:46:48.322 DEBUG: Taking gradient step
2022-12-29 18:46:48.331 DEBUG: Loss 7: {'policy_loss': -0.0013211263975893145, 'entropy_loss': -0.01106357597745955, 'vf_loss': 9.207218409195032e-05, 'total_loss': -0.01229263019095691, 'approx_kl': 0.018501883139833808, 'clip_fraction': 0.037760416977107525, 'grad_norm': 22.391094207763672}
2022-12-29 18:46:49.175 DEBUG: Taking gradient step
2022-12-29 18:46:49.185 DEBUG: Loss 8: {'policy_loss': -0.1122532465330707, 'entropy_loss': -0.010537631576880813, 'vf_loss': 0.00011024798014159408, 'total_loss': -0.12268063012980991, 'approx_kl': 0.006465020356699824, 'clip_fraction': 0.109375, 'grad_norm': 5.741891384124756}
2022-12-29 18:46:49.985 DEBUG: Taking gradient step
2022-12-29 18:46:49.995 DEBUG: Loss 9: {'policy_loss': 0.012930239135320203, 'entropy_loss': -0.010490999789908528, 'vf_loss': 9.828339486647956e-05, 'total_loss': 0.0025375227402781517, 'approx_kl': 0.01347670191898942, 'clip_fraction': 0.2200520858168602, 'grad_norm': 11.618660926818848}
2022-12-29 18:46:49.995 INFO: Optimization: policy loss=0.013, vf loss=0.000, entropy loss=-0.010, total loss=0.003, num steps=10
2022-12-29 18:46:49.996 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:46:51.488 INFO: Evaluation rollout: return=0.775 (0.0), episode length=5.0
2022-12-29 18:46:51.489 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:46:51.492 INFO: Iteration: 97/137, steps: 20952
2022-12-29 18:47:23.266 DEBUG: There is a single atom floating around
2022-12-29 18:47:49.974 INFO: Training rollout: return=0.131 (3.3), episode length=5.0
2022-12-29 18:47:49.976 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:47:49.978 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-20952_train.pkl
2022-12-29 18:47:50.845 DEBUG: Taking gradient step
2022-12-29 18:47:50.855 DEBUG: Loss 0: {'policy_loss': -0.024576705259949694, 'entropy_loss': -0.010471626184880733, 'vf_loss': 0.007695301138291127, 'total_loss': -0.027353030306539298, 'approx_kl': 3.050081431865692e-08, 'clip_fraction': 0.0, 'grad_norm': 8.773533821105957}
2022-12-29 18:47:51.777 DEBUG: Taking gradient step
2022-12-29 18:47:51.787 DEBUG: Loss 1: {'policy_loss': -0.025104277860054723, 'entropy_loss': -0.010060234693810344, 'vf_loss': 0.007686238458920682, 'total_loss': -0.027478274094944383, 'approx_kl': 0.01107989507727325, 'clip_fraction': 0.03645833395421505, 'grad_norm': 4.229460716247559}
2022-12-29 18:47:52.661 DEBUG: Taking gradient step
2022-12-29 18:47:52.671 DEBUG: Loss 2: {'policy_loss': -0.0319545206712388, 'entropy_loss': -0.009666984435170889, 'vf_loss': 0.0076875033870042395, 'total_loss': -0.03393400171940544, 'approx_kl': 0.016309477388858795, 'clip_fraction': 0.057291666977107525, 'grad_norm': 4.202970027923584}
2022-12-29 18:47:53.523 DEBUG: Taking gradient step
2022-12-29 18:47:53.533 DEBUG: Loss 3: {'policy_loss': -0.03956200156371775, 'entropy_loss': -0.009712761268019676, 'vf_loss': 0.007690010162794706, 'total_loss': -0.041584752668942716, 'approx_kl': 0.03090052865445614, 'clip_fraction': 0.1171875, 'grad_norm': 3.4209303855895996}
2022-12-29 18:47:54.387 DEBUG: Taking gradient step
2022-12-29 18:47:54.402 DEBUG: Loss 4: {'policy_loss': -0.037914161493990385, 'entropy_loss': -0.009427532204426825, 'vf_loss': 0.007678443903019544, 'total_loss': -0.03966324979539766, 'approx_kl': 0.03553287254180759, 'clip_fraction': 0.2200520858168602, 'grad_norm': 1.6369982957839966}
2022-12-29 18:47:55.283 DEBUG: Early stopping at step 5 for reaching max KL.
2022-12-29 18:47:55.283 INFO: Optimization: policy loss=-0.038, vf loss=0.008, entropy loss=-0.009, total loss=-0.040, num steps=5
2022-12-29 18:47:55.284 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:47:56.742 INFO: Evaluation rollout: return=0.774 (0.0), episode length=5.0
2022-12-29 18:47:56.743 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:47:56.745 INFO: Iteration: 98/137, steps: 21168
2022-12-29 18:48:28.218 DEBUG: There is a single atom floating around
2022-12-29 18:48:55.737 INFO: Training rollout: return=0.125 (3.3), episode length=5.0
2022-12-29 18:48:55.738 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:48:55.741 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-21168_train.pkl
2022-12-29 18:48:56.576 DEBUG: Taking gradient step
2022-12-29 18:48:56.586 DEBUG: Loss 0: {'policy_loss': -0.025884061820210875, 'entropy_loss': -0.009350118692964315, 'vf_loss': 0.007835119242581628, 'total_loss': -0.027399061270593564, 'approx_kl': -6.364037830053348e-08, 'clip_fraction': 0.0, 'grad_norm': 16.028526306152344}
2022-12-29 18:48:57.429 DEBUG: Taking gradient step
2022-12-29 18:48:57.438 DEBUG: Loss 1: {'policy_loss': -0.032891113901873825, 'entropy_loss': -0.009555685799568892, 'vf_loss': 0.00782809146952638, 'total_loss': -0.034618708231916345, 'approx_kl': 0.002166516613215208, 'clip_fraction': 0.02734375, 'grad_norm': 11.354331016540527}
2022-12-29 18:48:58.271 DEBUG: Taking gradient step
2022-12-29 18:48:58.280 DEBUG: Loss 2: {'policy_loss': -0.03205667813530739, 'entropy_loss': -0.009493439923971891, 'vf_loss': 0.007823140350692644, 'total_loss': -0.03372697770858663, 'approx_kl': 0.026771900127641857, 'clip_fraction': 0.2278645858168602, 'grad_norm': 9.37755012512207}
2022-12-29 18:48:59.101 DEBUG: Taking gradient step
2022-12-29 18:48:59.112 DEBUG: Loss 3: {'policy_loss': -0.035119430505763216, 'entropy_loss': -0.009189122938551009, 'vf_loss': 0.00782226257402364, 'total_loss': -0.03648629087029059, 'approx_kl': 0.04494575201533735, 'clip_fraction': 0.3776041716337204, 'grad_norm': 7.449401378631592}
2022-12-29 18:48:59.921 DEBUG: Taking gradient step
2022-12-29 18:48:59.932 DEBUG: Loss 4: {'policy_loss': -0.03798350062608156, 'entropy_loss': -0.009056469425559044, 'vf_loss': 0.007823157627213712, 'total_loss': -0.0392168124244269, 'approx_kl': 0.037835391238331795, 'clip_fraction': 0.4375, 'grad_norm': 3.9368488788604736}
2022-12-29 18:49:00.847 DEBUG: Early stopping at step 5 for reaching max KL.
2022-12-29 18:49:00.848 INFO: Optimization: policy loss=-0.038, vf loss=0.008, entropy loss=-0.009, total loss=-0.039, num steps=5
2022-12-29 18:49:00.848 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:49:02.305 INFO: Evaluation rollout: return=0.790 (0.0), episode length=5.0
2022-12-29 18:49:02.306 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:49:02.309 INFO: Iteration: 99/137, steps: 21384
2022-12-29 18:50:00.980 INFO: Training rollout: return=0.694 (0.1), episode length=5.0
2022-12-29 18:50:00.982 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:50:00.984 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-21384_train.pkl
2022-12-29 18:50:01.884 DEBUG: Taking gradient step
2022-12-29 18:50:01.894 DEBUG: Loss 0: {'policy_loss': -0.020613560454273386, 'entropy_loss': -0.009038251359015703, 'vf_loss': 0.00017186504676454717, 'total_loss': -0.029479946766524545, 'approx_kl': -3.255748559638505e-08, 'clip_fraction': 0.0, 'grad_norm': 14.2144193649292}
2022-12-29 18:50:02.714 DEBUG: Taking gradient step
2022-12-29 18:50:02.723 DEBUG: Loss 1: {'policy_loss': 0.009266959050707693, 'entropy_loss': -0.008894561557099223, 'vf_loss': 0.00018251776568782436, 'total_loss': 0.0005549152592962975, 'approx_kl': 0.0010247909231111407, 'clip_fraction': 0.00390625, 'grad_norm': 13.812382698059082}
2022-12-29 18:50:03.507 DEBUG: Taking gradient step
2022-12-29 18:50:03.517 DEBUG: Loss 2: {'policy_loss': 0.006410265670104068, 'entropy_loss': -0.008873005281202495, 'vf_loss': 0.00017929682713019725, 'total_loss': -0.0022834427839682307, 'approx_kl': -0.0017253964906558394, 'clip_fraction': 0.061197916977107525, 'grad_norm': 15.209090232849121}
2022-12-29 18:50:04.307 DEBUG: Taking gradient step
2022-12-29 18:50:04.316 DEBUG: Loss 3: {'policy_loss': 0.00961810788608303, 'entropy_loss': -0.009309120941907167, 'vf_loss': 0.00017894598553446975, 'total_loss': 0.0004879329297103347, 'approx_kl': -0.0066486376454122365, 'clip_fraction': 0.1510416679084301, 'grad_norm': 10.688490867614746}
2022-12-29 18:50:05.134 DEBUG: Taking gradient step
2022-12-29 18:50:05.144 DEBUG: Loss 4: {'policy_loss': -0.043709729002416084, 'entropy_loss': -0.009148930665105581, 'vf_loss': 0.000186420020955187, 'total_loss': -0.05267223964656648, 'approx_kl': -0.019880501087754965, 'clip_fraction': 0.2200520858168602, 'grad_norm': 19.134342193603516}
2022-12-29 18:50:05.944 DEBUG: Taking gradient step
2022-12-29 18:50:05.954 DEBUG: Loss 5: {'policy_loss': 0.018997323380308517, 'entropy_loss': -0.008936314843595028, 'vf_loss': 0.00017420057464888464, 'total_loss': 0.010235209111362373, 'approx_kl': -0.012507241684943438, 'clip_fraction': 0.1497395858168602, 'grad_norm': 10.576589584350586}
2022-12-29 18:50:06.763 DEBUG: Taking gradient step
2022-12-29 18:50:06.772 DEBUG: Loss 6: {'policy_loss': 0.06531889975374985, 'entropy_loss': -0.00889596645720303, 'vf_loss': 0.00016774552279858596, 'total_loss': 0.05659067881934541, 'approx_kl': 0.0009658938506618142, 'clip_fraction': 0.022135416977107525, 'grad_norm': 15.570714950561523}
2022-12-29 18:50:07.545 DEBUG: Taking gradient step
2022-12-29 18:50:07.556 DEBUG: Loss 7: {'policy_loss': -0.010805290023344214, 'entropy_loss': -0.008896575542166829, 'vf_loss': 0.0001722359583235508, 'total_loss': -0.019529629607187498, 'approx_kl': -0.007304564816877246, 'clip_fraction': 0.04427083395421505, 'grad_norm': 10.221179008483887}
2022-12-29 18:50:08.383 DEBUG: Taking gradient step
2022-12-29 18:50:08.393 DEBUG: Loss 8: {'policy_loss': -0.03658906525533326, 'entropy_loss': -0.008601806126534939, 'vf_loss': 0.00016756133857658587, 'total_loss': -0.04502331004329161, 'approx_kl': 0.012928309384733438, 'clip_fraction': 0.1953125, 'grad_norm': 20.295459747314453}
2022-12-29 18:50:09.156 DEBUG: Taking gradient step
2022-12-29 18:50:09.165 DEBUG: Loss 9: {'policy_loss': -0.045812541264031094, 'entropy_loss': -0.008666067151352763, 'vf_loss': 0.00017350521959213564, 'total_loss': -0.054305103195791724, 'approx_kl': -0.0020869558211416006, 'clip_fraction': 0.2356770858168602, 'grad_norm': 13.851654052734375}
2022-12-29 18:50:09.165 INFO: Optimization: policy loss=-0.046, vf loss=0.000, entropy loss=-0.009, total loss=-0.054, num steps=10
2022-12-29 18:50:09.166 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:50:10.631 INFO: Evaluation rollout: return=0.788 (0.0), episode length=5.0
2022-12-29 18:50:10.633 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:50:10.636 INFO: Iteration: 100/137, steps: 21600
2022-12-29 18:51:08.724 INFO: Training rollout: return=0.691 (0.1), episode length=5.0
2022-12-29 18:51:08.725 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:51:08.728 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-21600_train.pkl
2022-12-29 18:51:09.627 DEBUG: Taking gradient step
2022-12-29 18:51:09.637 DEBUG: Loss 0: {'policy_loss': -0.008274653206514252, 'entropy_loss': -0.008582978742197156, 'vf_loss': 0.00014095937448467045, 'total_loss': -0.016716672574226737, 'approx_kl': -8.451752364635468e-08, 'clip_fraction': 0.0, 'grad_norm': 7.214296340942383}
2022-12-29 18:51:10.467 DEBUG: Taking gradient step
2022-12-29 18:51:10.476 DEBUG: Loss 1: {'policy_loss': -0.030308257793113125, 'entropy_loss': -0.008457112358883023, 'vf_loss': 0.000134452618661712, 'total_loss': -0.03863091753333443, 'approx_kl': 0.0010159845696762204, 'clip_fraction': 0.01171875, 'grad_norm': 2.704476833343506}
2022-12-29 18:51:11.327 DEBUG: Taking gradient step
2022-12-29 18:51:11.337 DEBUG: Loss 2: {'policy_loss': 0.0015212454507033913, 'entropy_loss': -0.008431395515799522, 'vf_loss': 0.00011877247337631378, 'total_loss': -0.006791377591719817, 'approx_kl': 0.002851161450962536, 'clip_fraction': 0.061197916977107525, 'grad_norm': 5.555516242980957}
2022-12-29 18:51:12.143 DEBUG: Taking gradient step
2022-12-29 18:51:12.152 DEBUG: Loss 3: {'policy_loss': -0.042765928701933145, 'entropy_loss': -0.008326863870024681, 'vf_loss': 0.00011258613016636419, 'total_loss': -0.050980206441791465, 'approx_kl': -0.001380012952722609, 'clip_fraction': 0.07552083395421505, 'grad_norm': 3.2209603786468506}
2022-12-29 18:51:12.970 DEBUG: Taking gradient step
2022-12-29 18:51:12.981 DEBUG: Loss 4: {'policy_loss': -0.024086530876003823, 'entropy_loss': -0.00800278875976801, 'vf_loss': 9.895158779990106e-05, 'total_loss': -0.03199036804797192, 'approx_kl': 0.012828236096538603, 'clip_fraction': 0.09895833395421505, 'grad_norm': 4.680774211883545}
2022-12-29 18:51:13.876 DEBUG: Taking gradient step
2022-12-29 18:51:13.885 DEBUG: Loss 5: {'policy_loss': -0.008379549147781839, 'entropy_loss': -0.0077087185345590115, 'vf_loss': 8.92812425466703e-05, 'total_loss': -0.015998986439794173, 'approx_kl': 0.004848566808504984, 'clip_fraction': 0.12890625, 'grad_norm': 13.908434867858887}
2022-12-29 18:51:14.752 DEBUG: Taking gradient step
2022-12-29 18:51:14.764 DEBUG: Loss 6: {'policy_loss': -0.007434657848809103, 'entropy_loss': -0.0076673023868352175, 'vf_loss': 7.800603770974575e-05, 'total_loss': -0.015023954197934576, 'approx_kl': 0.012730212416499853, 'clip_fraction': 0.1184895858168602, 'grad_norm': 4.545096397399902}
2022-12-29 18:51:15.573 DEBUG: Taking gradient step
2022-12-29 18:51:15.583 DEBUG: Loss 7: {'policy_loss': 0.027560495911409258, 'entropy_loss': -0.007461322238668799, 'vf_loss': 6.734703204708557e-05, 'total_loss': 0.02016652070478754, 'approx_kl': 0.023399886675179005, 'clip_fraction': 0.07942708395421505, 'grad_norm': 9.126914978027344}
2022-12-29 18:51:16.397 DEBUG: Taking gradient step
2022-12-29 18:51:16.407 DEBUG: Loss 8: {'policy_loss': 0.05095114993548474, 'entropy_loss': -0.007145687006413937, 'vf_loss': 6.128799635760376e-05, 'total_loss': 0.043866750925428404, 'approx_kl': 0.015475135063752532, 'clip_fraction': 0.06510416697710752, 'grad_norm': 8.16997241973877}
2022-12-29 18:51:17.223 DEBUG: Taking gradient step
2022-12-29 18:51:17.233 DEBUG: Loss 9: {'policy_loss': -0.0004422319050225703, 'entropy_loss': -0.00725994526874274, 'vf_loss': 5.755225916319021e-05, 'total_loss': -0.00764462491460212, 'approx_kl': 0.020080404356122017, 'clip_fraction': 0.09244791697710752, 'grad_norm': 7.762028694152832}
2022-12-29 18:51:17.233 INFO: Optimization: policy loss=-0.000, vf loss=0.000, entropy loss=-0.007, total loss=-0.008, num steps=10
2022-12-29 18:51:17.234 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:51:18.677 INFO: Evaluation rollout: return=0.791 (0.0), episode length=5.0
2022-12-29 18:51:18.678 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:51:18.680 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-19656.model
2022-12-29 18:51:18.687 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-21816.model
2022-12-29 18:51:18.717 INFO: Iteration: 101/137, steps: 21816
2022-12-29 18:52:17.687 INFO: Training rollout: return=0.703 (0.1), episode length=5.0
2022-12-29 18:52:17.689 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:52:17.691 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-21816_train.pkl
2022-12-29 18:52:18.524 DEBUG: Taking gradient step
2022-12-29 18:52:18.534 DEBUG: Loss 0: {'policy_loss': 0.0043601081847013685, 'entropy_loss': -0.006976780015975237, 'vf_loss': 4.428448237634132e-05, 'total_loss': -0.0025723873488975235, 'approx_kl': -6.752088665962219e-09, 'clip_fraction': 0.0, 'grad_norm': 13.555455207824707}
2022-12-29 18:52:19.346 DEBUG: Taking gradient step
2022-12-29 18:52:19.356 DEBUG: Loss 1: {'policy_loss': -0.04278351522673319, 'entropy_loss': -0.006991536007262766, 'vf_loss': 4.154112439483672e-05, 'total_loss': -0.049733510109601115, 'approx_kl': 0.011132170679047704, 'clip_fraction': 0.049479166977107525, 'grad_norm': 10.182982444763184}
2022-12-29 18:52:20.136 DEBUG: Taking gradient step
2022-12-29 18:52:20.145 DEBUG: Loss 2: {'policy_loss': -0.025927007156729343, 'entropy_loss': -0.0066610819194465876, 'vf_loss': 3.493871047033339e-05, 'total_loss': -0.032553150365705594, 'approx_kl': 0.030761453555896878, 'clip_fraction': 0.1901041679084301, 'grad_norm': 14.820265769958496}
2022-12-29 18:52:20.981 DEBUG: Taking gradient step
2022-12-29 18:52:20.991 DEBUG: Loss 3: {'policy_loss': -0.05589016974125474, 'entropy_loss': -0.006788457743823528, 'vf_loss': 3.071636335677575e-05, 'total_loss': -0.06264791112172151, 'approx_kl': 0.021568250842392445, 'clip_fraction': 0.2239583358168602, 'grad_norm': 9.2327299118042}
2022-12-29 18:52:21.785 DEBUG: Taking gradient step
2022-12-29 18:52:21.794 DEBUG: Loss 4: {'policy_loss': -0.0523978664691644, 'entropy_loss': -0.006449047359637916, 'vf_loss': 2.810649767376886e-05, 'total_loss': -0.058818807331128556, 'approx_kl': 0.029016255401074886, 'clip_fraction': 0.1484375, 'grad_norm': 2.8458023071289062}
2022-12-29 18:52:22.578 DEBUG: Taking gradient step
2022-12-29 18:52:22.587 DEBUG: Loss 5: {'policy_loss': 0.0015322311387919972, 'entropy_loss': -0.006551090977154672, 'vf_loss': 2.4164465824375327e-05, 'total_loss': -0.004994695372538303, 'approx_kl': 0.03300699591636658, 'clip_fraction': 0.14453125, 'grad_norm': 8.832972526550293}
2022-12-29 18:52:23.365 DEBUG: Taking gradient step
2022-12-29 18:52:23.376 DEBUG: Loss 6: {'policy_loss': -0.02146032326194467, 'entropy_loss': -0.006206753430888057, 'vf_loss': 2.2995575021295288e-05, 'total_loss': -0.027644081117811435, 'approx_kl': 0.027771899476647377, 'clip_fraction': 0.10416666697710752, 'grad_norm': 17.573165893554688}
2022-12-29 18:52:24.161 DEBUG: Taking gradient step
2022-12-29 18:52:24.171 DEBUG: Loss 7: {'policy_loss': -0.04985786827911233, 'entropy_loss': -0.006300013628788292, 'vf_loss': 2.0517352244049477e-05, 'total_loss': -0.056137364555656585, 'approx_kl': 0.021356454235501587, 'clip_fraction': 0.09505208395421505, 'grad_norm': 11.1724214553833}
2022-12-29 18:52:24.966 DEBUG: Taking gradient step
2022-12-29 18:52:24.975 DEBUG: Loss 8: {'policy_loss': -0.02100833496084354, 'entropy_loss': -0.006040634121745825, 'vf_loss': 1.8573958811386257e-05, 'total_loss': -0.02703039512377798, 'approx_kl': 0.02969173510791734, 'clip_fraction': 0.171875, 'grad_norm': 6.921696186065674}
2022-12-29 18:52:25.800 DEBUG: Early stopping at step 9 for reaching max KL.
2022-12-29 18:52:25.800 INFO: Optimization: policy loss=-0.021, vf loss=0.000, entropy loss=-0.006, total loss=-0.027, num steps=9
2022-12-29 18:52:25.800 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:52:27.210 INFO: Evaluation rollout: return=0.790 (0.0), episode length=5.0
2022-12-29 18:52:27.211 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:52:27.213 INFO: Iteration: 102/137, steps: 22032
2022-12-29 18:53:26.110 INFO: Training rollout: return=0.719 (0.1), episode length=5.0
2022-12-29 18:53:26.112 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:53:26.115 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-22032_train.pkl
2022-12-29 18:53:26.952 DEBUG: Taking gradient step
2022-12-29 18:53:26.961 DEBUG: Loss 0: {'policy_loss': 0.021077691463899612, 'entropy_loss': -0.006088378489948809, 'vf_loss': 2.688409950832015e-05, 'total_loss': 0.015016197073459123, 'approx_kl': 5.74315599521924e-08, 'clip_fraction': 0.0, 'grad_norm': 8.052020072937012}
2022-12-29 18:53:27.788 DEBUG: Taking gradient step
2022-12-29 18:53:27.798 DEBUG: Loss 1: {'policy_loss': 0.012323764825715407, 'entropy_loss': -0.006228487589396536, 'vf_loss': 2.576922593875231e-05, 'total_loss': 0.006121046462257619, 'approx_kl': 0.004679619800299406, 'clip_fraction': 0.049479166977107525, 'grad_norm': 9.769434928894043}
2022-12-29 18:53:28.603 DEBUG: Taking gradient step
2022-12-29 18:53:28.613 DEBUG: Loss 2: {'policy_loss': 0.023230938018129123, 'entropy_loss': -0.006004688912071288, 'vf_loss': 2.4152106447075237e-05, 'total_loss': 0.017250401212504908, 'approx_kl': 0.04233611887320876, 'clip_fraction': 0.21484375, 'grad_norm': 3.6587226390838623}
2022-12-29 18:53:29.452 DEBUG: Taking gradient step
2022-12-29 18:53:29.464 DEBUG: Loss 3: {'policy_loss': -0.06424682104591029, 'entropy_loss': -0.00596465659327805, 'vf_loss': 2.834091181055016e-05, 'total_loss': -0.07018313672737779, 'approx_kl': 0.04390594083815813, 'clip_fraction': 0.31640625, 'grad_norm': 9.393647193908691}
2022-12-29 18:53:30.277 DEBUG: Taking gradient step
2022-12-29 18:53:30.286 DEBUG: Loss 4: {'policy_loss': -0.0043072803616323145, 'entropy_loss': -0.006078326725400984, 'vf_loss': 2.2432339419942274e-05, 'total_loss': -0.01036317474761335, 'approx_kl': 0.04116957914084196, 'clip_fraction': 0.3359375, 'grad_norm': 7.829350471496582}
2022-12-29 18:53:31.113 DEBUG: Taking gradient step
2022-12-29 18:53:31.122 DEBUG: Loss 5: {'policy_loss': -0.06042354889237488, 'entropy_loss': -0.006160337361507118, 'vf_loss': 2.431541058474959e-05, 'total_loss': -0.06655957084329725, 'approx_kl': 0.03366989269852638, 'clip_fraction': 0.28515625, 'grad_norm': 7.011741638183594}
2022-12-29 18:53:31.934 DEBUG: Taking gradient step
2022-12-29 18:53:31.944 DEBUG: Loss 6: {'policy_loss': 0.009719500003552005, 'entropy_loss': -0.0063561099814251065, 'vf_loss': 1.900101995253743e-05, 'total_loss': 0.0033823910420794356, 'approx_kl': 0.03708134172484279, 'clip_fraction': 0.13411458395421505, 'grad_norm': 4.5381269454956055}
2022-12-29 18:53:32.851 DEBUG: Taking gradient step
2022-12-29 18:53:32.866 DEBUG: Loss 7: {'policy_loss': 0.019115269632901513, 'entropy_loss': -0.006055944715626538, 'vf_loss': 1.87027685266768e-05, 'total_loss': 0.013078027685801664, 'approx_kl': 0.024087832076475024, 'clip_fraction': 0.045572916977107525, 'grad_norm': 11.513748168945312}
2022-12-29 18:53:33.715 DEBUG: Taking gradient step
2022-12-29 18:53:33.730 DEBUG: Loss 8: {'policy_loss': 0.032347529771248905, 'entropy_loss': -0.006426464882679284, 'vf_loss': 1.8031109480082874e-05, 'total_loss': 0.0259390959980497, 'approx_kl': 0.02134296647273004, 'clip_fraction': 0.045572916977107525, 'grad_norm': 12.648887634277344}
2022-12-29 18:53:34.539 DEBUG: Taking gradient step
2022-12-29 18:53:34.548 DEBUG: Loss 9: {'policy_loss': 0.006950483457008112, 'entropy_loss': -0.0067989586386829615, 'vf_loss': 1.695624362054271e-05, 'total_loss': 0.00016848106194569668, 'approx_kl': 0.020899886032566428, 'clip_fraction': 0.1666666716337204, 'grad_norm': 11.792637825012207}
2022-12-29 18:53:34.549 INFO: Optimization: policy loss=0.007, vf loss=0.000, entropy loss=-0.007, total loss=0.000, num steps=10
2022-12-29 18:53:34.549 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:53:35.982 INFO: Evaluation rollout: return=0.785 (0.0), episode length=5.0
2022-12-29 18:53:35.983 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:53:35.986 INFO: Iteration: 103/137, steps: 22248
2022-12-29 18:55:21.584 INFO: {
    "bag_scale": 5,
    "beta": "-10",
    "canvas_size": 5,
    "clip_ratio": 0.2,
    "cutoff": 4.0,
    "data_dir": "runs/CH4/data",
    "device": "cuda",
    "discount": 1.0,
    "entropy_coef": 0.06,
    "eval_formulas": "CH4",
    "eval_freq": 1,
    "formulas": "CH4",
    "gradient_clip": 0.5,
    "keep_models": false,
    "lam": 1.0,
    "learning_rate": 0.0005,
    "load_latest": false,
    "load_model": null,
    "log_dir": "runs/CH4/logs",
    "log_level": "DEBUG",
    "max_mean_distance": 1.8,
    "max_num_steps": 30000,
    "max_num_train_iters": 10,
    "max_solo_distance": 2.0,
    "maxl": 4,
    "min_atomic_distance": 0.6,
    "min_mean_distance": 0.8,
    "min_reward": -20.0,
    "mini_batch_size": 64,
    "model": "schnet_edge",
    "model_dir": "runs/CH4/models",
    "name": "CH4",
    "network_width": 128,
    "num_cg_levels": 3,
    "num_channels_hidden": 10,
    "num_channels_per_element": 4,
    "num_envs": 12,
    "num_eval_episodes": null,
    "num_gaussians": 3,
    "num_interactions": 3,
    "num_steps_per_iter": 216,
    "optimizer": "adam",
    "results_dir": "runs/CH4/results",
    "save_freq": 10,
    "save_rollouts": "train",
    "seed": 1,
    "symbols": "X,H,C",
    "target_kl": 0.03,
    "update_edges": true,
    "vf_coef": 0.001
}
2022-12-29 18:55:21.628 INFO: CUDA Device: 0
2022-12-29 18:55:21.629 INFO: Training bags: ['CH4']
2022-12-29 18:55:21.629 INFO: Evaluation bags: ['CH4']
2022-12-29 18:55:23.570 INFO: Number of parameters: 299732
2022-12-29 18:55:23.582 INFO: Starting PPO
2022-12-29 18:55:23.582 INFO: Iteration: 0/137, steps: 0
2022-12-29 18:55:26.723 DEBUG: There is a single atom floating around
2022-12-29 18:55:27.003 DEBUG: There is a single atom floating around
2022-12-29 18:55:27.576 DEBUG: There is a single atom floating around
2022-12-29 18:55:28.779 DEBUG: There is a single atom floating around
2022-12-29 18:55:30.726 DEBUG: Atoms are too close
2022-12-29 18:55:30.992 DEBUG: Atoms are too close
2022-12-29 18:55:35.110 DEBUG: Atoms are too close
2022-12-29 18:55:35.111 DEBUG: Atoms are too close
2022-12-29 18:55:35.112 DEBUG: Atoms are too close
2022-12-29 18:55:37.491 DEBUG: There is a single atom floating around
2022-12-29 18:55:40.733 DEBUG: There is a single atom floating around
2022-12-29 18:55:42.533 DEBUG: Atoms are too close
2022-12-29 18:55:42.593 DEBUG: Atoms are too close
2022-12-29 18:55:42.873 DEBUG: Atoms are too close
2022-12-29 18:55:44.529 DEBUG: Atoms are too close
2022-12-29 18:55:45.866 DEBUG: Atoms are too close
2022-12-29 18:55:46.157 DEBUG: Atoms are too close
2022-12-29 18:55:47.915 DEBUG: Atoms are too close
2022-12-29 18:55:47.916 DEBUG: Atoms are too close
2022-12-29 18:55:48.815 DEBUG: Atoms are too close
2022-12-29 18:55:49.756 DEBUG: Atoms are too close
2022-12-29 18:55:50.855 DEBUG: There is a single atom floating around
2022-12-29 18:55:51.602 DEBUG: Atoms are too close
2022-12-29 18:55:54.962 DEBUG: There is a single atom floating around
2022-12-29 18:55:54.963 DEBUG: There is a single atom floating around
2022-12-29 18:55:55.874 DEBUG: Atoms are too close
2022-12-29 18:55:56.345 DEBUG: Atoms are too close
2022-12-29 18:55:57.821 DEBUG: Atoms are too close
2022-12-29 18:55:59.851 DEBUG: There is a single atom floating around
2022-12-29 18:56:00.119 DEBUG: There is a single atom floating around
2022-12-29 18:56:01.731 DEBUG: Atoms are too close
2022-12-29 18:56:01.731 DEBUG: There is a single atom floating around
2022-12-29 18:56:03.744 DEBUG: Atoms are too close
2022-12-29 18:56:03.746 DEBUG: Atoms are too close
2022-12-29 18:56:04.057 DEBUG: There is a single atom floating around
2022-12-29 18:56:04.375 DEBUG: Atoms are too close
2022-12-29 18:56:04.375 DEBUG: There is a single atom floating around
2022-12-29 18:56:04.667 DEBUG: There is a single atom floating around
2022-12-29 18:56:06.918 DEBUG: Atoms are too close
2022-12-29 18:56:08.426 DEBUG: Atoms are too close
2022-12-29 18:56:08.773 INFO: Training rollout: return=-15.220 (8.4), episode length=3.8
2022-12-29 18:56:08.774 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:56:08.777 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-0_train.pkl
2022-12-29 18:56:09.593 DEBUG: Taking gradient step
2022-12-29 18:56:09.607 DEBUG: Loss 0: {'policy_loss': 0.06023447263519249, 'entropy_loss': -0.04163164272904396, 'vf_loss': 0.2744628968123957, 'total_loss': 0.2930657267185442, 'approx_kl': -8.719507604837418e-08, 'clip_fraction': 0.0, 'grad_norm': 18.759912490844727}
2022-12-29 18:56:10.370 DEBUG: Early stopping at step 1 for reaching max KL.
2022-12-29 18:56:10.370 INFO: Optimization: policy loss=0.060, vf loss=0.274, entropy loss=-0.042, total loss=0.293, num steps=1
2022-12-29 18:56:10.370 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:56:11.895 INFO: Evaluation rollout: return=0.355 (0.0), episode length=5.0
2022-12-29 18:56:11.896 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:56:11.899 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-216.model
2022-12-29 18:56:11.930 INFO: Iteration: 1/137, steps: 216
2022-12-29 18:56:14.757 DEBUG: There is a single atom floating around
2022-12-29 18:56:15.857 DEBUG: There is a single atom floating around
2022-12-29 18:56:15.858 DEBUG: There is a single atom floating around
2022-12-29 18:56:16.458 DEBUG: Atoms are too close
2022-12-29 18:56:20.881 DEBUG: Atoms are too close
2022-12-29 18:56:22.745 DEBUG: Atoms are too close
2022-12-29 18:56:23.325 DEBUG: Atoms are too close
2022-12-29 18:56:24.734 DEBUG: Atoms are too close
2022-12-29 18:56:24.735 DEBUG: Atoms are too close
2022-12-29 18:56:26.052 DEBUG: There is a single atom floating around
2022-12-29 18:56:28.273 DEBUG: There is a single atom floating around
2022-12-29 18:56:29.001 DEBUG: Atoms are too close
2022-12-29 18:56:29.002 DEBUG: Atoms are too close
2022-12-29 18:56:29.696 DEBUG: Atoms are too close
2022-12-29 18:56:30.598 DEBUG: Atoms are too close
2022-12-29 18:56:30.598 DEBUG: There is a single atom floating around
2022-12-29 18:56:32.830 DEBUG: There is a single atom floating around
2022-12-29 18:56:33.418 DEBUG: There is a single atom floating around
2022-12-29 18:56:34.333 DEBUG: There is a single atom floating around
2022-12-29 18:56:38.899 DEBUG: There is a single atom floating around
2022-12-29 18:56:40.115 DEBUG: There is a single atom floating around
2022-12-29 18:56:40.116 DEBUG: Atoms are too close
2022-12-29 18:56:40.413 DEBUG: Atoms are too close
2022-12-29 18:56:41.144 DEBUG: Atoms are too close
2022-12-29 18:56:41.429 DEBUG: There is a single atom floating around
2022-12-29 18:56:42.833 DEBUG: Atoms are too close
2022-12-29 18:56:43.124 DEBUG: There is a single atom floating around
2022-12-29 18:56:44.643 DEBUG: There is a single atom floating around
2022-12-29 18:56:44.644 DEBUG: Atoms are too close
2022-12-29 18:56:44.644 DEBUG: There is a single atom floating around
2022-12-29 18:56:44.840 DEBUG: Atoms are too close
2022-12-29 18:56:45.286 DEBUG: Atoms are too close
2022-12-29 18:56:45.916 DEBUG: Atoms are too close
2022-12-29 18:56:47.389 DEBUG: Atoms are too close
2022-12-29 18:56:49.175 DEBUG: There is a single atom floating around
2022-12-29 18:56:49.506 DEBUG: There is a single atom floating around
2022-12-29 18:56:50.295 DEBUG: Atoms are too close
2022-12-29 18:56:51.013 DEBUG: Atoms are too close
2022-12-29 18:56:51.295 DEBUG: Atoms are too close
2022-12-29 18:56:51.433 DEBUG: Atoms are too close
2022-12-29 18:56:55.362 DEBUG: There is a single atom floating around
2022-12-29 18:56:55.427 INFO: Training rollout: return=-15.598 (8.3), episode length=3.7
2022-12-29 18:56:55.428 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:56:55.431 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-216_train.pkl
2022-12-29 18:56:56.213 DEBUG: Taking gradient step
2022-12-29 18:56:56.222 DEBUG: Loss 0: {'policy_loss': 0.010271727111676947, 'entropy_loss': -0.043058762326836586, 'vf_loss': 0.2539387563106785, 'total_loss': 0.2211517210955189, 'approx_kl': -4.668254405260086e-08, 'clip_fraction': 0.0, 'grad_norm': 22.184846878051758}
2022-12-29 18:56:56.979 DEBUG: Early stopping at step 1 for reaching max KL.
2022-12-29 18:56:56.979 INFO: Optimization: policy loss=0.010, vf loss=0.254, entropy loss=-0.043, total loss=0.221, num steps=1
2022-12-29 18:56:56.980 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:56:58.419 INFO: Evaluation rollout: return=0.430 (0.0), episode length=5.0
2022-12-29 18:56:58.420 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:56:58.423 INFO: Iteration: 2/137, steps: 432
2022-12-29 18:57:00.280 DEBUG: There is a single atom floating around
2022-12-29 18:57:00.591 DEBUG: There is a single atom floating around
2022-12-29 18:57:00.848 DEBUG: There is a single atom floating around
2022-12-29 18:57:10.686 DEBUG: Atoms are too close
2022-12-29 18:57:11.466 DEBUG: Atoms are too close
2022-12-29 18:57:11.468 DEBUG: Atoms are too close
2022-12-29 18:57:12.097 DEBUG: Atoms are too close
2022-12-29 18:57:14.872 DEBUG: Atoms are too close
2022-12-29 18:57:15.158 DEBUG: Atoms are too close
2022-12-29 18:57:15.466 DEBUG: Atoms are too close
2022-12-29 18:57:16.922 DEBUG: There is a single atom floating around
2022-12-29 18:57:20.909 DEBUG: There is a single atom floating around
2022-12-29 18:57:21.193 DEBUG: There is a single atom floating around
2022-12-29 18:57:22.695 DEBUG: There is a single atom floating around
2022-12-29 18:57:23.366 DEBUG: Atoms are too close
2022-12-29 18:57:24.657 DEBUG: Atoms are too close
2022-12-29 18:57:24.658 DEBUG: Atoms are too close
2022-12-29 18:57:27.815 DEBUG: Atoms are too close
2022-12-29 18:57:27.816 DEBUG: There is a single atom floating around
2022-12-29 18:57:28.696 DEBUG: There is a single atom floating around
2022-12-29 18:57:34.434 DEBUG: Atoms are too close
2022-12-29 18:57:37.527 DEBUG: Atoms are too close
2022-12-29 18:57:37.651 DEBUG: Atoms are too close
2022-12-29 18:57:37.783 DEBUG: Atoms are too close
2022-12-29 18:57:41.226 DEBUG: There is a single atom floating around
2022-12-29 18:57:42.036 DEBUG: Atoms are too close
2022-12-29 18:57:42.037 DEBUG: There is a single atom floating around
2022-12-29 18:57:47.233 DEBUG: Atoms are too close
2022-12-29 18:57:47.300 INFO: Training rollout: return=-11.972 (9.8), episode length=4.1
2022-12-29 18:57:47.301 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:57:47.304 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-432_train.pkl
2022-12-29 18:57:48.092 DEBUG: Taking gradient step
2022-12-29 18:57:48.102 DEBUG: Loss 0: {'policy_loss': 0.03852570144165776, 'entropy_loss': -0.04543157294392586, 'vf_loss': 0.18617478979160362, 'total_loss': 0.17926891828933555, 'approx_kl': -2.87157799760962e-08, 'clip_fraction': 0.0, 'grad_norm': 22.324844360351562}
2022-12-29 18:57:48.893 DEBUG: Early stopping at step 1 for reaching max KL.
2022-12-29 18:57:48.893 INFO: Optimization: policy loss=0.039, vf loss=0.186, entropy loss=-0.045, total loss=0.179, num steps=1
2022-12-29 18:57:48.894 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:57:50.319 INFO: Evaluation rollout: return=0.482 (0.0), episode length=5.0
2022-12-29 18:57:50.320 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:57:50.324 INFO: Iteration: 3/137, steps: 648
2022-12-29 18:57:52.998 DEBUG: There is a single atom floating around
2022-12-29 18:57:53.946 DEBUG: There is a single atom floating around
2022-12-29 18:57:54.243 DEBUG: There is a single atom floating around
2022-12-29 18:57:54.244 DEBUG: There is a single atom floating around
2022-12-29 18:57:54.245 DEBUG: There is a single atom floating around
2022-12-29 18:57:54.245 DEBUG: There is a single atom floating around
2022-12-29 18:57:58.666 DEBUG: There is a single atom floating around
2022-12-29 18:57:58.667 DEBUG: Atoms are too close
2022-12-29 18:57:59.859 DEBUG: Atoms are too close
2022-12-29 18:58:00.206 DEBUG: Atoms are too close
2022-12-29 18:58:00.806 DEBUG: Atoms are too close
2022-12-29 18:58:04.729 DEBUG: Atoms are too close
2022-12-29 18:58:07.130 DEBUG: Atoms are too close
2022-12-29 18:58:07.131 DEBUG: Atoms are too close
2022-12-29 18:58:12.433 DEBUG: Atoms are too close
2022-12-29 18:58:12.434 DEBUG: Atoms are too close
2022-12-29 18:58:12.713 DEBUG: There is a single atom floating around
2022-12-29 18:58:13.400 DEBUG: Atoms are too close
2022-12-29 18:58:13.401 DEBUG: Atoms are too close
2022-12-29 18:58:13.988 DEBUG: Atoms are too close
2022-12-29 18:58:13.989 DEBUG: There is a single atom floating around
2022-12-29 18:58:17.792 DEBUG: Atoms are too close
2022-12-29 18:58:23.061 DEBUG: Atoms are too close
2022-12-29 18:58:26.555 DEBUG: There is a single atom floating around
2022-12-29 18:58:26.556 DEBUG: Atoms are too close
2022-12-29 18:58:32.776 DEBUG: Atoms are too close
2022-12-29 18:58:37.927 DEBUG: There is a single atom floating around
2022-12-29 18:58:39.192 INFO: Training rollout: return=-12.326 (9.7), episode length=4.3
2022-12-29 18:58:39.193 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:58:39.197 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-648_train.pkl
2022-12-29 18:58:40.016 DEBUG: Taking gradient step
2022-12-29 18:58:40.026 DEBUG: Loss 0: {'policy_loss': 0.034257492075758594, 'entropy_loss': -0.04321777541190386, 'vf_loss': 0.1874920537886336, 'total_loss': 0.17853177045248833, 'approx_kl': -9.86425829907489e-08, 'clip_fraction': 0.0, 'grad_norm': 15.457026481628418}
2022-12-29 18:58:40.814 DEBUG: Early stopping at step 1 for reaching max KL.
2022-12-29 18:58:40.814 INFO: Optimization: policy loss=0.034, vf loss=0.187, entropy loss=-0.043, total loss=0.179, num steps=1
2022-12-29 18:58:40.815 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:58:42.154 INFO: Evaluation rollout: return=0.537 (0.0), episode length=5.0
2022-12-29 18:58:42.155 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:58:42.157 INFO: Iteration: 4/137, steps: 864
2022-12-29 18:58:46.397 DEBUG: There is a single atom floating around
2022-12-29 18:58:46.399 DEBUG: There is a single atom floating around
2022-12-29 18:58:54.279 DEBUG: Atoms are too close
2022-12-29 18:58:54.280 DEBUG: There is a single atom floating around
2022-12-29 18:58:54.280 DEBUG: Atoms are too close
2022-12-29 18:59:08.195 DEBUG: There is a single atom floating around
2022-12-29 18:59:10.117 DEBUG: Atoms are too close
2022-12-29 18:59:11.516 DEBUG: There is a single atom floating around
2022-12-29 18:59:11.516 DEBUG: Atoms are too close
2022-12-29 18:59:13.127 DEBUG: There is a single atom floating around
2022-12-29 18:59:15.864 DEBUG: Atoms are too close
2022-12-29 18:59:23.781 DEBUG: Atoms are too close
2022-12-29 18:59:25.548 DEBUG: Atoms are too close
2022-12-29 18:59:29.861 DEBUG: There is a single atom floating around
2022-12-29 18:59:35.153 INFO: Training rollout: return=-6.641 (9.6), episode length=4.6
2022-12-29 18:59:35.155 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 18:59:35.157 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-864_train.pkl
2022-12-29 18:59:35.981 DEBUG: Taking gradient step
2022-12-29 18:59:35.990 DEBUG: Loss 0: {'policy_loss': -0.018704493499056934, 'entropy_loss': -0.04493771493434906, 'vf_loss': 0.09198936202369132, 'total_loss': 0.02834715359028533, 'approx_kl': 1.819959472015853e-08, 'clip_fraction': 0.0, 'grad_norm': 20.862207412719727}
2022-12-29 18:59:36.793 DEBUG: Early stopping at step 1 for reaching max KL.
2022-12-29 18:59:36.794 INFO: Optimization: policy loss=-0.019, vf loss=0.092, entropy loss=-0.045, total loss=0.028, num steps=1
2022-12-29 18:59:36.794 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 18:59:38.363 INFO: Evaluation rollout: return=0.586 (0.0), episode length=5.0
2022-12-29 18:59:38.364 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 18:59:38.366 INFO: Iteration: 5/137, steps: 1080
2022-12-29 18:59:40.976 DEBUG: There is a single atom floating around
2022-12-29 18:59:40.977 DEBUG: There is a single atom floating around
2022-12-29 18:59:53.392 DEBUG: Atoms are too close
2022-12-29 18:59:55.604 DEBUG: There is a single atom floating around
2022-12-29 18:59:56.775 DEBUG: There is a single atom floating around
2022-12-29 18:59:57.922 DEBUG: There is a single atom floating around
2022-12-29 19:00:04.426 DEBUG: There is a single atom floating around
2022-12-29 19:00:06.575 DEBUG: Atoms are too close
2022-12-29 19:00:11.415 DEBUG: Atoms are too close
2022-12-29 19:00:19.557 DEBUG: There is a single atom floating around
2022-12-29 19:00:20.318 DEBUG: Atoms are too close
2022-12-29 19:00:26.986 DEBUG: There is a single atom floating around
2022-12-29 19:00:32.078 INFO: Training rollout: return=-5.327 (9.2), episode length=4.5
2022-12-29 19:00:32.080 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:00:32.084 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-1080_train.pkl
2022-12-29 19:00:32.986 DEBUG: Taking gradient step
2022-12-29 19:00:32.995 DEBUG: Loss 0: {'policy_loss': 0.02766310246807629, 'entropy_loss': -0.045014350675046444, 'vf_loss': 0.0704052391449997, 'total_loss': 0.05305399093802954, 'approx_kl': -2.06831227700377e-08, 'clip_fraction': 0.0, 'grad_norm': 10.155452728271484}
2022-12-29 19:00:33.859 DEBUG: Early stopping at step 1 for reaching max KL.
2022-12-29 19:00:33.860 INFO: Optimization: policy loss=0.028, vf loss=0.070, entropy loss=-0.045, total loss=0.053, num steps=1
2022-12-29 19:00:33.860 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:00:35.019 DEBUG: Atoms are too close
2022-12-29 19:00:35.021 INFO: Evaluation rollout: return=-19.528 (0.0), episode length=5.0
2022-12-29 19:00:35.022 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:00:35.024 INFO: Iteration: 6/137, steps: 1296
2022-12-29 19:00:40.259 DEBUG: There is a single atom floating around
2022-12-29 19:00:47.714 DEBUG: Atoms are too close
2022-12-29 19:00:53.450 DEBUG: There is a single atom floating around
2022-12-29 19:01:04.724 DEBUG: There is a single atom floating around
2022-12-29 19:01:10.042 DEBUG: There is a single atom floating around
2022-12-29 19:01:11.159 DEBUG: There is a single atom floating around
2022-12-29 19:01:11.160 DEBUG: Atoms are too close
2022-12-29 19:01:18.239 DEBUG: Atoms are too close
2022-12-29 19:01:18.781 DEBUG: Atoms are too close
2022-12-29 19:01:19.675 DEBUG: Atoms are too close
2022-12-29 19:01:19.676 DEBUG: Atoms are too close
2022-12-29 19:01:23.293 DEBUG: There is a single atom floating around
2022-12-29 19:01:23.575 DEBUG: There is a single atom floating around
2022-12-29 19:01:28.406 INFO: Training rollout: return=-5.771 (9.4), episode length=4.6
2022-12-29 19:01:28.407 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:01:28.410 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-1296_train.pkl
2022-12-29 19:01:29.208 DEBUG: Taking gradient step
2022-12-29 19:01:29.217 DEBUG: Loss 0: {'policy_loss': -0.000502703667257984, 'entropy_loss': -0.04399942513555288, 'vf_loss': 0.07838990336523495, 'total_loss': 0.033887774562424096, 'approx_kl': -2.7745652886324024e-08, 'clip_fraction': 0.0, 'grad_norm': 18.914562225341797}
2022-12-29 19:01:30.022 DEBUG: Taking gradient step
2022-12-29 19:01:30.035 DEBUG: Loss 1: {'policy_loss': 0.044723614930990474, 'entropy_loss': -0.04253235924988985, 'vf_loss': 0.08065554323158214, 'total_loss': 0.08284679891268276, 'approx_kl': 0.043072737753391266, 'clip_fraction': 0.3932291716337204, 'grad_norm': 31.39032554626465}
2022-12-29 19:01:30.858 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:01:30.858 INFO: Optimization: policy loss=0.045, vf loss=0.081, entropy loss=-0.043, total loss=0.083, num steps=2
2022-12-29 19:01:30.859 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:01:32.367 INFO: Evaluation rollout: return=0.698 (0.0), episode length=5.0
2022-12-29 19:01:32.369 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:01:32.372 INFO: Iteration: 7/137, steps: 1512
2022-12-29 19:01:37.527 DEBUG: There is a single atom floating around
2022-12-29 19:01:38.439 DEBUG: There is a single atom floating around
2022-12-29 19:01:41.792 DEBUG: There is a single atom floating around
2022-12-29 19:01:41.793 DEBUG: There is a single atom floating around
2022-12-29 19:01:41.794 DEBUG: There is a single atom floating around
2022-12-29 19:01:41.794 DEBUG: There is a single atom floating around
2022-12-29 19:01:43.065 DEBUG: There is a single atom floating around
2022-12-29 19:01:43.066 DEBUG: There is a single atom floating around
2022-12-29 19:01:49.566 DEBUG: There is a single atom floating around
2022-12-29 19:01:51.887 DEBUG: There is a single atom floating around
2022-12-29 19:01:51.888 DEBUG: There is a single atom floating around
2022-12-29 19:01:55.402 DEBUG: There is a single atom floating around
2022-12-29 19:01:55.732 DEBUG: There is a single atom floating around
2022-12-29 19:02:02.622 DEBUG: There is a single atom floating around
2022-12-29 19:02:05.282 DEBUG: There is a single atom floating around
2022-12-29 19:02:06.304 DEBUG: Atoms are too close
2022-12-29 19:02:09.768 DEBUG: There is a single atom floating around
2022-12-29 19:02:13.500 DEBUG: There is a single atom floating around
2022-12-29 19:02:15.590 DEBUG: There is a single atom floating around
2022-12-29 19:02:18.968 DEBUG: There is a single atom floating around
2022-12-29 19:02:19.915 DEBUG: There is a single atom floating around
2022-12-29 19:02:23.058 INFO: Training rollout: return=-9.333 (10.1), episode length=4.5
2022-12-29 19:02:23.059 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:02:23.061 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-1512_train.pkl
2022-12-29 19:02:23.898 DEBUG: Taking gradient step
2022-12-29 19:02:23.907 DEBUG: Loss 0: {'policy_loss': 0.039582374990895786, 'entropy_loss': -0.04137637931853533, 'vf_loss': 0.14015146505155612, 'total_loss': 0.13835746072391658, 'approx_kl': 7.221630582421312e-08, 'clip_fraction': 0.0, 'grad_norm': 34.52960968017578}
2022-12-29 19:02:24.720 DEBUG: Taking gradient step
2022-12-29 19:02:24.730 DEBUG: Loss 1: {'policy_loss': 0.00409231341102986, 'entropy_loss': -0.03935808967798948, 'vf_loss': 0.13102719604390006, 'total_loss': 0.09576141977694044, 'approx_kl': 0.016490064561367035, 'clip_fraction': 0.28515625, 'grad_norm': 35.532230377197266}
2022-12-29 19:02:25.537 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:02:25.538 INFO: Optimization: policy loss=0.004, vf loss=0.131, entropy loss=-0.039, total loss=0.096, num steps=2
2022-12-29 19:02:25.538 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:02:26.898 INFO: Evaluation rollout: return=0.736 (0.0), episode length=5.0
2022-12-29 19:02:26.899 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:02:26.904 INFO: Iteration: 8/137, steps: 1728
2022-12-29 19:02:30.472 DEBUG: There is a single atom floating around
2022-12-29 19:02:34.680 DEBUG: There is a single atom floating around
2022-12-29 19:02:37.734 DEBUG: There is a single atom floating around
2022-12-29 19:02:39.223 DEBUG: There is a single atom floating around
2022-12-29 19:02:39.516 DEBUG: There is a single atom floating around
2022-12-29 19:02:40.932 DEBUG: Atoms are too close
2022-12-29 19:02:49.477 DEBUG: There is a single atom floating around
2022-12-29 19:02:52.467 DEBUG: Atoms are too close
2022-12-29 19:02:55.326 DEBUG: There is a single atom floating around
2022-12-29 19:02:59.142 DEBUG: There is a single atom floating around
2022-12-29 19:03:00.972 DEBUG: There is a single atom floating around
2022-12-29 19:03:00.973 DEBUG: There is a single atom floating around
2022-12-29 19:03:04.921 DEBUG: There is a single atom floating around
2022-12-29 19:03:07.253 DEBUG: Atoms are too close
2022-12-29 19:03:08.040 DEBUG: There is a single atom floating around
2022-12-29 19:03:11.099 DEBUG: There is a single atom floating around
2022-12-29 19:03:15.412 DEBUG: There is a single atom floating around
2022-12-29 19:03:16.790 DEBUG: There is a single atom floating around
2022-12-29 19:03:18.651 INFO: Training rollout: return=-8.323 (10.1), episode length=4.5
2022-12-29 19:03:18.652 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:03:18.655 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-1728_train.pkl
2022-12-29 19:03:19.476 DEBUG: Taking gradient step
2022-12-29 19:03:19.486 DEBUG: Loss 0: {'policy_loss': 0.05521540083085193, 'entropy_loss': -0.03757055103778839, 'vf_loss': 0.12221540034476727, 'total_loss': 0.1398602501378308, 'approx_kl': 3.519623259506943e-08, 'clip_fraction': 0.0, 'grad_norm': 28.128944396972656}
2022-12-29 19:03:20.257 DEBUG: Taking gradient step
2022-12-29 19:03:20.267 DEBUG: Loss 1: {'policy_loss': -0.08225677753003638, 'entropy_loss': -0.03741828817874193, 'vf_loss': 0.102905929782564, 'total_loss': -0.016769135926214304, 'approx_kl': 0.011753111146390438, 'clip_fraction': 0.2513020858168602, 'grad_norm': 17.482961654663086}
2022-12-29 19:03:21.072 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:03:21.073 INFO: Optimization: policy loss=-0.082, vf loss=0.103, entropy loss=-0.037, total loss=-0.017, num steps=2
2022-12-29 19:03:21.073 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:03:22.444 INFO: Evaluation rollout: return=0.755 (0.0), episode length=5.0
2022-12-29 19:03:22.445 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:03:22.448 INFO: Iteration: 9/137, steps: 1944
2022-12-29 19:03:27.085 DEBUG: There is a single atom floating around
2022-12-29 19:03:34.812 DEBUG: There is a single atom floating around
2022-12-29 19:03:39.970 DEBUG: There is a single atom floating around
2022-12-29 19:03:42.270 DEBUG: There is a single atom floating around
2022-12-29 19:03:42.272 DEBUG: Atoms are too close
2022-12-29 19:03:48.234 DEBUG: There is a single atom floating around
2022-12-29 19:03:54.958 DEBUG: There is a single atom floating around
2022-12-29 19:04:06.704 DEBUG: Atoms are too close
2022-12-29 19:04:06.994 DEBUG: Atoms are too close
2022-12-29 19:04:10.504 DEBUG: Atoms are too close
2022-12-29 19:04:13.207 DEBUG: Atoms are too close
2022-12-29 19:04:16.673 INFO: Training rollout: return=-5.018 (9.1), episode length=4.6
2022-12-29 19:04:16.674 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:04:16.677 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-1944_train.pkl
2022-12-29 19:04:17.506 DEBUG: Taking gradient step
2022-12-29 19:04:17.515 DEBUG: Loss 0: {'policy_loss': 0.007741324556910704, 'entropy_loss': -0.035876634530723095, 'vf_loss': 0.06745331316478526, 'total_loss': 0.03931800319097287, 'approx_kl': 1.9480165036611652e-08, 'clip_fraction': 0.0, 'grad_norm': 11.457162857055664}
2022-12-29 19:04:18.326 DEBUG: Taking gradient step
2022-12-29 19:04:18.335 DEBUG: Loss 1: {'policy_loss': 0.011063023259775671, 'entropy_loss': -0.035160863772034645, 'vf_loss': 0.06828374450260177, 'total_loss': 0.04418590399034281, 'approx_kl': 0.003771830757614225, 'clip_fraction': 0.1614583358168602, 'grad_norm': 17.253978729248047}
2022-12-29 19:04:19.164 DEBUG: Taking gradient step
2022-12-29 19:04:19.173 DEBUG: Loss 2: {'policy_loss': 0.022286480239431635, 'entropy_loss': -0.035111187025904655, 'vf_loss': 0.06846965830975574, 'total_loss': 0.055644951523282714, 'approx_kl': 0.02016633772291243, 'clip_fraction': 0.3059895858168602, 'grad_norm': 11.05088996887207}
2022-12-29 19:04:20.002 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 19:04:20.002 INFO: Optimization: policy loss=0.022, vf loss=0.068, entropy loss=-0.035, total loss=0.056, num steps=3
2022-12-29 19:04:20.003 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:04:21.450 INFO: Evaluation rollout: return=0.771 (0.0), episode length=5.0
2022-12-29 19:04:21.451 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:04:21.453 INFO: Iteration: 10/137, steps: 2160
2022-12-29 19:04:36.118 DEBUG: Atoms are too close
2022-12-29 19:04:45.511 DEBUG: There is a single atom floating around
2022-12-29 19:04:55.533 DEBUG: There is a single atom floating around
2022-12-29 19:04:56.387 DEBUG: There is a single atom floating around
2022-12-29 19:05:12.327 DEBUG: Atoms are too close
2022-12-29 19:05:16.067 DEBUG: There is a single atom floating around
2022-12-29 19:05:17.054 INFO: Training rollout: return=-2.471 (7.3), episode length=4.8
2022-12-29 19:05:17.055 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:05:17.057 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-2160_train.pkl
2022-12-29 19:05:17.923 DEBUG: Taking gradient step
2022-12-29 19:05:17.937 DEBUG: Loss 0: {'policy_loss': -0.001101547230922995, 'entropy_loss': -0.035106098279356956, 'vf_loss': 0.039491445551486495, 'total_loss': 0.0032838000412065364, 'approx_kl': 4.220055416226387e-08, 'clip_fraction': 0.0, 'grad_norm': 13.859790802001953}
2022-12-29 19:05:18.782 DEBUG: Taking gradient step
2022-12-29 19:05:18.792 DEBUG: Loss 1: {'policy_loss': -0.030533422929787248, 'entropy_loss': -0.03367975587025285, 'vf_loss': 0.03765947261619579, 'total_loss': -0.0265537061838443, 'approx_kl': -0.008603872964158654, 'clip_fraction': 0.046875, 'grad_norm': 8.671772003173828}
2022-12-29 19:05:19.638 DEBUG: Taking gradient step
2022-12-29 19:05:19.648 DEBUG: Loss 2: {'policy_loss': 0.05768356048868997, 'entropy_loss': -0.03292138781398535, 'vf_loss': 0.04379111060666992, 'total_loss': 0.06855328328137454, 'approx_kl': -0.006038609775714576, 'clip_fraction': 0.37890625, 'grad_norm': 27.79619026184082}
2022-12-29 19:05:20.505 DEBUG: Taking gradient step
2022-12-29 19:05:20.519 DEBUG: Loss 3: {'policy_loss': -0.005913260232348723, 'entropy_loss': -0.033321409951895475, 'vf_loss': 0.03986324220410099, 'total_loss': 0.000628572019856795, 'approx_kl': 0.004684142768383026, 'clip_fraction': 0.42578125, 'grad_norm': 11.72453784942627}
2022-12-29 19:05:21.343 DEBUG: Taking gradient step
2022-12-29 19:05:21.352 DEBUG: Loss 4: {'policy_loss': -0.02487653183787076, 'entropy_loss': -0.03390392381697893, 'vf_loss': 0.03926560636765371, 'total_loss': -0.01951484928719599, 'approx_kl': 0.0030054576927796006, 'clip_fraction': 0.37109375, 'grad_norm': 9.080916404724121}
2022-12-29 19:05:22.169 DEBUG: Taking gradient step
2022-12-29 19:05:22.178 DEBUG: Loss 5: {'policy_loss': -0.000508800774378388, 'entropy_loss': -0.03395131602883339, 'vf_loss': 0.038862595547708496, 'total_loss': 0.004402478744496714, 'approx_kl': -0.00019342917948961258, 'clip_fraction': 0.3828125, 'grad_norm': 12.23257064819336}
2022-12-29 19:05:22.976 DEBUG: Taking gradient step
2022-12-29 19:05:22.985 DEBUG: Loss 6: {'policy_loss': -0.0193931346971914, 'entropy_loss': -0.034215524792671204, 'vf_loss': 0.0388660577418178, 'total_loss': -0.01474260174804481, 'approx_kl': 0.00019153370521962643, 'clip_fraction': 0.2356770858168602, 'grad_norm': 11.178799629211426}
2022-12-29 19:05:23.792 DEBUG: Taking gradient step
2022-12-29 19:05:23.803 DEBUG: Loss 7: {'policy_loss': 0.015349245456961182, 'entropy_loss': -0.03513625031337142, 'vf_loss': 0.042390935675746685, 'total_loss': 0.02260393081933646, 'approx_kl': 0.004444235935807228, 'clip_fraction': 0.11067708395421505, 'grad_norm': 12.861478805541992}
2022-12-29 19:05:24.657 DEBUG: Taking gradient step
2022-12-29 19:05:24.666 DEBUG: Loss 8: {'policy_loss': -0.03669490897344708, 'entropy_loss': -0.03533979598432779, 'vf_loss': 0.037173218519961346, 'total_loss': -0.034861486437813526, 'approx_kl': -0.02015630155801773, 'clip_fraction': 0.2018229216337204, 'grad_norm': 12.132170677185059}
2022-12-29 19:05:25.489 DEBUG: Taking gradient step
2022-12-29 19:05:25.502 DEBUG: Loss 9: {'policy_loss': -0.019224065116797792, 'entropy_loss': -0.03578351158648729, 'vf_loss': 0.039114706324361995, 'total_loss': -0.01589287037892309, 'approx_kl': -0.006747859064489603, 'clip_fraction': 0.26171875, 'grad_norm': 2.473862886428833}
2022-12-29 19:05:25.503 INFO: Optimization: policy loss=-0.019, vf loss=0.039, entropy loss=-0.036, total loss=-0.016, num steps=10
2022-12-29 19:05:25.503 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:05:26.884 INFO: Evaluation rollout: return=0.778 (0.0), episode length=5.0
2022-12-29 19:05:26.885 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:05:26.888 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-216.model
2022-12-29 19:05:26.893 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-2376.model
2022-12-29 19:05:26.923 INFO: Iteration: 11/137, steps: 2376
2022-12-29 19:05:33.430 DEBUG: There is a single atom floating around
2022-12-29 19:05:52.373 DEBUG: There is a single atom floating around
2022-12-29 19:05:52.375 DEBUG: There is a single atom floating around
2022-12-29 19:05:55.806 DEBUG: There is a single atom floating around
2022-12-29 19:05:56.077 DEBUG: Atoms are too close
2022-12-29 19:06:02.407 DEBUG: There is a single atom floating around
2022-12-29 19:06:04.652 DEBUG: There is a single atom floating around
2022-12-29 19:06:06.977 DEBUG: Atoms are too close
2022-12-29 19:06:10.127 DEBUG: Atoms are too close
2022-12-29 19:06:22.202 INFO: Training rollout: return=-4.094 (8.6), episode length=4.7
2022-12-29 19:06:22.203 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:06:22.205 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-2376_train.pkl
2022-12-29 19:06:23.053 DEBUG: Taking gradient step
2022-12-29 19:06:23.065 DEBUG: Loss 0: {'policy_loss': -0.02319879772405526, 'entropy_loss': -0.03362243063747883, 'vf_loss': 0.053304889256041925, 'total_loss': -0.003516339105492164, 'approx_kl': -5.122274160385132e-09, 'clip_fraction': 0.0, 'grad_norm': 14.404052734375}
2022-12-29 19:06:23.910 DEBUG: Taking gradient step
2022-12-29 19:06:23.921 DEBUG: Loss 1: {'policy_loss': 0.004644332894456633, 'entropy_loss': -0.03162528807297349, 'vf_loss': 0.05526417374292296, 'total_loss': 0.0282832185644061, 'approx_kl': 0.007892191992141306, 'clip_fraction': 0.0, 'grad_norm': 19.72646141052246}
2022-12-29 19:06:24.729 DEBUG: Taking gradient step
2022-12-29 19:06:24.743 DEBUG: Loss 2: {'policy_loss': 0.008990729324984726, 'entropy_loss': -0.0322976247407496, 'vf_loss': 0.057686036582698594, 'total_loss': 0.03437914116693372, 'approx_kl': 0.024169337237253785, 'clip_fraction': 0.1171875, 'grad_norm': 13.09281063079834}
2022-12-29 19:06:25.656 DEBUG: Taking gradient step
2022-12-29 19:06:25.665 DEBUG: Loss 3: {'policy_loss': 0.013935172014952188, 'entropy_loss': -0.03107968857511878, 'vf_loss': 0.057354142558526736, 'total_loss': 0.040209625998360136, 'approx_kl': 0.02219796646386385, 'clip_fraction': 0.15234375, 'grad_norm': 8.722772598266602}
2022-12-29 19:06:26.490 DEBUG: Taking gradient step
2022-12-29 19:06:26.499 DEBUG: Loss 4: {'policy_loss': -0.014322072030337177, 'entropy_loss': -0.03172649908810854, 'vf_loss': 0.05287380161264908, 'total_loss': 0.006825230494203374, 'approx_kl': 0.011214626487344503, 'clip_fraction': 0.2994791716337204, 'grad_norm': 13.19157600402832}
2022-12-29 19:06:27.321 DEBUG: Taking gradient step
2022-12-29 19:06:27.331 DEBUG: Loss 5: {'policy_loss': 0.000979577114741179, 'entropy_loss': -0.031886445358395576, 'vf_loss': 0.05467972239677, 'total_loss': 0.023772854153115593, 'approx_kl': 0.0013338793069124222, 'clip_fraction': 0.3645833358168602, 'grad_norm': 18.217464447021484}
2022-12-29 19:06:28.137 DEBUG: Taking gradient step
2022-12-29 19:06:28.147 DEBUG: Loss 6: {'policy_loss': -0.02834472220149106, 'entropy_loss': -0.03229606477543712, 'vf_loss': 0.054634960765769, 'total_loss': -0.0060058262111591775, 'approx_kl': 0.026459602173417807, 'clip_fraction': 0.34765625, 'grad_norm': 14.3756685256958}
2022-12-29 19:06:28.957 DEBUG: Taking gradient step
2022-12-29 19:06:28.966 DEBUG: Loss 7: {'policy_loss': -0.04232327604672568, 'entropy_loss': -0.03324343916028738, 'vf_loss': 0.050206276089648956, 'total_loss': -0.025360439117364096, 'approx_kl': 0.017485752003267407, 'clip_fraction': 0.2708333358168602, 'grad_norm': 14.980485916137695}
2022-12-29 19:06:29.806 DEBUG: Taking gradient step
2022-12-29 19:06:29.815 DEBUG: Loss 8: {'policy_loss': 0.010643543277398908, 'entropy_loss': -0.03368910262361169, 'vf_loss': 0.05671566924632175, 'total_loss': 0.03367010990010898, 'approx_kl': 0.018632614985108376, 'clip_fraction': 0.2213541679084301, 'grad_norm': 7.449830532073975}
2022-12-29 19:06:30.612 DEBUG: Taking gradient step
2022-12-29 19:06:30.621 DEBUG: Loss 9: {'policy_loss': -0.01908123357449778, 'entropy_loss': -0.03359805466607213, 'vf_loss': 0.055144905329442925, 'total_loss': 0.002465617088873011, 'approx_kl': 0.02043055882677436, 'clip_fraction': 0.2005208358168602, 'grad_norm': 6.311373233795166}
2022-12-29 19:06:30.621 INFO: Optimization: policy loss=-0.019, vf loss=0.055, entropy loss=-0.034, total loss=0.002, num steps=10
2022-12-29 19:06:30.622 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:06:32.045 INFO: Evaluation rollout: return=0.779 (0.0), episode length=5.0
2022-12-29 19:06:32.046 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:06:32.048 INFO: Iteration: 12/137, steps: 2592
2022-12-29 19:06:37.915 DEBUG: There is a single atom floating around
2022-12-29 19:06:41.918 DEBUG: There is a single atom floating around
2022-12-29 19:07:01.030 DEBUG: Atoms are too close
2022-12-29 19:07:16.640 DEBUG: Atoms are too close
2022-12-29 19:07:29.425 INFO: Training rollout: return=-1.594 (6.3), episode length=4.9
2022-12-29 19:07:29.427 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:07:29.430 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-2592_train.pkl
2022-12-29 19:07:30.255 DEBUG: Taking gradient step
2022-12-29 19:07:30.264 DEBUG: Loss 0: {'policy_loss': -0.028510111280075315, 'entropy_loss': -0.03412479069083929, 'vf_loss': 0.025538073070881476, 'total_loss': -0.03709682890003313, 'approx_kl': -8.849504951058407e-08, 'clip_fraction': 0.0, 'grad_norm': 15.83389949798584}
2022-12-29 19:07:31.124 DEBUG: Taking gradient step
2022-12-29 19:07:31.138 DEBUG: Loss 1: {'policy_loss': 0.02055576789339899, 'entropy_loss': -0.03508732374757528, 'vf_loss': 0.031563150521182165, 'total_loss': 0.017031594667005878, 'approx_kl': 0.02459238888695836, 'clip_fraction': 0.16796875, 'grad_norm': 24.78772735595703}
2022-12-29 19:07:32.044 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:07:32.044 INFO: Optimization: policy loss=0.021, vf loss=0.032, entropy loss=-0.035, total loss=0.017, num steps=2
2022-12-29 19:07:32.045 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:07:33.567 INFO: Evaluation rollout: return=0.788 (0.0), episode length=5.0
2022-12-29 19:07:33.569 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:07:33.572 INFO: Iteration: 13/137, steps: 2808
2022-12-29 19:07:44.176 DEBUG: Atoms are too close
2022-12-29 19:07:48.847 DEBUG: Atoms are too close
2022-12-29 19:08:04.435 DEBUG: Atoms are too close
2022-12-29 19:08:30.779 INFO: Training rollout: return=-1.088 (5.6), episode length=5.0
2022-12-29 19:08:30.780 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:08:30.783 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-2808_train.pkl
2022-12-29 19:08:31.592 DEBUG: Taking gradient step
2022-12-29 19:08:31.602 DEBUG: Loss 0: {'policy_loss': 0.01079115714236683, 'entropy_loss': -0.03689495846629143, 'vf_loss': 0.026238044895437213, 'total_loss': 0.00013424357151261938, 'approx_kl': -6.798654794692993e-08, 'clip_fraction': 0.0, 'grad_norm': 20.95957374572754}
2022-12-29 19:08:32.429 DEBUG: Taking gradient step
2022-12-29 19:08:32.439 DEBUG: Loss 1: {'policy_loss': -0.022499877918617003, 'entropy_loss': -0.0370295736938715, 'vf_loss': 0.023729440370485536, 'total_loss': -0.035800011242002966, 'approx_kl': 0.011460553854703903, 'clip_fraction': 0.1393229179084301, 'grad_norm': 8.182390213012695}
2022-12-29 19:08:33.232 DEBUG: Taking gradient step
2022-12-29 19:08:33.241 DEBUG: Loss 2: {'policy_loss': 0.017905734068244493, 'entropy_loss': -0.03729819227010012, 'vf_loss': 0.02880923737858651, 'total_loss': 0.00941677917673088, 'approx_kl': 0.041287435218691826, 'clip_fraction': 0.2786458358168602, 'grad_norm': 14.253617286682129}
2022-12-29 19:08:34.067 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 19:08:34.067 INFO: Optimization: policy loss=0.018, vf loss=0.029, entropy loss=-0.037, total loss=0.009, num steps=3
2022-12-29 19:08:34.068 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:08:35.462 INFO: Evaluation rollout: return=0.794 (0.0), episode length=5.0
2022-12-29 19:08:35.463 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:08:35.466 INFO: Iteration: 14/137, steps: 3024
2022-12-29 19:08:51.794 DEBUG: There is a single atom floating around
2022-12-29 19:09:05.453 DEBUG: Atoms are too close
2022-12-29 19:09:05.746 DEBUG: Atoms are too close
2022-12-29 19:09:18.637 DEBUG: There is a single atom floating around
2022-12-29 19:09:19.300 DEBUG: Atoms are too close
2022-12-29 19:09:19.301 DEBUG: Atoms are too close
2022-12-29 19:09:21.527 DEBUG: Atoms are too close
2022-12-29 19:09:30.783 INFO: Training rollout: return=-3.373 (7.9), episode length=5.0
2022-12-29 19:09:30.784 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:09:30.787 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-3024_train.pkl
2022-12-29 19:09:31.631 DEBUG: Taking gradient step
2022-12-29 19:09:31.642 DEBUG: Loss 0: {'policy_loss': 0.013498325040876447, 'entropy_loss': -0.03720825258642435, 'vf_loss': 0.05430637313771526, 'total_loss': 0.030596445592167353, 'approx_kl': -3.573950380086899e-08, 'clip_fraction': 0.0, 'grad_norm': 10.379048347473145}
2022-12-29 19:09:32.461 DEBUG: Taking gradient step
2022-12-29 19:09:32.471 DEBUG: Loss 1: {'policy_loss': 0.0012368876359990484, 'entropy_loss': -0.04005257040262222, 'vf_loss': 0.05468637659959143, 'total_loss': 0.015870693832968245, 'approx_kl': -0.01594814728014171, 'clip_fraction': 0.1575520858168602, 'grad_norm': 20.808879852294922}
2022-12-29 19:09:33.286 DEBUG: Taking gradient step
2022-12-29 19:09:33.295 DEBUG: Loss 2: {'policy_loss': -0.032500617291638154, 'entropy_loss': -0.03895462304353714, 'vf_loss': 0.05049831119568545, 'total_loss': -0.02095692913948985, 'approx_kl': -0.008072584867477417, 'clip_fraction': 0.2421875, 'grad_norm': 15.29349136352539}
2022-12-29 19:09:34.096 DEBUG: Taking gradient step
2022-12-29 19:09:34.106 DEBUG: Loss 3: {'policy_loss': 0.013443299296063888, 'entropy_loss': -0.03950174432247877, 'vf_loss': 0.05716750349322274, 'total_loss': 0.031109058466807865, 'approx_kl': 0.017842159722931683, 'clip_fraction': 0.3138020858168602, 'grad_norm': 14.470575332641602}
2022-12-29 19:09:34.929 DEBUG: Taking gradient step
2022-12-29 19:09:34.938 DEBUG: Loss 4: {'policy_loss': 0.007916257570733304, 'entropy_loss': -0.040684000588953495, 'vf_loss': 0.054694739162586256, 'total_loss': 0.021926996144366065, 'approx_kl': 0.03573816642165184, 'clip_fraction': 0.3723958358168602, 'grad_norm': 14.491057395935059}
2022-12-29 19:09:35.781 DEBUG: Early stopping at step 5 for reaching max KL.
2022-12-29 19:09:35.781 INFO: Optimization: policy loss=0.008, vf loss=0.055, entropy loss=-0.041, total loss=0.022, num steps=5
2022-12-29 19:09:35.782 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:09:37.157 INFO: Evaluation rollout: return=0.792 (0.0), episode length=5.0
2022-12-29 19:09:37.158 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:09:37.160 INFO: Iteration: 15/137, steps: 3240
2022-12-29 19:10:03.830 DEBUG: There is a single atom floating around
2022-12-29 19:10:04.993 DEBUG: Atoms are too close
2022-12-29 19:10:07.142 DEBUG: Atoms are too close
2022-12-29 19:10:07.416 DEBUG: There is a single atom floating around
2022-12-29 19:10:14.899 DEBUG: There is a single atom floating around
2022-12-29 19:10:24.052 DEBUG: Atoms are too close
2022-12-29 19:10:24.053 DEBUG: Atoms are too close
2022-12-29 19:10:33.599 INFO: Training rollout: return=-3.277 (7.9), episode length=4.9
2022-12-29 19:10:33.601 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:10:33.603 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-3240_train.pkl
2022-12-29 19:10:34.428 DEBUG: Taking gradient step
2022-12-29 19:10:34.439 DEBUG: Loss 0: {'policy_loss': -0.023035483340338343, 'entropy_loss': -0.04153712745755911, 'vf_loss': 0.047731433420118013, 'total_loss': -0.01684117737777943, 'approx_kl': 2.716356917176199e-09, 'clip_fraction': 0.0, 'grad_norm': 21.1748104095459}
2022-12-29 19:10:35.254 DEBUG: Taking gradient step
2022-12-29 19:10:35.265 DEBUG: Loss 1: {'policy_loss': -0.03126036654649954, 'entropy_loss': -0.04217607155442238, 'vf_loss': 0.04743488406464496, 'total_loss': -0.026001554036276964, 'approx_kl': 0.02029189991299063, 'clip_fraction': 0.2044270858168602, 'grad_norm': 12.645035743713379}
2022-12-29 19:10:36.100 DEBUG: Taking gradient step
2022-12-29 19:10:36.114 DEBUG: Loss 2: {'policy_loss': 0.04421027322963525, 'entropy_loss': -0.040559484623372555, 'vf_loss': 0.055449386623819985, 'total_loss': 0.05910017523008268, 'approx_kl': 0.04261835105717182, 'clip_fraction': 0.2838541716337204, 'grad_norm': 17.541059494018555}
2022-12-29 19:10:36.987 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 19:10:36.987 INFO: Optimization: policy loss=0.044, vf loss=0.055, entropy loss=-0.041, total loss=0.059, num steps=3
2022-12-29 19:10:36.987 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:10:38.328 INFO: Evaluation rollout: return=0.793 (0.0), episode length=5.0
2022-12-29 19:10:38.329 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:10:38.331 INFO: Iteration: 16/137, steps: 3456
2022-12-29 19:10:42.700 DEBUG: There is a single atom floating around
2022-12-29 19:11:09.693 DEBUG: Atoms are too close
2022-12-29 19:11:23.124 DEBUG: Atoms are too close
2022-12-29 19:11:28.983 DEBUG: There is a single atom floating around
2022-12-29 19:11:30.746 DEBUG: Atoms are too close
2022-12-29 19:11:34.723 INFO: Training rollout: return=-2.158 (6.8), episode length=4.8
2022-12-29 19:11:34.724 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:11:34.726 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-3456_train.pkl
2022-12-29 19:11:35.538 DEBUG: Taking gradient step
2022-12-29 19:11:35.553 DEBUG: Loss 0: {'policy_loss': -0.04788387101086753, 'entropy_loss': -0.04167392663657665, 'vf_loss': 0.028775125840293556, 'total_loss': -0.060782671807150634, 'approx_kl': 6.58910721540451e-08, 'clip_fraction': 0.0, 'grad_norm': 12.957533836364746}
2022-12-29 19:11:36.454 DEBUG: Taking gradient step
2022-12-29 19:11:36.468 DEBUG: Loss 1: {'policy_loss': -0.0035364089217652696, 'entropy_loss': -0.03976815566420555, 'vf_loss': 0.033138838004694526, 'total_loss': -0.010165726581276293, 'approx_kl': -0.005177447805181146, 'clip_fraction': 0.04817708395421505, 'grad_norm': 11.643320083618164}
2022-12-29 19:11:37.357 DEBUG: Taking gradient step
2022-12-29 19:11:37.366 DEBUG: Loss 2: {'policy_loss': -0.04738340024767247, 'entropy_loss': -0.04225859045982361, 'vf_loss': 0.031065163756476114, 'total_loss': -0.05857682695101997, 'approx_kl': 0.0019026885274797678, 'clip_fraction': 0.2877604216337204, 'grad_norm': 6.192523002624512}
2022-12-29 19:11:38.193 DEBUG: Taking gradient step
2022-12-29 19:11:38.203 DEBUG: Loss 3: {'policy_loss': -0.06467155004245131, 'entropy_loss': -0.042233516462147236, 'vf_loss': 0.02896009604480957, 'total_loss': -0.07794497045978899, 'approx_kl': 0.039727017283439636, 'clip_fraction': 0.4375, 'grad_norm': 7.352450847625732}
2022-12-29 19:11:39.024 DEBUG: Taking gradient step
2022-12-29 19:11:39.033 DEBUG: Loss 4: {'policy_loss': -0.018184238144957492, 'entropy_loss': -0.042804659344255924, 'vf_loss': 0.03469529287912735, 'total_loss': -0.026293604610086067, 'approx_kl': 0.03499461989849806, 'clip_fraction': 0.4518229216337204, 'grad_norm': 6.008729457855225}
2022-12-29 19:11:39.867 DEBUG: Early stopping at step 5 for reaching max KL.
2022-12-29 19:11:39.868 INFO: Optimization: policy loss=-0.018, vf loss=0.035, entropy loss=-0.043, total loss=-0.026, num steps=5
2022-12-29 19:11:39.869 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:11:41.301 INFO: Evaluation rollout: return=0.795 (0.0), episode length=5.0
2022-12-29 19:11:41.302 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:11:41.304 INFO: Iteration: 17/137, steps: 3672
2022-12-29 19:11:53.114 DEBUG: Atoms are too close
2022-12-29 19:11:55.023 DEBUG: Atoms are too close
2022-12-29 19:12:27.036 DEBUG: Atoms are too close
2022-12-29 19:12:27.315 DEBUG: Atoms are too close
2022-12-29 19:12:38.111 INFO: Training rollout: return=-1.772 (6.3), episode length=5.0
2022-12-29 19:12:38.112 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:12:38.114 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-3672_train.pkl
2022-12-29 19:12:38.935 DEBUG: Taking gradient step
2022-12-29 19:12:38.946 DEBUG: Loss 0: {'policy_loss': 0.015585792396664879, 'entropy_loss': -0.04311554320156574, 'vf_loss': 0.03459583533290978, 'total_loss': 0.0070660845280089216, 'approx_kl': 8.226682446377254e-09, 'clip_fraction': 0.0, 'grad_norm': 17.681928634643555}
2022-12-29 19:12:39.794 DEBUG: Taking gradient step
2022-12-29 19:12:39.807 DEBUG: Loss 1: {'policy_loss': -0.024995321740162273, 'entropy_loss': -0.04137525334954262, 'vf_loss': 0.030112817541575656, 'total_loss': -0.03625775754812923, 'approx_kl': -0.0034409561048960313, 'clip_fraction': 0.045572916977107525, 'grad_norm': 12.932000160217285}
2022-12-29 19:12:40.691 DEBUG: Taking gradient step
2022-12-29 19:12:40.700 DEBUG: Loss 2: {'policy_loss': -0.04527331554452249, 'entropy_loss': -0.04280687402933836, 'vf_loss': 0.028052242841875623, 'total_loss': -0.060027946731985235, 'approx_kl': -0.0009322704281657934, 'clip_fraction': 0.2200520858168602, 'grad_norm': 11.707246780395508}
2022-12-29 19:12:41.506 DEBUG: Taking gradient step
2022-12-29 19:12:41.515 DEBUG: Loss 3: {'policy_loss': 0.017687704398499135, 'entropy_loss': -0.04210943914949894, 'vf_loss': 0.0341247749013238, 'total_loss': 0.009703040150323996, 'approx_kl': 0.01571446447633207, 'clip_fraction': 0.26171875, 'grad_norm': 7.770909786224365}
2022-12-29 19:12:42.372 DEBUG: Taking gradient step
2022-12-29 19:12:42.381 DEBUG: Loss 4: {'policy_loss': 0.002326427658234307, 'entropy_loss': -0.042044997215270996, 'vf_loss': 0.03217325252151437, 'total_loss': -0.007545317035522311, 'approx_kl': 0.00872090458869934, 'clip_fraction': 0.2838541716337204, 'grad_norm': 10.91086483001709}
2022-12-29 19:12:43.199 DEBUG: Taking gradient step
2022-12-29 19:12:43.210 DEBUG: Loss 5: {'policy_loss': -0.020269388499477584, 'entropy_loss': -0.04214220214635134, 'vf_loss': 0.032082301715494796, 'total_loss': -0.030329288930334118, 'approx_kl': 0.007827830413589254, 'clip_fraction': 0.24609375, 'grad_norm': 8.459012031555176}
2022-12-29 19:12:44.055 DEBUG: Taking gradient step
2022-12-29 19:12:44.064 DEBUG: Loss 6: {'policy_loss': -0.01809460170582732, 'entropy_loss': -0.04163329675793648, 'vf_loss': 0.03200173333806573, 'total_loss': -0.02772616512569806, 'approx_kl': 0.022968794219195843, 'clip_fraction': 0.2760416716337204, 'grad_norm': 12.393501281738281}
2022-12-29 19:12:44.895 DEBUG: Early stopping at step 7 for reaching max KL.
2022-12-29 19:12:44.895 INFO: Optimization: policy loss=-0.018, vf loss=0.032, entropy loss=-0.042, total loss=-0.028, num steps=7
2022-12-29 19:12:44.896 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:12:46.385 INFO: Evaluation rollout: return=0.789 (0.0), episode length=5.0
2022-12-29 19:12:46.386 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:12:46.388 INFO: Iteration: 18/137, steps: 3888
2022-12-29 19:12:58.194 DEBUG: There is a single atom floating around
2022-12-29 19:13:17.312 DEBUG: Atoms are too close
2022-12-29 19:13:31.836 DEBUG: Atoms are too close
2022-12-29 19:13:43.509 INFO: Training rollout: return=-1.173 (5.5), episode length=5.0
2022-12-29 19:13:43.510 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:13:43.512 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-3888_train.pkl
2022-12-29 19:13:44.369 DEBUG: Taking gradient step
2022-12-29 19:13:44.378 DEBUG: Loss 0: {'policy_loss': -0.039904900747041734, 'entropy_loss': -0.04103146493434906, 'vf_loss': 0.021059673722149952, 'total_loss': -0.059876691959240856, 'approx_kl': 1.8238400345538253e-08, 'clip_fraction': 0.0, 'grad_norm': 6.813007354736328}
2022-12-29 19:13:45.197 DEBUG: Taking gradient step
2022-12-29 19:13:45.207 DEBUG: Loss 1: {'policy_loss': -0.02133704679549531, 'entropy_loss': -0.03951217979192734, 'vf_loss': 0.02325585509677516, 'total_loss': -0.03759337149064749, 'approx_kl': 0.005655976594425738, 'clip_fraction': 0.05078125, 'grad_norm': 6.56299352645874}
2022-12-29 19:13:46.051 DEBUG: Taking gradient step
2022-12-29 19:13:46.060 DEBUG: Loss 2: {'policy_loss': -0.03263529438482951, 'entropy_loss': -0.040245383977890015, 'vf_loss': 0.02085105324543554, 'total_loss': -0.052029625117283984, 'approx_kl': 0.023873582016676664, 'clip_fraction': 0.2239583358168602, 'grad_norm': 6.674075603485107}
2022-12-29 19:13:46.917 DEBUG: Taking gradient step
2022-12-29 19:13:46.926 DEBUG: Loss 3: {'policy_loss': 0.025994119692018796, 'entropy_loss': -0.04093219991773367, 'vf_loss': 0.027129842566391126, 'total_loss': 0.012191762340676253, 'approx_kl': 0.03478769614594057, 'clip_fraction': 0.2838541716337204, 'grad_norm': 7.280904293060303}
2022-12-29 19:13:47.755 DEBUG: Taking gradient step
2022-12-29 19:13:47.764 DEBUG: Loss 4: {'policy_loss': -0.0007046746310887496, 'entropy_loss': -0.04132189508527517, 'vf_loss': 0.02519436317127139, 'total_loss': -0.01683220654509253, 'approx_kl': 0.024350028485059738, 'clip_fraction': 0.3372395858168602, 'grad_norm': 6.678921699523926}
2022-12-29 19:13:48.634 DEBUG: Taking gradient step
2022-12-29 19:13:48.649 DEBUG: Loss 5: {'policy_loss': 0.050411681043343474, 'entropy_loss': -0.04123818315565586, 'vf_loss': 0.0295203538062708, 'total_loss': 0.038693851693958406, 'approx_kl': 0.04048760607838631, 'clip_fraction': 0.33984375, 'grad_norm': 3.9234542846679688}
2022-12-29 19:13:49.473 DEBUG: Early stopping at step 6 for reaching max KL.
2022-12-29 19:13:49.473 INFO: Optimization: policy loss=0.050, vf loss=0.030, entropy loss=-0.041, total loss=0.039, num steps=6
2022-12-29 19:13:49.474 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:13:50.856 INFO: Evaluation rollout: return=0.774 (0.0), episode length=5.0
2022-12-29 19:13:50.858 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:13:50.860 INFO: Iteration: 19/137, steps: 4104
2022-12-29 19:14:36.380 DEBUG: There is a single atom floating around
2022-12-29 19:14:48.156 INFO: Training rollout: return=-0.057 (3.3), episode length=5.0
2022-12-29 19:14:48.157 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:14:48.159 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-4104_train.pkl
2022-12-29 19:14:48.969 DEBUG: Taking gradient step
2022-12-29 19:14:48.978 DEBUG: Loss 0: {'policy_loss': -0.03127636810312757, 'entropy_loss': -0.04164172802120447, 'vf_loss': 0.00929056415394788, 'total_loss': -0.06362753197038415, 'approx_kl': 1.1408701539039612e-08, 'clip_fraction': 0.0, 'grad_norm': 17.579345703125}
2022-12-29 19:14:49.815 DEBUG: Taking gradient step
2022-12-29 19:14:49.824 DEBUG: Loss 1: {'policy_loss': 0.025663396436329866, 'entropy_loss': -0.04190198238939047, 'vf_loss': 0.011231722077690532, 'total_loss': -0.005006863875370068, 'approx_kl': 0.02219482557848096, 'clip_fraction': 0.08333333395421505, 'grad_norm': 9.201665878295898}
2022-12-29 19:14:50.615 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:14:50.615 INFO: Optimization: policy loss=0.026, vf loss=0.011, entropy loss=-0.042, total loss=-0.005, num steps=2
2022-12-29 19:14:50.615 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:14:52.118 INFO: Evaluation rollout: return=0.765 (0.0), episode length=5.0
2022-12-29 19:14:52.119 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:14:52.121 INFO: Iteration: 20/137, steps: 4320
2022-12-29 19:15:06.355 DEBUG: Atoms are too close
2022-12-29 19:15:22.934 DEBUG: Atoms are too close
2022-12-29 19:15:22.935 DEBUG: Atoms are too close
2022-12-29 19:15:23.502 DEBUG: Atoms are too close
2022-12-29 19:15:36.941 DEBUG: Atoms are too close
2022-12-29 19:15:48.632 INFO: Training rollout: return=-2.318 (7.0), episode length=5.0
2022-12-29 19:15:48.633 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:15:48.636 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-4320_train.pkl
2022-12-29 19:15:49.456 DEBUG: Taking gradient step
2022-12-29 19:15:49.465 DEBUG: Loss 0: {'policy_loss': 0.04401630244540937, 'entropy_loss': -0.04234603885561228, 'vf_loss': 0.0465275041431477, 'total_loss': 0.04819776773294479, 'approx_kl': -2.1963690421955562e-08, 'clip_fraction': 0.0, 'grad_norm': 19.242475509643555}
2022-12-29 19:15:50.269 DEBUG: Taking gradient step
2022-12-29 19:15:50.278 DEBUG: Loss 1: {'policy_loss': 0.007502666778006191, 'entropy_loss': -0.04191169515252113, 'vf_loss': 0.04215788669527519, 'total_loss': 0.007748858320760244, 'approx_kl': 0.0028127216501161456, 'clip_fraction': 0.01171875, 'grad_norm': 21.59126091003418}
2022-12-29 19:15:51.095 DEBUG: Taking gradient step
2022-12-29 19:15:51.104 DEBUG: Loss 2: {'policy_loss': -0.02783140514957568, 'entropy_loss': -0.040770930238068104, 'vf_loss': 0.04012377096780903, 'total_loss': -0.028478564419834755, 'approx_kl': 0.023722688667476177, 'clip_fraction': 0.15234375, 'grad_norm': 13.923572540283203}
2022-12-29 19:15:51.896 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 19:15:51.896 INFO: Optimization: policy loss=-0.028, vf loss=0.040, entropy loss=-0.041, total loss=-0.028, num steps=3
2022-12-29 19:15:51.897 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:15:53.319 INFO: Evaluation rollout: return=0.763 (0.0), episode length=5.0
2022-12-29 19:15:53.321 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:15:53.324 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-2376.model
2022-12-29 19:15:53.326 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-4536.model
2022-12-29 19:15:53.362 INFO: Iteration: 21/137, steps: 4536
2022-12-29 19:16:05.210 DEBUG: Atoms are too close
2022-12-29 19:16:06.445 DEBUG: Atoms are too close
2022-12-29 19:16:06.758 DEBUG: Atoms are too close
2022-12-29 19:16:22.860 DEBUG: Atoms are too close
2022-12-29 19:16:50.221 INFO: Training rollout: return=-1.787 (6.3), episode length=5.0
2022-12-29 19:16:50.222 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:16:50.225 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-4536_train.pkl
2022-12-29 19:16:51.147 DEBUG: Taking gradient step
2022-12-29 19:16:51.156 DEBUG: Loss 0: {'policy_loss': 0.03738942380687141, 'entropy_loss': -0.04289403464645147, 'vf_loss': 0.03622264996364356, 'total_loss': 0.030718039124063498, 'approx_kl': -1.928613890811448e-08, 'clip_fraction': 0.0, 'grad_norm': 13.334301948547363}
2022-12-29 19:16:51.970 DEBUG: Taking gradient step
2022-12-29 19:16:51.981 DEBUG: Loss 1: {'policy_loss': -0.03787848061017332, 'entropy_loss': -0.04150106851011515, 'vf_loss': 0.029633591516499927, 'total_loss': -0.04974595760378854, 'approx_kl': 0.015362425707280636, 'clip_fraction': 0.16276041697710752, 'grad_norm': 3.3807477951049805}
2022-12-29 19:16:52.801 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:16:52.801 INFO: Optimization: policy loss=-0.038, vf loss=0.030, entropy loss=-0.042, total loss=-0.050, num steps=2
2022-12-29 19:16:52.802 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:16:54.225 INFO: Evaluation rollout: return=0.762 (0.0), episode length=5.0
2022-12-29 19:16:54.227 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:16:54.229 INFO: Iteration: 22/137, steps: 4752
2022-12-29 19:17:06.980 DEBUG: There is a single atom floating around
2022-12-29 19:17:08.760 DEBUG: There is a single atom floating around
2022-12-29 19:17:23.505 DEBUG: Atoms are too close
2022-12-29 19:17:24.407 DEBUG: Atoms are too close
2022-12-29 19:17:51.281 INFO: Training rollout: return=-1.737 (6.3), episode length=5.0
2022-12-29 19:17:51.282 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:17:51.285 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-4752_train.pkl
2022-12-29 19:17:52.115 DEBUG: Taking gradient step
2022-12-29 19:17:52.124 DEBUG: Loss 0: {'policy_loss': 0.01331590903192706, 'entropy_loss': -0.0420231819152832, 'vf_loss': 0.035505699311419295, 'total_loss': 0.006798426428063154, 'approx_kl': -1.0011717677116394e-08, 'clip_fraction': 0.0, 'grad_norm': 6.902570724487305}
2022-12-29 19:17:52.953 DEBUG: Taking gradient step
2022-12-29 19:17:52.967 DEBUG: Loss 1: {'policy_loss': 0.023512694788499613, 'entropy_loss': -0.04182951431721449, 'vf_loss': 0.03534752192534704, 'total_loss': 0.017030702396632177, 'approx_kl': 0.02831993345171213, 'clip_fraction': 0.20703125, 'grad_norm': 14.646493911743164}
2022-12-29 19:17:53.772 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:17:53.772 INFO: Optimization: policy loss=0.024, vf loss=0.035, entropy loss=-0.042, total loss=0.017, num steps=2
2022-12-29 19:17:53.772 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:17:55.233 INFO: Evaluation rollout: return=0.357 (0.0), episode length=5.0
2022-12-29 19:17:55.234 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:17:55.237 INFO: Iteration: 23/137, steps: 4968
2022-12-29 19:18:05.391 DEBUG: There is a single atom floating around
2022-12-29 19:18:07.943 DEBUG: There is a single atom floating around
2022-12-29 19:18:08.694 DEBUG: Atoms are too close
2022-12-29 19:18:17.481 DEBUG: There is a single atom floating around
2022-12-29 19:18:32.052 DEBUG: There is a single atom floating around
2022-12-29 19:18:32.926 DEBUG: There is a single atom floating around
2022-12-29 19:18:36.670 DEBUG: There is a single atom floating around
2022-12-29 19:18:36.671 DEBUG: There is a single atom floating around
2022-12-29 19:18:50.841 INFO: Training rollout: return=-3.793 (8.2), episode length=4.8
2022-12-29 19:18:50.843 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:18:50.846 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-4968_train.pkl
2022-12-29 19:18:51.707 DEBUG: Taking gradient step
2022-12-29 19:18:51.717 DEBUG: Loss 0: {'policy_loss': 0.04189394903326117, 'entropy_loss': -0.04296066705137491, 'vf_loss': 0.05931453239673187, 'total_loss': 0.05824781437861814, 'approx_kl': -1.1796752019677115e-08, 'clip_fraction': 0.0, 'grad_norm': 25.886138916015625}
2022-12-29 19:18:52.541 DEBUG: Taking gradient step
2022-12-29 19:18:52.552 DEBUG: Loss 1: {'policy_loss': 0.006544589037669473, 'entropy_loss': -0.040946257300674915, 'vf_loss': 0.05491943783292552, 'total_loss': 0.020517769569920075, 'approx_kl': 0.0041639441915322095, 'clip_fraction': 0.0, 'grad_norm': 17.008705139160156}
2022-12-29 19:18:53.401 DEBUG: Taking gradient step
2022-12-29 19:18:53.410 DEBUG: Loss 2: {'policy_loss': -0.03338681180580702, 'entropy_loss': -0.04102076683193445, 'vf_loss': 0.05056074982019191, 'total_loss': -0.023846828817549563, 'approx_kl': 0.023211893159896135, 'clip_fraction': 0.12890625, 'grad_norm': 9.488407135009766}
2022-12-29 19:18:54.252 DEBUG: Taking gradient step
2022-12-29 19:18:54.262 DEBUG: Loss 3: {'policy_loss': -0.01665662910405593, 'entropy_loss': -0.04207127820700407, 'vf_loss': 0.05238821383274244, 'total_loss': -0.006339693478317556, 'approx_kl': 0.030809184070676565, 'clip_fraction': 0.24609375, 'grad_norm': 13.337006568908691}
2022-12-29 19:18:55.046 DEBUG: Taking gradient step
2022-12-29 19:18:55.054 DEBUG: Loss 4: {'policy_loss': -0.009341792319773272, 'entropy_loss': -0.0417358772829175, 'vf_loss': 0.05469181375222965, 'total_loss': 0.0036141441495388775, 'approx_kl': 0.04139054426923394, 'clip_fraction': 0.296875, 'grad_norm': 11.964638710021973}
2022-12-29 19:18:55.896 DEBUG: Taking gradient step
2022-12-29 19:18:55.905 DEBUG: Loss 5: {'policy_loss': -0.014516575395165246, 'entropy_loss': -0.04173576273024082, 'vf_loss': 0.05440966688380176, 'total_loss': -0.0018426712416043135, 'approx_kl': 0.015475662890821695, 'clip_fraction': 0.2942708358168602, 'grad_norm': 10.981367111206055}
2022-12-29 19:18:56.703 DEBUG: Taking gradient step
2022-12-29 19:18:56.711 DEBUG: Loss 6: {'policy_loss': -0.01521693271290763, 'entropy_loss': -0.04135759361088276, 'vf_loss': 0.054086033469274535, 'total_loss': -0.0024884928545158523, 'approx_kl': 0.016807275358587503, 'clip_fraction': 0.3229166716337204, 'grad_norm': 10.873159408569336}
2022-12-29 19:18:57.545 DEBUG: Early stopping at step 7 for reaching max KL.
2022-12-29 19:18:57.545 INFO: Optimization: policy loss=-0.015, vf loss=0.054, entropy loss=-0.041, total loss=-0.002, num steps=7
2022-12-29 19:18:57.545 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:18:59.021 INFO: Evaluation rollout: return=0.350 (0.0), episode length=5.0
2022-12-29 19:18:59.022 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:18:59.024 INFO: Iteration: 24/137, steps: 5184
2022-12-29 19:19:06.956 DEBUG: There is a single atom floating around
2022-12-29 19:19:30.370 DEBUG: Atoms are too close
2022-12-29 19:19:44.511 DEBUG: Atoms are too close
2022-12-29 19:19:55.940 INFO: Training rollout: return=-1.212 (5.5), episode length=4.9
2022-12-29 19:19:55.942 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:19:55.944 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-5184_train.pkl
2022-12-29 19:19:56.771 DEBUG: Taking gradient step
2022-12-29 19:19:56.780 DEBUG: Loss 0: {'policy_loss': -0.012665818838954387, 'entropy_loss': -0.04209705535322428, 'vf_loss': 0.022402554294555004, 'total_loss': -0.032360319897623654, 'approx_kl': 1.5289213450842e-08, 'clip_fraction': 0.0, 'grad_norm': 19.257080078125}
2022-12-29 19:19:57.638 DEBUG: Taking gradient step
2022-12-29 19:19:57.647 DEBUG: Loss 1: {'policy_loss': -0.0051276997714096995, 'entropy_loss': -0.04228085000067949, 'vf_loss': 0.024549331537745273, 'total_loss': -0.022859218234343916, 'approx_kl': 0.02242344233673066, 'clip_fraction': 0.125, 'grad_norm': 5.312407970428467}
2022-12-29 19:19:58.466 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:19:58.467 INFO: Optimization: policy loss=-0.005, vf loss=0.025, entropy loss=-0.042, total loss=-0.023, num steps=2
2022-12-29 19:19:58.467 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:19:59.945 INFO: Evaluation rollout: return=0.418 (0.0), episode length=5.0
2022-12-29 19:19:59.946 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:19:59.948 INFO: Iteration: 25/137, steps: 5400
2022-12-29 19:20:06.231 DEBUG: Atoms are too close
2022-12-29 19:20:13.524 DEBUG: There is a single atom floating around
2022-12-29 19:20:13.526 DEBUG: Atoms are too close
2022-12-29 19:20:14.128 DEBUG: Atoms are too close
2022-12-29 19:20:21.216 DEBUG: Atoms are too close
2022-12-29 19:20:29.474 DEBUG: Atoms are too close
2022-12-29 19:20:43.960 DEBUG: There is a single atom floating around
2022-12-29 19:20:45.456 DEBUG: There is a single atom floating around
2022-12-29 19:20:45.457 DEBUG: There is a single atom floating around
2022-12-29 19:20:55.043 INFO: Training rollout: return=-4.452 (8.7), episode length=4.9
2022-12-29 19:20:55.044 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:20:55.047 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-5400_train.pkl
2022-12-29 19:20:55.869 DEBUG: Taking gradient step
2022-12-29 19:20:55.878 DEBUG: Loss 0: {'policy_loss': 0.01753144903314551, 'entropy_loss': -0.041019150987267494, 'vf_loss': 0.064303892208157, 'total_loss': 0.040816190254035024, 'approx_kl': -2.010104549476921e-08, 'clip_fraction': 0.0, 'grad_norm': 17.31308364868164}
2022-12-29 19:20:56.671 DEBUG: Taking gradient step
2022-12-29 19:20:56.680 DEBUG: Loss 1: {'policy_loss': 0.014144519159778001, 'entropy_loss': -0.04155381675809622, 'vf_loss': 0.06380673038027435, 'total_loss': 0.03639743278195613, 'approx_kl': 0.016549680847674608, 'clip_fraction': 0.1979166716337204, 'grad_norm': 14.40041732788086}
2022-12-29 19:20:57.490 DEBUG: Taking gradient step
2022-12-29 19:20:57.499 DEBUG: Loss 2: {'policy_loss': 0.036811500613916234, 'entropy_loss': -0.04202910978347063, 'vf_loss': 0.06595246627907753, 'total_loss': 0.060734857109523116, 'approx_kl': 0.024409878649748862, 'clip_fraction': 0.3671875, 'grad_norm': 12.06491756439209}
2022-12-29 19:20:58.302 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 19:20:58.302 INFO: Optimization: policy loss=0.037, vf loss=0.066, entropy loss=-0.042, total loss=0.061, num steps=3
2022-12-29 19:20:58.302 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:20:59.674 INFO: Evaluation rollout: return=0.531 (0.0), episode length=5.0
2022-12-29 19:20:59.676 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:20:59.678 INFO: Iteration: 26/137, steps: 5616
2022-12-29 19:21:10.722 DEBUG: Atoms are too close
2022-12-29 19:21:15.407 DEBUG: Atoms are too close
2022-12-29 19:21:29.336 DEBUG: There is a single atom floating around
2022-12-29 19:21:30.649 DEBUG: There is a single atom floating around
2022-12-29 19:21:56.108 INFO: Training rollout: return=-1.823 (6.3), episode length=5.0
2022-12-29 19:21:56.109 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:21:56.112 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-5616_train.pkl
2022-12-29 19:21:56.955 DEBUG: Taking gradient step
2022-12-29 19:21:56.970 DEBUG: Loss 0: {'policy_loss': -0.006924064803216171, 'entropy_loss': -0.04077812284231186, 'vf_loss': 0.031761749708641714, 'total_loss': -0.015940437936886313, 'approx_kl': 3.2208239186815035e-08, 'clip_fraction': 0.0, 'grad_norm': 22.739349365234375}
2022-12-29 19:21:57.835 DEBUG: Taking gradient step
2022-12-29 19:21:57.844 DEBUG: Loss 1: {'policy_loss': -0.0562799815779476, 'entropy_loss': -0.0423152782022953, 'vf_loss': 0.030062313565317, 'total_loss': -0.0685329462149259, 'approx_kl': 0.009928177110850811, 'clip_fraction': 0.15234375, 'grad_norm': 4.824628829956055}
2022-12-29 19:21:58.650 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:21:58.650 INFO: Optimization: policy loss=-0.056, vf loss=0.030, entropy loss=-0.042, total loss=-0.069, num steps=2
2022-12-29 19:21:58.651 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:22:00.063 INFO: Evaluation rollout: return=0.790 (0.0), episode length=5.0
2022-12-29 19:22:00.065 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:22:00.067 INFO: Iteration: 27/137, steps: 5832
2022-12-29 19:22:13.561 DEBUG: Atoms are too close
2022-12-29 19:22:28.541 DEBUG: Atoms are too close
2022-12-29 19:22:43.146 DEBUG: Atoms are too close
2022-12-29 19:22:45.616 DEBUG: Atoms are too close
2022-12-29 19:22:56.511 INFO: Training rollout: return=-1.778 (6.3), episode length=5.0
2022-12-29 19:22:56.512 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:22:56.515 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-5832_train.pkl
2022-12-29 19:22:57.386 DEBUG: Taking gradient step
2022-12-29 19:22:57.396 DEBUG: Loss 0: {'policy_loss': -0.0016688317066108683, 'entropy_loss': -0.042655834928154945, 'vf_loss': 0.03176028567004002, 'total_loss': -0.012564380964725789, 'approx_kl': -3.275150817216854e-08, 'clip_fraction': 0.0, 'grad_norm': 24.260757446289062}
2022-12-29 19:22:58.213 DEBUG: Taking gradient step
2022-12-29 19:22:58.224 DEBUG: Loss 1: {'policy_loss': 0.07411904429761554, 'entropy_loss': -0.04233144782483578, 'vf_loss': 0.03942234989211327, 'total_loss': 0.07120994636489304, 'approx_kl': 0.021981114987283945, 'clip_fraction': 0.13802083395421505, 'grad_norm': 33.82066345214844}
2022-12-29 19:22:59.019 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:22:59.019 INFO: Optimization: policy loss=0.074, vf loss=0.039, entropy loss=-0.042, total loss=0.071, num steps=2
2022-12-29 19:22:59.020 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:23:00.499 INFO: Evaluation rollout: return=0.788 (0.0), episode length=5.0
2022-12-29 19:23:00.500 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:23:00.503 INFO: Iteration: 28/137, steps: 6048
2022-12-29 19:23:14.332 DEBUG: Atoms are too close
2022-12-29 19:23:15.848 DEBUG: There is a single atom floating around
2022-12-29 19:23:45.481 DEBUG: Atoms are too close
2022-12-29 19:23:45.482 DEBUG: Atoms are too close
2022-12-29 19:23:46.323 DEBUG: There is a single atom floating around
2022-12-29 19:23:57.252 INFO: Training rollout: return=-2.291 (7.0), episode length=5.0
2022-12-29 19:23:57.253 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:23:57.256 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-6048_train.pkl
2022-12-29 19:23:58.095 DEBUG: Taking gradient step
2022-12-29 19:23:58.108 DEBUG: Loss 0: {'policy_loss': 0.04245735738389646, 'entropy_loss': -0.04189686197787523, 'vf_loss': 0.04476645595416753, 'total_loss': 0.045326951360188765, 'approx_kl': -2.9802322387695312e-08, 'clip_fraction': 0.0, 'grad_norm': 19.64496421813965}
2022-12-29 19:23:58.982 DEBUG: Taking gradient step
2022-12-29 19:23:58.991 DEBUG: Loss 1: {'policy_loss': 0.016188665836403316, 'entropy_loss': -0.04105139710009098, 'vf_loss': 0.04302733878925519, 'total_loss': 0.018164607525567526, 'approx_kl': 0.0018158240709453821, 'clip_fraction': 0.1354166679084301, 'grad_norm': 14.949170112609863}
2022-12-29 19:23:59.818 DEBUG: Taking gradient step
2022-12-29 19:23:59.827 DEBUG: Loss 2: {'policy_loss': 0.04716009915399478, 'entropy_loss': -0.040571512654423714, 'vf_loss': 0.04482276027992339, 'total_loss': 0.051411346779494466, 'approx_kl': 0.036664156476035714, 'clip_fraction': 0.3229166716337204, 'grad_norm': 12.7921724319458}
2022-12-29 19:24:00.721 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 19:24:00.722 INFO: Optimization: policy loss=0.047, vf loss=0.045, entropy loss=-0.041, total loss=0.051, num steps=3
2022-12-29 19:24:00.722 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:24:02.064 INFO: Evaluation rollout: return=0.771 (0.0), episode length=5.0
2022-12-29 19:24:02.065 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:24:02.067 INFO: Iteration: 29/137, steps: 6264
2022-12-29 19:24:13.087 DEBUG: Atoms are too close
2022-12-29 19:24:15.675 DEBUG: Atoms are too close
2022-12-29 19:24:16.657 DEBUG: There is a single atom floating around
2022-12-29 19:24:46.821 DEBUG: Atoms are too close
2022-12-29 19:24:58.334 INFO: Training rollout: return=-1.738 (6.3), episode length=5.0
2022-12-29 19:24:58.336 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:24:58.338 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-6264_train.pkl
2022-12-29 19:24:59.168 DEBUG: Taking gradient step
2022-12-29 19:24:59.178 DEBUG: Loss 0: {'policy_loss': -0.0071408690211996004, 'entropy_loss': -0.03952852450311184, 'vf_loss': 0.03250654834259757, 'total_loss': -0.014162845181713866, 'approx_kl': 1.0671404648832095e-08, 'clip_fraction': 0.0, 'grad_norm': 11.50867748260498}
2022-12-29 19:25:00.033 DEBUG: Taking gradient step
2022-12-29 19:25:00.042 DEBUG: Loss 1: {'policy_loss': 0.008210068254373863, 'entropy_loss': -0.039327437058091164, 'vf_loss': 0.03415880497717685, 'total_loss': 0.0030414361734595417, 'approx_kl': 0.011286405613645911, 'clip_fraction': 0.08463541697710752, 'grad_norm': 14.36595344543457}
2022-12-29 19:25:00.903 DEBUG: Taking gradient step
2022-12-29 19:25:00.920 DEBUG: Loss 2: {'policy_loss': 0.08216751639444703, 'entropy_loss': -0.0392574742436409, 'vf_loss': 0.03982662560907295, 'total_loss': 0.08273666775987909, 'approx_kl': 0.026525794062763453, 'clip_fraction': 0.2408854216337204, 'grad_norm': 10.181233406066895}
2022-12-29 19:25:01.731 DEBUG: Taking gradient step
2022-12-29 19:25:01.741 DEBUG: Loss 3: {'policy_loss': -0.032376356615948496, 'entropy_loss': -0.03795143496245146, 'vf_loss': 0.030561003930018496, 'total_loss': -0.03976678764838146, 'approx_kl': 0.025632464792579412, 'clip_fraction': 0.2252604179084301, 'grad_norm': 8.203988075256348}
2022-12-29 19:25:02.547 DEBUG: Taking gradient step
2022-12-29 19:25:02.556 DEBUG: Loss 4: {'policy_loss': -0.031394658941840584, 'entropy_loss': -0.03910743352025747, 'vf_loss': 0.030303832984070328, 'total_loss': -0.04019825947802773, 'approx_kl': 0.023962490202393383, 'clip_fraction': 0.1848958358168602, 'grad_norm': 9.595108985900879}
2022-12-29 19:25:03.373 DEBUG: Taking gradient step
2022-12-29 19:25:03.383 DEBUG: Loss 5: {'policy_loss': 0.00909370970553993, 'entropy_loss': -0.03829718753695488, 'vf_loss': 0.0343034876760646, 'total_loss': 0.005100009844649651, 'approx_kl': 0.022594320122152567, 'clip_fraction': 0.21875, 'grad_norm': 8.96091365814209}
2022-12-29 19:25:04.202 DEBUG: Taking gradient step
2022-12-29 19:25:04.211 DEBUG: Loss 6: {'policy_loss': -0.03491268905449872, 'entropy_loss': -0.03639172948896885, 'vf_loss': 0.030184085783836134, 'total_loss': -0.04112033275963143, 'approx_kl': 0.029129075352102518, 'clip_fraction': 0.2395833358168602, 'grad_norm': 4.948970794677734}
2022-12-29 19:25:05.027 DEBUG: Taking gradient step
2022-12-29 19:25:05.036 DEBUG: Loss 7: {'policy_loss': -0.025002335273625158, 'entropy_loss': -0.03749087639153004, 'vf_loss': 0.029945690133829514, 'total_loss': -0.03254752153132569, 'approx_kl': 0.015373661881312728, 'clip_fraction': 0.3046875, 'grad_norm': 14.550065994262695}
2022-12-29 19:25:05.855 DEBUG: Taking gradient step
2022-12-29 19:25:05.865 DEBUG: Loss 8: {'policy_loss': -0.04782907177581948, 'entropy_loss': -0.03750458639115095, 'vf_loss': 0.027813644569563305, 'total_loss': -0.05752001359740712, 'approx_kl': 0.014355565421283245, 'clip_fraction': 0.3385416716337204, 'grad_norm': 8.203116416931152}
2022-12-29 19:25:06.691 DEBUG: Taking gradient step
2022-12-29 19:25:06.702 DEBUG: Loss 9: {'policy_loss': -0.03093505129986503, 'entropy_loss': -0.03761342540383339, 'vf_loss': 0.029764427694636574, 'total_loss': -0.03878404900906184, 'approx_kl': 0.01286831102333963, 'clip_fraction': 0.2864583358168602, 'grad_norm': 5.88711404800415}
2022-12-29 19:25:06.702 INFO: Optimization: policy loss=-0.031, vf loss=0.030, entropy loss=-0.038, total loss=-0.039, num steps=10
2022-12-29 19:25:06.702 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:25:08.064 INFO: Evaluation rollout: return=0.768 (0.0), episode length=5.0
2022-12-29 19:25:08.065 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:25:08.068 INFO: Iteration: 30/137, steps: 6480
2022-12-29 19:25:14.708 DEBUG: Atoms are too close
2022-12-29 19:25:30.902 DEBUG: Atoms are too close
2022-12-29 19:25:36.689 DEBUG: Atoms are too close
2022-12-29 19:25:52.413 DEBUG: There is a single atom floating around
2022-12-29 19:26:01.753 DEBUG: Atoms are too close
2022-12-29 19:26:03.902 INFO: Training rollout: return=-2.127 (6.8), episode length=4.9
2022-12-29 19:26:03.904 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:26:03.906 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-6480_train.pkl
2022-12-29 19:26:04.734 DEBUG: Taking gradient step
2022-12-29 19:26:04.745 DEBUG: Loss 0: {'policy_loss': -0.05249969892150236, 'entropy_loss': -0.03699705749750137, 'vf_loss': 0.03047715835817514, 'total_loss': -0.05901959806082859, 'approx_kl': -5.97598663887311e-08, 'clip_fraction': 0.0, 'grad_norm': 15.768218040466309}
2022-12-29 19:26:05.618 DEBUG: Taking gradient step
2022-12-29 19:26:05.633 DEBUG: Loss 1: {'policy_loss': -0.014829927358552979, 'entropy_loss': -0.03805035538971424, 'vf_loss': 0.03443562318651535, 'total_loss': -0.018444659561751862, 'approx_kl': 0.005046673119068146, 'clip_fraction': 0.01953125, 'grad_norm': 4.860374927520752}
2022-12-29 19:26:06.472 DEBUG: Taking gradient step
2022-12-29 19:26:06.481 DEBUG: Loss 2: {'policy_loss': 0.0054757419051419745, 'entropy_loss': -0.03901079576462507, 'vf_loss': 0.03653413338993954, 'total_loss': 0.0029990795304564444, 'approx_kl': 0.011505784001201391, 'clip_fraction': 0.07552083395421505, 'grad_norm': 3.506558418273926}
2022-12-29 19:26:07.322 DEBUG: Taking gradient step
2022-12-29 19:26:07.331 DEBUG: Loss 3: {'policy_loss': 0.004952877896115732, 'entropy_loss': -0.039588894695043564, 'vf_loss': 0.03646411251269709, 'total_loss': 0.0018280957137692677, 'approx_kl': 0.02640750538557768, 'clip_fraction': 0.2838541716337204, 'grad_norm': 8.427284240722656}
2022-12-29 19:26:08.122 DEBUG: Taking gradient step
2022-12-29 19:26:08.131 DEBUG: Loss 4: {'policy_loss': 0.05035212734657118, 'entropy_loss': -0.03920413739979267, 'vf_loss': 0.04063130832702449, 'total_loss': 0.05177929827380299, 'approx_kl': 0.04468358703888953, 'clip_fraction': 0.328125, 'grad_norm': 11.376346588134766}
2022-12-29 19:26:09.024 DEBUG: Taking gradient step
2022-12-29 19:26:09.038 DEBUG: Loss 5: {'policy_loss': -0.0200590783621811, 'entropy_loss': -0.03835206478834152, 'vf_loss': 0.03443770823984398, 'total_loss': -0.02397343491067864, 'approx_kl': 0.04167020844761282, 'clip_fraction': 0.3098958358168602, 'grad_norm': 5.131464958190918}
2022-12-29 19:26:09.845 DEBUG: Taking gradient step
2022-12-29 19:26:09.855 DEBUG: Loss 6: {'policy_loss': -0.05983187334262752, 'entropy_loss': -0.040024000220000744, 'vf_loss': 0.030173156393709835, 'total_loss': -0.06968271716891844, 'approx_kl': 0.03605904523283243, 'clip_fraction': 0.3489583358168602, 'grad_norm': 8.302021980285645}
2022-12-29 19:26:10.671 DEBUG: Early stopping at step 7 for reaching max KL.
2022-12-29 19:26:10.672 INFO: Optimization: policy loss=-0.060, vf loss=0.030, entropy loss=-0.040, total loss=-0.070, num steps=7
2022-12-29 19:26:10.672 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:26:12.115 INFO: Evaluation rollout: return=0.784 (0.0), episode length=5.0
2022-12-29 19:26:12.116 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:26:12.118 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-4536.model
2022-12-29 19:26:12.122 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-6696.model
2022-12-29 19:26:12.152 INFO: Iteration: 31/137, steps: 6696
2022-12-29 19:26:43.000 DEBUG: Atoms are too close
2022-12-29 19:27:09.187 INFO: Training rollout: return=-0.059 (3.3), episode length=5.0
2022-12-29 19:27:09.188 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:27:09.190 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-6696_train.pkl
2022-12-29 19:27:10.041 DEBUG: Taking gradient step
2022-12-29 19:27:10.052 DEBUG: Loss 0: {'policy_loss': -0.02963423667478607, 'entropy_loss': -0.04056965094059706, 'vf_loss': 0.00990955326872344, 'total_loss': -0.06029433434665968, 'approx_kl': -8.137430995702744e-08, 'clip_fraction': 0.0, 'grad_norm': 19.48232078552246}
2022-12-29 19:27:10.889 DEBUG: Taking gradient step
2022-12-29 19:27:10.898 DEBUG: Loss 1: {'policy_loss': -0.033330560158216074, 'entropy_loss': -0.040182541124522686, 'vf_loss': 0.009769972728304979, 'total_loss': -0.06374312855443379, 'approx_kl': 0.009166342788375914, 'clip_fraction': 0.033854166977107525, 'grad_norm': 11.814671516418457}
2022-12-29 19:27:11.737 DEBUG: Taking gradient step
2022-12-29 19:27:11.747 DEBUG: Loss 2: {'policy_loss': -0.030135719147937216, 'entropy_loss': -0.03953281510621309, 'vf_loss': 0.009621870989646506, 'total_loss': -0.06004666326450381, 'approx_kl': 0.02460723789408803, 'clip_fraction': 0.24609375, 'grad_norm': 3.2690670490264893}
2022-12-29 19:27:12.554 DEBUG: Taking gradient step
2022-12-29 19:27:12.564 DEBUG: Loss 3: {'policy_loss': -0.03481675451758647, 'entropy_loss': -0.040525796823203564, 'vf_loss': 0.009462366718046216, 'total_loss': -0.06588018462274382, 'approx_kl': 0.03395114676095545, 'clip_fraction': 0.3125, 'grad_norm': 3.5747931003570557}
2022-12-29 19:27:13.378 DEBUG: Taking gradient step
2022-12-29 19:27:13.387 DEBUG: Loss 4: {'policy_loss': -0.038077204589140386, 'entropy_loss': -0.040324825793504715, 'vf_loss': 0.009247157070510104, 'total_loss': -0.06915487331213499, 'approx_kl': 0.03940625721588731, 'clip_fraction': 0.2786458358168602, 'grad_norm': 2.8467612266540527}
2022-12-29 19:27:14.193 DEBUG: Taking gradient step
2022-12-29 19:27:14.202 DEBUG: Loss 5: {'policy_loss': -0.016907708895980943, 'entropy_loss': -0.03868210129439831, 'vf_loss': 0.011280048304868664, 'total_loss': -0.0443097618855106, 'approx_kl': 0.02532971976324916, 'clip_fraction': 0.2057291679084301, 'grad_norm': 4.66085147857666}
2022-12-29 19:27:15.003 DEBUG: Taking gradient step
2022-12-29 19:27:15.012 DEBUG: Loss 6: {'policy_loss': -0.05780380499768912, 'entropy_loss': -0.03870656434446573, 'vf_loss': 0.009037776660290871, 'total_loss': -0.08747259268186398, 'approx_kl': 0.007377471774816513, 'clip_fraction': 0.15494791697710752, 'grad_norm': 4.606565475463867}
2022-12-29 19:27:15.861 DEBUG: Taking gradient step
2022-12-29 19:27:15.870 DEBUG: Loss 7: {'policy_loss': -0.004078155225754068, 'entropy_loss': -0.03934598807245493, 'vf_loss': 0.010911015610504348, 'total_loss': -0.032513127687704646, 'approx_kl': 0.002467040583724156, 'clip_fraction': 0.25, 'grad_norm': 11.31459903717041}
2022-12-29 19:27:16.729 DEBUG: Taking gradient step
2022-12-29 19:27:16.738 DEBUG: Loss 8: {'policy_loss': -0.03516742576638369, 'entropy_loss': -0.0407235873863101, 'vf_loss': 0.008476091766848668, 'total_loss': -0.06741492138584512, 'approx_kl': 0.003996678628027439, 'clip_fraction': 0.3567708432674408, 'grad_norm': 5.70632791519165}
2022-12-29 19:27:17.607 DEBUG: Taking gradient step
2022-12-29 19:27:17.616 DEBUG: Loss 9: {'policy_loss': -0.036794469861939, 'entropy_loss': -0.04003630764782429, 'vf_loss': 0.008452590394713485, 'total_loss': -0.06837818711504981, 'approx_kl': 0.025226424913853407, 'clip_fraction': 0.34375, 'grad_norm': 2.9100255966186523}
2022-12-29 19:27:17.616 INFO: Optimization: policy loss=-0.037, vf loss=0.008, entropy loss=-0.040, total loss=-0.068, num steps=10
2022-12-29 19:27:17.617 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:27:19.082 INFO: Evaluation rollout: return=0.779 (0.0), episode length=5.0
2022-12-29 19:27:19.083 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:27:19.086 INFO: Iteration: 32/137, steps: 6912
2022-12-29 19:27:51.572 DEBUG: Atoms are too close
2022-12-29 19:28:17.088 INFO: Training rollout: return=-0.033 (3.3), episode length=5.0
2022-12-29 19:28:17.090 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:28:17.092 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-6912_train.pkl
2022-12-29 19:28:17.902 DEBUG: Taking gradient step
2022-12-29 19:28:17.911 DEBUG: Loss 0: {'policy_loss': -0.03288089119251762, 'entropy_loss': -0.03892950154840946, 'vf_loss': 0.008607889529427313, 'total_loss': -0.06320250321149978, 'approx_kl': 3.845586071804519e-08, 'clip_fraction': 0.0, 'grad_norm': 10.056817054748535}
2022-12-29 19:28:18.751 DEBUG: Taking gradient step
2022-12-29 19:28:18.762 DEBUG: Loss 1: {'policy_loss': 0.10567486411577925, 'entropy_loss': -0.039864434860646725, 'vf_loss': 0.015357153039819625, 'total_loss': 0.08116758229495216, 'approx_kl': -0.0007717114931438118, 'clip_fraction': 0.04817708395421505, 'grad_norm': 19.095355987548828}
2022-12-29 19:28:19.558 DEBUG: Taking gradient step
2022-12-29 19:28:19.572 DEBUG: Loss 2: {'policy_loss': -0.03307394415574059, 'entropy_loss': -0.03848553914576769, 'vf_loss': 0.00839573018333415, 'total_loss': -0.06316375311817413, 'approx_kl': 0.013302463688887656, 'clip_fraction': 0.1197916679084301, 'grad_norm': 5.798398017883301}
2022-12-29 19:28:20.385 DEBUG: Taking gradient step
2022-12-29 19:28:20.394 DEBUG: Loss 3: {'policy_loss': -0.03413506385727236, 'entropy_loss': -0.03908351808786392, 'vf_loss': 0.00836270253562324, 'total_loss': -0.06485587940951304, 'approx_kl': 0.03523620776832104, 'clip_fraction': 0.19270833395421505, 'grad_norm': 2.6434826850891113}
2022-12-29 19:28:21.223 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 19:28:21.223 INFO: Optimization: policy loss=-0.034, vf loss=0.008, entropy loss=-0.039, total loss=-0.065, num steps=4
2022-12-29 19:28:21.224 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:28:22.679 INFO: Evaluation rollout: return=0.782 (0.0), episode length=5.0
2022-12-29 19:28:22.681 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:28:22.684 INFO: Iteration: 33/137, steps: 7128
2022-12-29 19:28:32.574 DEBUG: Atoms are too close
2022-12-29 19:28:35.048 DEBUG: Atoms are too close
2022-12-29 19:28:35.049 DEBUG: Atoms are too close
2022-12-29 19:28:48.270 DEBUG: Atoms are too close
2022-12-29 19:28:50.560 DEBUG: Atoms are too close
2022-12-29 19:28:52.774 DEBUG: Atoms are too close
2022-12-29 19:28:59.016 DEBUG: Atoms are too close
2022-12-29 19:29:02.989 DEBUG: Atoms are too close
2022-12-29 19:29:07.598 DEBUG: Atoms are too close
2022-12-29 19:29:17.717 INFO: Training rollout: return=-4.415 (8.7), episode length=4.9
2022-12-29 19:29:17.719 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:29:17.721 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-7128_train.pkl
2022-12-29 19:29:18.559 DEBUG: Taking gradient step
2022-12-29 19:29:18.568 DEBUG: Loss 0: {'policy_loss': 0.05268990114709899, 'entropy_loss': -0.037604584358632565, 'vf_loss': 0.08255759408377047, 'total_loss': 0.09764291087223691, 'approx_kl': -2.8444143929107213e-08, 'clip_fraction': 0.0, 'grad_norm': 28.329832077026367}
2022-12-29 19:29:19.359 DEBUG: Taking gradient step
2022-12-29 19:29:19.368 DEBUG: Loss 1: {'policy_loss': -0.030460917387093066, 'entropy_loss': -0.038221356458961964, 'vf_loss': 0.07080803340828686, 'total_loss': 0.0021257595622318215, 'approx_kl': -0.0045106198522262275, 'clip_fraction': 0.049479166977107525, 'grad_norm': 17.83295440673828}
2022-12-29 19:29:20.155 DEBUG: Taking gradient step
2022-12-29 19:29:20.165 DEBUG: Loss 2: {'policy_loss': 0.06765814434132911, 'entropy_loss': -0.03902732767164707, 'vf_loss': 0.08564836711992904, 'total_loss': 0.11427918378961109, 'approx_kl': 0.006006475694448454, 'clip_fraction': 0.3020833358168602, 'grad_norm': 17.575937271118164}
2022-12-29 19:29:20.962 DEBUG: Taking gradient step
2022-12-29 19:29:20.977 DEBUG: Loss 3: {'policy_loss': 0.0552288034907991, 'entropy_loss': -0.04031355865299702, 'vf_loss': 0.08622832805848872, 'total_loss': 0.1011435728962908, 'approx_kl': 0.014036467066034675, 'clip_fraction': 0.3619791716337204, 'grad_norm': 14.278287887573242}
2022-12-29 19:29:21.789 DEBUG: Taking gradient step
2022-12-29 19:29:21.799 DEBUG: Loss 4: {'policy_loss': -0.03767557856125259, 'entropy_loss': -0.03900068998336792, 'vf_loss': 0.07100195567524084, 'total_loss': -0.005674312869379659, 'approx_kl': 0.01892312988638878, 'clip_fraction': 0.3528645858168602, 'grad_norm': 11.98807144165039}
2022-12-29 19:29:22.593 DEBUG: Taking gradient step
2022-12-29 19:29:22.602 DEBUG: Loss 5: {'policy_loss': -0.04158270680185859, 'entropy_loss': -0.038072903640568256, 'vf_loss': 0.06988815321342742, 'total_loss': -0.00976745722899943, 'approx_kl': 0.030601175501942635, 'clip_fraction': 0.2955729216337204, 'grad_norm': 7.74251127243042}
2022-12-29 19:29:23.393 DEBUG: Taking gradient step
2022-12-29 19:29:23.402 DEBUG: Loss 6: {'policy_loss': -0.004370594089254606, 'entropy_loss': -0.039864661172032356, 'vf_loss': 0.0749106329077941, 'total_loss': 0.030675377646507133, 'approx_kl': 0.02243754593655467, 'clip_fraction': 0.2473958358168602, 'grad_norm': 17.621076583862305}
2022-12-29 19:29:24.210 DEBUG: Taking gradient step
2022-12-29 19:29:24.219 DEBUG: Loss 7: {'policy_loss': -0.013349202167199918, 'entropy_loss': -0.038218443281948566, 'vf_loss': 0.0741911732442643, 'total_loss': 0.02262352779511581, 'approx_kl': 0.04149128030985594, 'clip_fraction': 0.2252604179084301, 'grad_norm': 10.128318786621094}
2022-12-29 19:29:25.132 DEBUG: Taking gradient step
2022-12-29 19:29:25.145 DEBUG: Loss 8: {'policy_loss': -0.036085054593660446, 'entropy_loss': -0.039918772876262665, 'vf_loss': 0.07085528072492818, 'total_loss': -0.005148546744994924, 'approx_kl': 0.017173252708744258, 'clip_fraction': 0.1888020858168602, 'grad_norm': 8.676508903503418}
2022-12-29 19:29:26.078 DEBUG: Taking gradient step
2022-12-29 19:29:26.087 DEBUG: Loss 9: {'policy_loss': -0.03242380118963524, 'entropy_loss': -0.03912865370512009, 'vf_loss': 0.07039169174287899, 'total_loss': -0.0011607631518763487, 'approx_kl': 0.014979698695242405, 'clip_fraction': 0.2317708358168602, 'grad_norm': 12.572457313537598}
2022-12-29 19:29:26.087 INFO: Optimization: policy loss=-0.032, vf loss=0.070, entropy loss=-0.039, total loss=-0.001, num steps=10
2022-12-29 19:29:26.088 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:29:27.532 INFO: Evaluation rollout: return=0.791 (0.0), episode length=5.0
2022-12-29 19:29:27.533 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:29:27.535 INFO: Iteration: 34/137, steps: 7344
2022-12-29 19:29:57.669 DEBUG: Atoms are too close
2022-12-29 19:29:58.545 DEBUG: Atoms are too close
2022-12-29 19:29:58.837 DEBUG: Atoms are too close
2022-12-29 19:30:12.199 DEBUG: Atoms are too close
2022-12-29 19:30:22.869 DEBUG: Atoms are too close
2022-12-29 19:30:23.778 INFO: Training rollout: return=-2.194 (6.9), episode length=4.9
2022-12-29 19:30:23.780 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:30:23.782 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-7344_train.pkl
2022-12-29 19:30:24.666 DEBUG: Taking gradient step
2022-12-29 19:30:24.684 DEBUG: Loss 0: {'policy_loss': 0.049531503896385935, 'entropy_loss': -0.03807949600741267, 'vf_loss': 0.044919899940472144, 'total_loss': 0.056371907829445414, 'approx_kl': 3.531264769485176e-08, 'clip_fraction': 0.0, 'grad_norm': 11.062000274658203}
2022-12-29 19:30:25.495 DEBUG: Taking gradient step
2022-12-29 19:30:25.505 DEBUG: Loss 1: {'policy_loss': -0.02401918197985213, 'entropy_loss': -0.04098971653729677, 'vf_loss': 0.03768978884647278, 'total_loss': -0.027319109670676123, 'approx_kl': 0.024277427815832198, 'clip_fraction': 0.1119791679084301, 'grad_norm': 5.878330230712891}
2022-12-29 19:30:26.367 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:30:26.368 INFO: Optimization: policy loss=-0.024, vf loss=0.038, entropy loss=-0.041, total loss=-0.027, num steps=2
2022-12-29 19:30:26.368 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:30:27.733 INFO: Evaluation rollout: return=0.795 (0.0), episode length=5.0
2022-12-29 19:30:27.734 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:30:27.736 INFO: Iteration: 35/137, steps: 7560
2022-12-29 19:31:25.289 INFO: Training rollout: return=0.479 (0.1), episode length=5.0
2022-12-29 19:31:25.290 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:31:25.292 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-7560_train.pkl
2022-12-29 19:31:26.114 DEBUG: Taking gradient step
2022-12-29 19:31:26.123 DEBUG: Loss 0: {'policy_loss': -0.008227140078405491, 'entropy_loss': -0.039159257896244526, 'vf_loss': 0.0017146409693992138, 'total_loss': -0.04567175700525081, 'approx_kl': -1.4117298263727207e-07, 'clip_fraction': 0.0, 'grad_norm': 7.5101776123046875}
2022-12-29 19:31:26.989 DEBUG: Taking gradient step
2022-12-29 19:31:26.998 DEBUG: Loss 1: {'policy_loss': -0.013506669605272985, 'entropy_loss': -0.03985717333853245, 'vf_loss': 0.001964209508212543, 'total_loss': -0.05139963343559289, 'approx_kl': -0.0030571415554732084, 'clip_fraction': 0.06640625, 'grad_norm': 8.339214324951172}
2022-12-29 19:31:27.833 DEBUG: Taking gradient step
2022-12-29 19:31:27.847 DEBUG: Loss 2: {'policy_loss': 0.004550539696547024, 'entropy_loss': -0.03965178411453962, 'vf_loss': 0.002167912205398923, 'total_loss': -0.032933332212593674, 'approx_kl': -0.0007594726048409939, 'clip_fraction': 0.3125, 'grad_norm': 12.563746452331543}
2022-12-29 19:31:28.711 DEBUG: Taking gradient step
2022-12-29 19:31:28.721 DEBUG: Loss 3: {'policy_loss': 0.05503697370572925, 'entropy_loss': -0.03900301922112703, 'vf_loss': 0.0022541232795495257, 'total_loss': 0.01828807776415174, 'approx_kl': 0.009060781449079514, 'clip_fraction': 0.3919270858168602, 'grad_norm': 9.983759880065918}
2022-12-29 19:31:29.556 DEBUG: Taking gradient step
2022-12-29 19:31:29.565 DEBUG: Loss 4: {'policy_loss': -0.022049303336624936, 'entropy_loss': -0.03927595820277929, 'vf_loss': 0.002474855140956824, 'total_loss': -0.058850406398447394, 'approx_kl': 0.040745004545897245, 'clip_fraction': 0.3151041716337204, 'grad_norm': 4.237704277038574}
2022-12-29 19:31:30.362 DEBUG: Early stopping at step 5 for reaching max KL.
2022-12-29 19:31:30.363 INFO: Optimization: policy loss=-0.022, vf loss=0.002, entropy loss=-0.039, total loss=-0.059, num steps=5
2022-12-29 19:31:30.363 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:31:31.751 INFO: Evaluation rollout: return=0.792 (0.0), episode length=5.0
2022-12-29 19:31:31.752 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:31:31.754 INFO: Iteration: 36/137, steps: 7776
2022-12-29 19:32:01.046 DEBUG: Atoms are too close
2022-12-29 19:32:01.344 DEBUG: Atoms are too close
2022-12-29 19:32:28.997 INFO: Training rollout: return=-0.585 (4.6), episode length=5.0
2022-12-29 19:32:28.999 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:32:29.001 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-7776_train.pkl
2022-12-29 19:32:29.934 DEBUG: Taking gradient step
2022-12-29 19:32:29.943 DEBUG: Loss 0: {'policy_loss': 0.020292250499257508, 'entropy_loss': -0.04019172769039869, 'vf_loss': 0.020000076012168674, 'total_loss': 0.00010059882102749634, 'approx_kl': -2.1498030022826242e-08, 'clip_fraction': 0.0, 'grad_norm': 11.68014907836914}
2022-12-29 19:32:30.811 DEBUG: Taking gradient step
2022-12-29 19:32:30.823 DEBUG: Loss 1: {'policy_loss': 0.019985387489151074, 'entropy_loss': -0.04009876772761345, 'vf_loss': 0.019767110773555384, 'total_loss': -0.0003462694649069911, 'approx_kl': 0.00472296291263774, 'clip_fraction': 0.04817708395421505, 'grad_norm': 14.570795059204102}
2022-12-29 19:32:31.628 DEBUG: Taking gradient step
2022-12-29 19:32:31.637 DEBUG: Loss 2: {'policy_loss': 0.003924341728021575, 'entropy_loss': -0.04027144331485033, 'vf_loss': 0.019976654812971534, 'total_loss': -0.016370446773857218, 'approx_kl': 0.0011162209557369351, 'clip_fraction': 0.11197916697710752, 'grad_norm': 10.805839538574219}
2022-12-29 19:32:32.461 DEBUG: Taking gradient step
2022-12-29 19:32:32.470 DEBUG: Loss 3: {'policy_loss': -0.0177034309634945, 'entropy_loss': -0.03936068248003721, 'vf_loss': 0.017619828086775577, 'total_loss': -0.039444285356756134, 'approx_kl': -0.0020064092241227627, 'clip_fraction': 0.2942708358168602, 'grad_norm': 4.558409214019775}
2022-12-29 19:32:33.298 DEBUG: Taking gradient step
2022-12-29 19:32:33.307 DEBUG: Loss 4: {'policy_loss': -0.02580973314675932, 'entropy_loss': -0.0402769735082984, 'vf_loss': 0.017690037394668155, 'total_loss': -0.04839666926038956, 'approx_kl': 0.020526912063360214, 'clip_fraction': 0.3359375, 'grad_norm': 5.469964027404785}
2022-12-29 19:32:34.126 DEBUG: Taking gradient step
2022-12-29 19:32:34.136 DEBUG: Loss 5: {'policy_loss': -0.044669837069859504, 'entropy_loss': -0.03951520752161741, 'vf_loss': 0.015383762208301628, 'total_loss': -0.06880128238317529, 'approx_kl': 0.015221964490592654, 'clip_fraction': 0.3294270858168602, 'grad_norm': 4.143754482269287}
2022-12-29 19:32:34.957 DEBUG: Taking gradient step
2022-12-29 19:32:34.971 DEBUG: Loss 6: {'policy_loss': -0.0237454624041151, 'entropy_loss': -0.03991504479199648, 'vf_loss': 0.0174974568760348, 'total_loss': -0.04616305032007678, 'approx_kl': 0.04394840169698, 'clip_fraction': 0.3606770858168602, 'grad_norm': 4.725351333618164}
2022-12-29 19:32:35.845 DEBUG: Taking gradient step
2022-12-29 19:32:35.855 DEBUG: Loss 7: {'policy_loss': -0.057790303903141105, 'entropy_loss': -0.040236590430140495, 'vf_loss': 0.015366188591128763, 'total_loss': -0.08266070574215283, 'approx_kl': 0.007593768532387912, 'clip_fraction': 0.3138020858168602, 'grad_norm': 2.496551990509033}
2022-12-29 19:32:36.703 DEBUG: Taking gradient step
2022-12-29 19:32:36.712 DEBUG: Loss 8: {'policy_loss': -0.00527079584694649, 'entropy_loss': -0.04042540863156319, 'vf_loss': 0.019711276917524743, 'total_loss': -0.025984927560984934, 'approx_kl': 0.0011724770592991263, 'clip_fraction': 0.296875, 'grad_norm': 7.222516059875488}
2022-12-29 19:32:37.588 DEBUG: Taking gradient step
2022-12-29 19:32:37.602 DEBUG: Loss 9: {'policy_loss': -0.02658500165064554, 'entropy_loss': -0.04003409296274185, 'vf_loss': 0.017514967159202255, 'total_loss': -0.04910412745418513, 'approx_kl': 0.004296242957934737, 'clip_fraction': 0.2760416716337204, 'grad_norm': 7.116728782653809}
2022-12-29 19:32:37.603 INFO: Optimization: policy loss=-0.027, vf loss=0.018, entropy loss=-0.040, total loss=-0.049, num steps=10
2022-12-29 19:32:37.603 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:32:39.130 INFO: Evaluation rollout: return=0.654 (0.0), episode length=5.0
2022-12-29 19:32:39.131 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:32:39.133 INFO: Iteration: 37/137, steps: 7992
2022-12-29 19:33:09.719 DEBUG: Atoms are too close
2022-12-29 19:33:34.290 DEBUG: There is a single atom floating around
2022-12-29 19:33:37.016 INFO: Training rollout: return=-0.626 (4.6), episode length=4.9
2022-12-29 19:33:37.018 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:33:37.022 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-7992_train.pkl
2022-12-29 19:33:37.827 DEBUG: Taking gradient step
2022-12-29 19:33:37.837 DEBUG: Loss 0: {'policy_loss': 0.03334314751118068, 'entropy_loss': -0.04013179987668991, 'vf_loss': 0.01730179332376279, 'total_loss': 0.010513140958253557, 'approx_kl': 1.482355216353426e-08, 'clip_fraction': 0.0, 'grad_norm': 24.656131744384766}
2022-12-29 19:33:38.623 DEBUG: Taking gradient step
2022-12-29 19:33:38.632 DEBUG: Loss 1: {'policy_loss': 0.02334318854525233, 'entropy_loss': -0.037152709905058146, 'vf_loss': 0.017176518593525897, 'total_loss': 0.0033669972337200815, 'approx_kl': 0.006530369515530765, 'clip_fraction': 0.0859375, 'grad_norm': 15.884284973144531}
2022-12-29 19:33:39.437 DEBUG: Taking gradient step
2022-12-29 19:33:39.448 DEBUG: Loss 2: {'policy_loss': -0.01636161008494669, 'entropy_loss': -0.03906118497252464, 'vf_loss': 0.014768703124860463, 'total_loss': -0.04065409193261087, 'approx_kl': 0.034880545223131776, 'clip_fraction': 0.3671875, 'grad_norm': 5.721026420593262}
2022-12-29 19:33:40.284 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 19:33:40.284 INFO: Optimization: policy loss=-0.016, vf loss=0.015, entropy loss=-0.039, total loss=-0.041, num steps=3
2022-12-29 19:33:40.285 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:33:41.730 INFO: Evaluation rollout: return=0.647 (0.0), episode length=5.0
2022-12-29 19:33:41.732 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:33:41.735 INFO: Iteration: 38/137, steps: 8208
2022-12-29 19:33:55.515 DEBUG: Atoms are too close
2022-12-29 19:34:03.748 DEBUG: Atoms are too close
2022-12-29 19:34:04.679 DEBUG: Atoms are too close
2022-12-29 19:34:38.834 INFO: Training rollout: return=-1.123 (5.5), episode length=4.9
2022-12-29 19:34:38.836 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:34:38.839 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-8208_train.pkl
2022-12-29 19:34:39.682 DEBUG: Taking gradient step
2022-12-29 19:34:39.691 DEBUG: Loss 0: {'policy_loss': 0.053895782949878464, 'entropy_loss': -0.03831803426146507, 'vf_loss': 0.023557203103815132, 'total_loss': 0.03913495179222852, 'approx_kl': 5.510324641022635e-09, 'clip_fraction': 0.0, 'grad_norm': 28.405048370361328}
2022-12-29 19:34:40.512 DEBUG: Taking gradient step
2022-12-29 19:34:40.521 DEBUG: Loss 1: {'policy_loss': 0.012281000444656309, 'entropy_loss': -0.04059218987822533, 'vf_loss': 0.02146004871333526, 'total_loss': -0.00685114072023376, 'approx_kl': 0.008042605593800545, 'clip_fraction': 0.053385416977107525, 'grad_norm': 18.441381454467773}
2022-12-29 19:34:41.334 DEBUG: Taking gradient step
2022-12-29 19:34:41.346 DEBUG: Loss 2: {'policy_loss': 0.03147448122217937, 'entropy_loss': -0.03925357013940811, 'vf_loss': 0.023808678737310095, 'total_loss': 0.016029589820081354, 'approx_kl': 0.028754337457939982, 'clip_fraction': 0.30859375, 'grad_norm': 10.250723838806152}
2022-12-29 19:34:42.173 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 19:34:42.173 INFO: Optimization: policy loss=0.031, vf loss=0.024, entropy loss=-0.039, total loss=0.016, num steps=3
2022-12-29 19:34:42.173 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:34:43.563 INFO: Evaluation rollout: return=0.758 (0.0), episode length=5.0
2022-12-29 19:34:43.564 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:34:43.567 INFO: Iteration: 39/137, steps: 8424
2022-12-29 19:35:11.497 DEBUG: There is a single atom floating around
2022-12-29 19:35:23.885 DEBUG: Atoms are too close
2022-12-29 19:35:40.703 INFO: Training rollout: return=-0.623 (4.6), episode length=4.9
2022-12-29 19:35:40.705 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:35:40.707 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-8424_train.pkl
2022-12-29 19:35:41.519 DEBUG: Taking gradient step
2022-12-29 19:35:41.530 DEBUG: Loss 0: {'policy_loss': 0.04176403203231123, 'entropy_loss': -0.03911126498132944, 'vf_loss': 0.015672515055452408, 'total_loss': 0.018325282106434193, 'approx_kl': 2.650388886138444e-08, 'clip_fraction': 0.0, 'grad_norm': 7.424863338470459}
2022-12-29 19:35:42.344 DEBUG: Taking gradient step
2022-12-29 19:35:42.353 DEBUG: Loss 1: {'policy_loss': 0.07081732959574238, 'entropy_loss': -0.03934683185070753, 'vf_loss': 0.018083500000238825, 'total_loss': 0.04955399774527368, 'approx_kl': 0.009978391230106354, 'clip_fraction': 0.049479166977107525, 'grad_norm': 13.137944221496582}
2022-12-29 19:35:43.238 DEBUG: Taking gradient step
2022-12-29 19:35:43.251 DEBUG: Loss 2: {'policy_loss': -0.03580259515593293, 'entropy_loss': -0.03781442530453205, 'vf_loss': 0.010968128744528353, 'total_loss': -0.06264889171593663, 'approx_kl': 0.03230767681452562, 'clip_fraction': 0.2369791679084301, 'grad_norm': 3.360208034515381}
2022-12-29 19:35:44.055 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 19:35:44.055 INFO: Optimization: policy loss=-0.036, vf loss=0.011, entropy loss=-0.038, total loss=-0.063, num steps=3
2022-12-29 19:35:44.056 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:35:45.521 INFO: Evaluation rollout: return=0.751 (0.0), episode length=5.0
2022-12-29 19:35:45.522 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:35:45.525 INFO: Iteration: 40/137, steps: 8640
2022-12-29 19:35:54.114 DEBUG: Atoms are too close
2022-12-29 19:36:14.217 DEBUG: There is a single atom floating around
2022-12-29 19:36:43.034 INFO: Training rollout: return=-0.551 (4.6), episode length=4.9
2022-12-29 19:36:43.036 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:36:43.038 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-8640_train.pkl
2022-12-29 19:36:43.880 DEBUG: Taking gradient step
2022-12-29 19:36:43.893 DEBUG: Loss 0: {'policy_loss': 0.002900792829608709, 'entropy_loss': -0.03747803717851639, 'vf_loss': 0.014708483872767885, 'total_loss': -0.019868760476139787, 'approx_kl': 3.6476800691076505e-08, 'clip_fraction': 0.0, 'grad_norm': 9.832033157348633}
2022-12-29 19:36:44.703 DEBUG: Taking gradient step
2022-12-29 19:36:44.713 DEBUG: Loss 1: {'policy_loss': -0.01092734249302362, 'entropy_loss': -0.03787849843502045, 'vf_loss': 0.014853048254865706, 'total_loss': -0.033952792673178356, 'approx_kl': 0.0009820197010412812, 'clip_fraction': 0.049479166977107525, 'grad_norm': 7.722604751586914}
2022-12-29 19:36:45.620 DEBUG: Taking gradient step
2022-12-29 19:36:45.630 DEBUG: Loss 2: {'policy_loss': -0.01660156762324199, 'entropy_loss': -0.03754688799381256, 'vf_loss': 0.014710520270278478, 'total_loss': -0.039437935346776073, 'approx_kl': 0.019034007331356406, 'clip_fraction': 0.24609375, 'grad_norm': 2.8038265705108643}
2022-12-29 19:36:46.424 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 19:36:46.424 INFO: Optimization: policy loss=-0.017, vf loss=0.015, entropy loss=-0.038, total loss=-0.039, num steps=3
2022-12-29 19:36:46.425 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:36:47.914 INFO: Evaluation rollout: return=0.768 (0.0), episode length=5.0
2022-12-29 19:36:47.915 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:36:47.918 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-6696.model
2022-12-29 19:36:47.922 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-8856.model
2022-12-29 19:36:47.952 INFO: Iteration: 41/137, steps: 8856
2022-12-29 19:37:45.487 INFO: Training rollout: return=0.577 (0.1), episode length=5.0
2022-12-29 19:37:45.488 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:37:45.491 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-8856_train.pkl
2022-12-29 19:37:46.297 DEBUG: Taking gradient step
2022-12-29 19:37:46.307 DEBUG: Loss 0: {'policy_loss': 0.016768958149302134, 'entropy_loss': -0.037025860510766506, 'vf_loss': 0.0009711974544355313, 'total_loss': -0.01928570490702885, 'approx_kl': 2.6581499668054676e-08, 'clip_fraction': 0.0, 'grad_norm': 12.421324729919434}
2022-12-29 19:37:47.128 DEBUG: Taking gradient step
2022-12-29 19:37:47.137 DEBUG: Loss 1: {'policy_loss': 0.01630487688111859, 'entropy_loss': -0.03658125828951597, 'vf_loss': 0.0009339071742672427, 'total_loss': -0.01934247423413014, 'approx_kl': 0.009331569075584412, 'clip_fraction': 0.140625, 'grad_norm': 11.165289878845215}
2022-12-29 19:37:48.023 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:37:48.023 INFO: Optimization: policy loss=0.016, vf loss=0.001, entropy loss=-0.037, total loss=-0.019, num steps=2
2022-12-29 19:37:48.023 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:37:49.442 INFO: Evaluation rollout: return=0.785 (0.0), episode length=5.0
2022-12-29 19:37:49.443 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:37:49.446 INFO: Iteration: 42/137, steps: 9072
2022-12-29 19:38:47.546 INFO: Training rollout: return=0.582 (0.1), episode length=5.0
2022-12-29 19:38:47.547 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:38:47.550 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-9072_train.pkl
2022-12-29 19:38:48.445 DEBUG: Taking gradient step
2022-12-29 19:38:48.457 DEBUG: Loss 0: {'policy_loss': -0.0057366847544607635, 'entropy_loss': -0.033862643875181675, 'vf_loss': 0.00094233814365363, 'total_loss': -0.038656990485988814, 'approx_kl': 9.060993733100986e-09, 'clip_fraction': 0.0, 'grad_norm': 14.027971267700195}
2022-12-29 19:38:49.265 DEBUG: Taking gradient step
2022-12-29 19:38:49.277 DEBUG: Loss 1: {'policy_loss': 0.03258457754122943, 'entropy_loss': -0.032706710044294596, 'vf_loss': 0.0008639030209819357, 'total_loss': 0.0007417705179167747, 'approx_kl': 0.011048792395740747, 'clip_fraction': 0.1966145858168602, 'grad_norm': 6.807839393615723}
2022-12-29 19:38:50.121 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:38:50.121 INFO: Optimization: policy loss=0.033, vf loss=0.001, entropy loss=-0.033, total loss=0.001, num steps=2
2022-12-29 19:38:50.122 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:38:51.558 INFO: Evaluation rollout: return=0.794 (0.0), episode length=5.0
2022-12-29 19:38:51.559 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:38:51.562 INFO: Iteration: 43/137, steps: 9288
2022-12-29 19:39:38.124 DEBUG: There is a single atom floating around
2022-12-29 19:39:49.152 INFO: Training rollout: return=0.029 (3.3), episode length=5.0
2022-12-29 19:39:49.154 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:39:49.157 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-9288_train.pkl
2022-12-29 19:39:50.003 DEBUG: Taking gradient step
2022-12-29 19:39:50.013 DEBUG: Loss 0: {'policy_loss': -0.02158061655486329, 'entropy_loss': -0.03332957113161683, 'vf_loss': 0.008064216162154823, 'total_loss': -0.04684597152432529, 'approx_kl': -5.886735010562916e-08, 'clip_fraction': 0.0, 'grad_norm': 14.52067756652832}
2022-12-29 19:39:50.823 DEBUG: Taking gradient step
2022-12-29 19:39:50.832 DEBUG: Loss 1: {'policy_loss': -0.039229265986346545, 'entropy_loss': -0.031145951710641384, 'vf_loss': 0.00808404120279061, 'total_loss': -0.062291176494197326, 'approx_kl': 0.028007461689412594, 'clip_fraction': 0.10416666697710752, 'grad_norm': 3.5943801403045654}
2022-12-29 19:39:51.708 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:39:51.708 INFO: Optimization: policy loss=-0.039, vf loss=0.008, entropy loss=-0.031, total loss=-0.062, num steps=2
2022-12-29 19:39:51.709 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:39:53.169 INFO: Evaluation rollout: return=0.793 (0.0), episode length=5.0
2022-12-29 19:39:53.170 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:39:53.172 INFO: Iteration: 44/137, steps: 9504
2022-12-29 19:40:50.626 INFO: Training rollout: return=0.610 (0.1), episode length=5.0
2022-12-29 19:40:50.627 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:40:50.630 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-9504_train.pkl
2022-12-29 19:40:51.465 DEBUG: Taking gradient step
2022-12-29 19:40:51.475 DEBUG: Loss 0: {'policy_loss': 0.021449759841177016, 'entropy_loss': -0.030382923781871796, 'vf_loss': 0.0008302355844070001, 'total_loss': -0.008102928356287772, 'approx_kl': 7.91430156965589e-08, 'clip_fraction': 0.0, 'grad_norm': 8.735549926757812}
2022-12-29 19:40:52.302 DEBUG: Taking gradient step
2022-12-29 19:40:52.313 DEBUG: Loss 1: {'policy_loss': -0.006250850800564959, 'entropy_loss': -0.029512190725654364, 'vf_loss': 0.0008142137633934578, 'total_loss': -0.03494882776282586, 'approx_kl': 0.005154625978320837, 'clip_fraction': 0.041666666977107525, 'grad_norm': 7.92985725402832}
2022-12-29 19:40:53.155 DEBUG: Taking gradient step
2022-12-29 19:40:53.164 DEBUG: Loss 2: {'policy_loss': -0.017253151085585942, 'entropy_loss': -0.029300743248313665, 'vf_loss': 0.0007839926249279879, 'total_loss': -0.04576990170897162, 'approx_kl': 0.003316695336252451, 'clip_fraction': 0.1171875, 'grad_norm': 14.056450843811035}
2022-12-29 19:40:54.038 DEBUG: Taking gradient step
2022-12-29 19:40:54.048 DEBUG: Loss 3: {'policy_loss': -0.013963028368368796, 'entropy_loss': -0.029199576005339622, 'vf_loss': 0.0007138277788376193, 'total_loss': -0.0424487765948708, 'approx_kl': 0.008811364765278995, 'clip_fraction': 0.1705729179084301, 'grad_norm': 12.487760543823242}
2022-12-29 19:40:54.865 DEBUG: Taking gradient step
2022-12-29 19:40:54.880 DEBUG: Loss 4: {'policy_loss': -0.021178937714768583, 'entropy_loss': -0.02868371782824397, 'vf_loss': 0.0006603784755965077, 'total_loss': -0.04920227706741604, 'approx_kl': 0.023584298556670547, 'clip_fraction': 0.2526041716337204, 'grad_norm': 9.582539558410645}
2022-12-29 19:40:55.706 DEBUG: Early stopping at step 5 for reaching max KL.
2022-12-29 19:40:55.706 INFO: Optimization: policy loss=-0.021, vf loss=0.001, entropy loss=-0.029, total loss=-0.049, num steps=5
2022-12-29 19:40:55.707 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:40:57.116 INFO: Evaluation rollout: return=0.791 (0.0), episode length=5.0
2022-12-29 19:40:57.117 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:40:57.119 INFO: Iteration: 45/137, steps: 9720
2022-12-29 19:41:05.157 DEBUG: There is a single atom floating around
2022-12-29 19:41:42.880 DEBUG: Atoms are too close
2022-12-29 19:41:53.961 DEBUG: There is a single atom floating around
2022-12-29 19:41:54.334 INFO: Training rollout: return=-1.016 (5.5), episode length=4.9
2022-12-29 19:41:54.335 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:41:54.338 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-9720_train.pkl
2022-12-29 19:41:55.231 DEBUG: Taking gradient step
2022-12-29 19:41:55.240 DEBUG: Loss 0: {'policy_loss': 0.015622616771580104, 'entropy_loss': -0.02930139424279332, 'vf_loss': 0.021771703182428805, 'total_loss': 0.008092925711215598, 'approx_kl': 2.153683453798294e-09, 'clip_fraction': 0.0, 'grad_norm': 17.616716384887695}
2022-12-29 19:41:56.047 DEBUG: Taking gradient step
2022-12-29 19:41:56.056 DEBUG: Loss 1: {'policy_loss': -0.04966037700590513, 'entropy_loss': -0.029835300985723734, 'vf_loss': 0.01703022161229455, 'total_loss': -0.062465456379334316, 'approx_kl': -0.007040774216875434, 'clip_fraction': 0.18359375, 'grad_norm': 9.093188285827637}
2022-12-29 19:41:56.945 DEBUG: Taking gradient step
2022-12-29 19:41:56.954 DEBUG: Loss 2: {'policy_loss': -0.022371448711237394, 'entropy_loss': -0.03106702445074916, 'vf_loss': 0.019361764729408, 'total_loss': -0.034076708432578554, 'approx_kl': 0.02117911819368601, 'clip_fraction': 0.4270833358168602, 'grad_norm': 5.067368030548096}
2022-12-29 19:41:57.764 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 19:41:57.764 INFO: Optimization: policy loss=-0.022, vf loss=0.019, entropy loss=-0.031, total loss=-0.034, num steps=3
2022-12-29 19:41:57.764 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:41:59.201 INFO: Evaluation rollout: return=0.787 (0.0), episode length=5.0
2022-12-29 19:41:59.203 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:41:59.205 INFO: Iteration: 46/137, steps: 9936
2022-12-29 19:42:12.850 DEBUG: There is a single atom floating around
2022-12-29 19:42:46.548 DEBUG: Atoms are too close
2022-12-29 19:42:57.060 INFO: Training rollout: return=-0.493 (4.6), episode length=5.0
2022-12-29 19:42:57.061 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:42:57.064 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-9936_train.pkl
2022-12-29 19:42:57.882 DEBUG: Taking gradient step
2022-12-29 19:42:57.893 DEBUG: Loss 0: {'policy_loss': 0.033348715773921275, 'entropy_loss': -0.031570745166391134, 'vf_loss': 0.02071706444166638, 'total_loss': 0.02249503504919653, 'approx_kl': -5.840171013460349e-09, 'clip_fraction': 0.0, 'grad_norm': 17.850833892822266}
2022-12-29 19:42:58.678 DEBUG: Taking gradient step
2022-12-29 19:42:58.689 DEBUG: Loss 1: {'policy_loss': 0.05021162914145244, 'entropy_loss': -0.030759041663259268, 'vf_loss': 0.023096892422392427, 'total_loss': 0.04254947990058561, 'approx_kl': -0.0006520008901134133, 'clip_fraction': 0.08463541697710752, 'grad_norm': 9.079439163208008}
2022-12-29 19:42:59.507 DEBUG: Taking gradient step
2022-12-29 19:42:59.517 DEBUG: Loss 2: {'policy_loss': 0.0016436025303900325, 'entropy_loss': -0.03270543832331896, 'vf_loss': 0.01836780832543894, 'total_loss': -0.01269402746748998, 'approx_kl': 0.03600340080447495, 'clip_fraction': 0.2604166716337204, 'grad_norm': 9.281759262084961}
2022-12-29 19:43:00.425 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 19:43:00.425 INFO: Optimization: policy loss=0.002, vf loss=0.018, entropy loss=-0.033, total loss=-0.013, num steps=3
2022-12-29 19:43:00.426 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:43:01.867 INFO: Evaluation rollout: return=0.784 (0.0), episode length=5.0
2022-12-29 19:43:01.868 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:43:01.871 INFO: Iteration: 47/137, steps: 10152
2022-12-29 19:43:33.999 DEBUG: Atoms are too close
2022-12-29 19:43:46.424 DEBUG: Atoms are too close
2022-12-29 19:43:58.444 INFO: Training rollout: return=-0.565 (4.6), episode length=5.0
2022-12-29 19:43:58.446 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:43:58.449 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-10152_train.pkl
2022-12-29 19:43:59.275 DEBUG: Taking gradient step
2022-12-29 19:43:59.284 DEBUG: Loss 0: {'policy_loss': 0.026013838233320312, 'entropy_loss': -0.031395293306559324, 'vf_loss': 0.021169953132832642, 'total_loss': 0.015788498059593627, 'approx_kl': 7.285658876199363e-08, 'clip_fraction': 0.0, 'grad_norm': 16.990049362182617}
2022-12-29 19:44:00.087 DEBUG: Taking gradient step
2022-12-29 19:44:00.098 DEBUG: Loss 1: {'policy_loss': -0.04066023452683992, 'entropy_loss': -0.03020947938784957, 'vf_loss': 0.01565602020216511, 'total_loss': -0.05521369371252438, 'approx_kl': 0.013105740596074611, 'clip_fraction': 0.0390625, 'grad_norm': 4.362873554229736}
2022-12-29 19:44:00.947 DEBUG: Taking gradient step
2022-12-29 19:44:00.956 DEBUG: Loss 2: {'policy_loss': 0.01110559380892227, 'entropy_loss': -0.030448799021542072, 'vf_loss': 0.020588440293410122, 'total_loss': 0.0012452350807903266, 'approx_kl': 0.03776023199316114, 'clip_fraction': 0.2135416679084301, 'grad_norm': 5.443006992340088}
2022-12-29 19:44:01.782 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 19:44:01.783 INFO: Optimization: policy loss=0.011, vf loss=0.021, entropy loss=-0.030, total loss=0.001, num steps=3
2022-12-29 19:44:01.783 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:44:03.262 INFO: Evaluation rollout: return=0.790 (0.0), episode length=5.0
2022-12-29 19:44:03.263 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:44:03.266 INFO: Iteration: 48/137, steps: 10368
2022-12-29 19:45:01.143 INFO: Training rollout: return=0.590 (0.1), episode length=5.0
2022-12-29 19:45:01.145 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:45:01.147 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-10368_train.pkl
2022-12-29 19:45:01.962 DEBUG: Taking gradient step
2022-12-29 19:45:01.971 DEBUG: Loss 0: {'policy_loss': 0.006503445488532622, 'entropy_loss': -0.029929107055068016, 'vf_loss': 0.0005115795598985672, 'total_loss': -0.022914082006636825, 'approx_kl': -1.157168298959732e-07, 'clip_fraction': 0.0, 'grad_norm': 14.321354866027832}
2022-12-29 19:45:02.744 DEBUG: Taking gradient step
2022-12-29 19:45:02.754 DEBUG: Loss 1: {'policy_loss': 0.006680254536472384, 'entropy_loss': -0.031307957135140896, 'vf_loss': 0.000526059743270852, 'total_loss': -0.024101642855397655, 'approx_kl': 0.019770933780819178, 'clip_fraction': 0.1041666679084301, 'grad_norm': 9.182992935180664}
2022-12-29 19:45:03.525 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:45:03.525 INFO: Optimization: policy loss=0.007, vf loss=0.001, entropy loss=-0.031, total loss=-0.024, num steps=2
2022-12-29 19:45:03.526 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:45:05.009 INFO: Evaluation rollout: return=0.793 (0.0), episode length=5.0
2022-12-29 19:45:05.010 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:45:05.013 INFO: Iteration: 49/137, steps: 10584
2022-12-29 19:46:03.195 INFO: Training rollout: return=0.581 (0.1), episode length=5.0
2022-12-29 19:46:03.196 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:46:03.198 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-10584_train.pkl
2022-12-29 19:46:04.010 DEBUG: Taking gradient step
2022-12-29 19:46:04.020 DEBUG: Loss 0: {'policy_loss': 0.011728613837870053, 'entropy_loss': -0.02994004264473915, 'vf_loss': 0.0004892898262961798, 'total_loss': -0.017722138980572916, 'approx_kl': -1.7345882952213287e-08, 'clip_fraction': 0.0, 'grad_norm': 9.65871524810791}
2022-12-29 19:46:04.858 DEBUG: Taking gradient step
2022-12-29 19:46:04.867 DEBUG: Loss 1: {'policy_loss': -0.014297633016439987, 'entropy_loss': -0.030508178751915693, 'vf_loss': 0.0005213914386239618, 'total_loss': -0.04428442032973172, 'approx_kl': 0.017258178209885955, 'clip_fraction': 0.10546875, 'grad_norm': 6.648011684417725}
2022-12-29 19:46:05.691 DEBUG: Taking gradient step
2022-12-29 19:46:05.700 DEBUG: Loss 2: {'policy_loss': -0.034222577218680356, 'entropy_loss': -0.03008621372282505, 'vf_loss': 0.0005338267867451759, 'total_loss': -0.06377496415476022, 'approx_kl': 0.03391089476644993, 'clip_fraction': 0.2838541716337204, 'grad_norm': 7.800601005554199}
2022-12-29 19:46:06.535 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 19:46:06.536 INFO: Optimization: policy loss=-0.034, vf loss=0.001, entropy loss=-0.030, total loss=-0.064, num steps=3
2022-12-29 19:46:06.536 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:46:07.956 INFO: Evaluation rollout: return=0.795 (0.0), episode length=5.0
2022-12-29 19:46:07.957 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:46:07.959 INFO: Iteration: 50/137, steps: 10800
2022-12-29 19:47:06.390 INFO: Training rollout: return=0.580 (0.1), episode length=5.0
2022-12-29 19:47:06.392 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:47:06.395 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-10800_train.pkl
2022-12-29 19:47:07.284 DEBUG: Taking gradient step
2022-12-29 19:47:07.295 DEBUG: Loss 0: {'policy_loss': -0.022944462131420495, 'entropy_loss': -0.029985723085701466, 'vf_loss': 0.0005208280844856967, 'total_loss': -0.05240935713263626, 'approx_kl': -4.1831905939204717e-08, 'clip_fraction': 0.0, 'grad_norm': 13.481499671936035}
2022-12-29 19:47:08.092 DEBUG: Taking gradient step
2022-12-29 19:47:08.102 DEBUG: Loss 1: {'policy_loss': -0.027701229378945688, 'entropy_loss': -0.029016656801104546, 'vf_loss': 0.0005092758728228582, 'total_loss': -0.05620861030722738, 'approx_kl': 0.005776995385531336, 'clip_fraction': 0.07421875, 'grad_norm': 7.203414440155029}
2022-12-29 19:47:08.902 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:47:08.902 INFO: Optimization: policy loss=-0.028, vf loss=0.001, entropy loss=-0.029, total loss=-0.056, num steps=2
2022-12-29 19:47:08.903 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:47:10.351 INFO: Evaluation rollout: return=0.794 (0.0), episode length=5.0
2022-12-29 19:47:10.352 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:47:10.354 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-8856.model
2022-12-29 19:47:10.359 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-11016.model
2022-12-29 19:47:10.389 INFO: Iteration: 51/137, steps: 11016
2022-12-29 19:48:07.671 INFO: Training rollout: return=0.584 (0.1), episode length=5.0
2022-12-29 19:48:07.673 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:48:07.676 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-11016_train.pkl
2022-12-29 19:48:08.504 DEBUG: Taking gradient step
2022-12-29 19:48:08.513 DEBUG: Loss 0: {'policy_loss': 0.02347007747611013, 'entropy_loss': -0.03033257881179452, 'vf_loss': 0.0004582001005468475, 'total_loss': -0.0064043012351375425, 'approx_kl': 3.958120942115784e-09, 'clip_fraction': 0.0, 'grad_norm': 15.10326099395752}
2022-12-29 19:48:09.329 DEBUG: Taking gradient step
2022-12-29 19:48:09.339 DEBUG: Loss 1: {'policy_loss': -0.05009591112789197, 'entropy_loss': -0.03009301796555519, 'vf_loss': 0.0004726058779498699, 'total_loss': -0.07971632321549729, 'approx_kl': 0.00914851033303421, 'clip_fraction': 0.1705729179084301, 'grad_norm': 13.620316505432129}
2022-12-29 19:48:10.217 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:48:10.218 INFO: Optimization: policy loss=-0.050, vf loss=0.000, entropy loss=-0.030, total loss=-0.080, num steps=2
2022-12-29 19:48:10.218 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:48:11.712 INFO: Evaluation rollout: return=0.782 (0.0), episode length=5.0
2022-12-29 19:48:11.714 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:48:11.717 INFO: Iteration: 52/137, steps: 11232
2022-12-29 19:49:09.521 INFO: Training rollout: return=0.629 (0.1), episode length=5.0
2022-12-29 19:49:09.522 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:49:09.525 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-11232_train.pkl
2022-12-29 19:49:10.464 DEBUG: Taking gradient step
2022-12-29 19:49:10.474 DEBUG: Loss 0: {'policy_loss': -0.007895656548982504, 'entropy_loss': -0.02885160595178604, 'vf_loss': 0.0004950625833927869, 'total_loss': -0.03625219991737576, 'approx_kl': -6.752088665962219e-09, 'clip_fraction': 0.0, 'grad_norm': 5.6120991706848145}
2022-12-29 19:49:11.336 DEBUG: Taking gradient step
2022-12-29 19:49:11.345 DEBUG: Loss 1: {'policy_loss': 0.030758420944722517, 'entropy_loss': -0.03113666782155633, 'vf_loss': 0.00045007636196704053, 'total_loss': 7.182948513322768e-05, 'approx_kl': 0.0024276511976495385, 'clip_fraction': 0.06380208395421505, 'grad_norm': 6.2038350105285645}
2022-12-29 19:49:12.184 DEBUG: Taking gradient step
2022-12-29 19:49:12.193 DEBUG: Loss 2: {'policy_loss': 0.047851732151153153, 'entropy_loss': -0.028542361222207546, 'vf_loss': 0.0004247752366817118, 'total_loss': 0.019734146165627325, 'approx_kl': 0.0071097518084570765, 'clip_fraction': 0.1940104216337204, 'grad_norm': 12.414887428283691}
2022-12-29 19:49:13.012 DEBUG: Taking gradient step
2022-12-29 19:49:13.021 DEBUG: Loss 3: {'policy_loss': 0.027586563396291834, 'entropy_loss': -0.02937925886362791, 'vf_loss': 0.00040907266231920373, 'total_loss': -0.0013836228050168736, 'approx_kl': 0.017940220655873418, 'clip_fraction': 0.1419270858168602, 'grad_norm': 5.181386470794678}
2022-12-29 19:49:13.898 DEBUG: Taking gradient step
2022-12-29 19:49:13.912 DEBUG: Loss 4: {'policy_loss': -0.05797568757876831, 'entropy_loss': -0.02759108878672123, 'vf_loss': 0.00041000952287425396, 'total_loss': -0.08515676684261528, 'approx_kl': 0.01886624563485384, 'clip_fraction': 0.13541666697710752, 'grad_norm': 5.670774459838867}
2022-12-29 19:49:14.743 DEBUG: Early stopping at step 5 for reaching max KL.
2022-12-29 19:49:14.743 INFO: Optimization: policy loss=-0.058, vf loss=0.000, entropy loss=-0.028, total loss=-0.085, num steps=5
2022-12-29 19:49:14.744 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:49:16.196 INFO: Evaluation rollout: return=0.793 (0.0), episode length=5.0
2022-12-29 19:49:16.197 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:49:16.199 INFO: Iteration: 53/137, steps: 11448
2022-12-29 19:49:29.677 DEBUG: Atoms are too close
2022-12-29 19:50:02.681 DEBUG: Atoms are too close
2022-12-29 19:50:13.651 INFO: Training rollout: return=-0.490 (4.6), episode length=5.0
2022-12-29 19:50:13.653 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:50:13.655 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-11448_train.pkl
2022-12-29 19:50:14.498 DEBUG: Taking gradient step
2022-12-29 19:50:14.508 DEBUG: Loss 0: {'policy_loss': -0.004796635140644311, 'entropy_loss': -0.027753999456763268, 'vf_loss': 0.017735430586625273, 'total_loss': -0.014815204010782308, 'approx_kl': 5.675246939063072e-08, 'clip_fraction': 0.0, 'grad_norm': 17.787174224853516}
2022-12-29 19:50:15.323 DEBUG: Taking gradient step
2022-12-29 19:50:15.334 DEBUG: Loss 1: {'policy_loss': -0.013188449684141897, 'entropy_loss': -0.027341102249920368, 'vf_loss': 0.017622185450065712, 'total_loss': -0.022907366483996553, 'approx_kl': 0.011393609805963933, 'clip_fraction': 0.1419270858168602, 'grad_norm': 6.745476722717285}
2022-12-29 19:50:16.162 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:50:16.162 INFO: Optimization: policy loss=-0.013, vf loss=0.018, entropy loss=-0.027, total loss=-0.023, num steps=2
2022-12-29 19:50:16.163 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:50:17.600 INFO: Evaluation rollout: return=0.793 (0.0), episode length=5.0
2022-12-29 19:50:17.601 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:50:17.604 INFO: Iteration: 54/137, steps: 11664
2022-12-29 19:50:32.954 DEBUG: Atoms are too close
2022-12-29 19:51:15.655 INFO: Training rollout: return=0.092 (3.3), episode length=5.0
2022-12-29 19:51:15.657 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:51:15.659 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-11664_train.pkl
2022-12-29 19:51:16.440 DEBUG: Taking gradient step
2022-12-29 19:51:16.454 DEBUG: Loss 0: {'policy_loss': 0.019509351971306023, 'entropy_loss': -0.02839387720450759, 'vf_loss': 0.01031877212961793, 'total_loss': 0.0014342468964163683, 'approx_kl': -1.0388127336113939e-07, 'clip_fraction': 0.0, 'grad_norm': 23.76055335998535}
2022-12-29 19:51:17.257 DEBUG: Taking gradient step
2022-12-29 19:51:17.267 DEBUG: Loss 1: {'policy_loss': 0.016987588187153978, 'entropy_loss': -0.029235783498734236, 'vf_loss': 0.010294157855201078, 'total_loss': -0.0019540374563791763, 'approx_kl': 0.010038989596068859, 'clip_fraction': 0.03125, 'grad_norm': 23.15290069580078}
2022-12-29 19:51:18.047 DEBUG: Taking gradient step
2022-12-29 19:51:18.056 DEBUG: Loss 2: {'policy_loss': -0.03833898991324204, 'entropy_loss': -0.029044644441455603, 'vf_loss': 0.007758290866158112, 'total_loss': -0.05962534348853953, 'approx_kl': 0.03000306896865368, 'clip_fraction': 0.16015625, 'grad_norm': 6.051691055297852}
2022-12-29 19:51:18.890 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 19:51:18.890 INFO: Optimization: policy loss=-0.038, vf loss=0.008, entropy loss=-0.029, total loss=-0.060, num steps=3
2022-12-29 19:51:18.891 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:51:20.323 INFO: Evaluation rollout: return=0.799 (0.0), episode length=5.0
2022-12-29 19:51:20.325 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:51:20.327 INFO: Iteration: 55/137, steps: 11880
2022-12-29 19:52:16.998 INFO: Training rollout: return=0.606 (0.1), episode length=5.0
2022-12-29 19:52:17.000 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:52:17.002 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-11880_train.pkl
2022-12-29 19:52:17.883 DEBUG: Taking gradient step
2022-12-29 19:52:17.894 DEBUG: Loss 0: {'policy_loss': -0.012201197784292888, 'entropy_loss': -0.030751053243875504, 'vf_loss': 0.00027185025077170735, 'total_loss': -0.04268040077739669, 'approx_kl': -3.903793732717986e-08, 'clip_fraction': 0.0, 'grad_norm': 12.109703063964844}
2022-12-29 19:52:18.699 DEBUG: Taking gradient step
2022-12-29 19:52:18.709 DEBUG: Loss 1: {'policy_loss': -0.05459635626111907, 'entropy_loss': -0.03070456674322486, 'vf_loss': 0.00027293706258169043, 'total_loss': -0.08502798594176224, 'approx_kl': 0.03988072695210576, 'clip_fraction': 0.1549479179084301, 'grad_norm': 6.447160720825195}
2022-12-29 19:52:19.496 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:52:19.496 INFO: Optimization: policy loss=-0.055, vf loss=0.000, entropy loss=-0.031, total loss=-0.085, num steps=2
2022-12-29 19:52:19.497 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:52:20.938 INFO: Evaluation rollout: return=0.800 (0.0), episode length=5.0
2022-12-29 19:52:20.939 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:52:20.942 INFO: Iteration: 56/137, steps: 12096
2022-12-29 19:52:53.318 DEBUG: Atoms are too close
2022-12-29 19:53:18.521 INFO: Training rollout: return=0.006 (3.3), episode length=5.0
2022-12-29 19:53:18.522 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:53:18.525 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-12096_train.pkl
2022-12-29 19:53:19.376 DEBUG: Taking gradient step
2022-12-29 19:53:19.385 DEBUG: Loss 0: {'policy_loss': -0.02230724321923807, 'entropy_loss': -0.031334047205746174, 'vf_loss': 0.007893547659829997, 'total_loss': -0.04574774276515424, 'approx_kl': 5.5549513433561515e-08, 'clip_fraction': 0.0, 'grad_norm': 7.37266206741333}
2022-12-29 19:53:20.190 DEBUG: Taking gradient step
2022-12-29 19:53:20.200 DEBUG: Loss 1: {'policy_loss': -0.03444065548782968, 'entropy_loss': -0.031612145248800516, 'vf_loss': 0.00789086519772553, 'total_loss': -0.05816193553890466, 'approx_kl': 0.004292587749660015, 'clip_fraction': 0.1666666679084301, 'grad_norm': 7.206313133239746}
2022-12-29 19:53:21.007 DEBUG: Taking gradient step
2022-12-29 19:53:21.016 DEBUG: Loss 2: {'policy_loss': 0.03387834305165755, 'entropy_loss': -0.031180378049612045, 'vf_loss': 0.013046041688890713, 'total_loss': 0.015744006690936216, 'approx_kl': 0.018926964607089758, 'clip_fraction': 0.34375, 'grad_norm': 8.818634033203125}
2022-12-29 19:53:21.901 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 19:53:21.902 INFO: Optimization: policy loss=0.034, vf loss=0.013, entropy loss=-0.031, total loss=0.016, num steps=3
2022-12-29 19:53:21.902 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:53:23.260 INFO: Evaluation rollout: return=0.791 (0.0), episode length=5.0
2022-12-29 19:53:23.261 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:53:23.263 INFO: Iteration: 57/137, steps: 12312
2022-12-29 19:54:20.756 INFO: Training rollout: return=0.524 (0.2), episode length=5.0
2022-12-29 19:54:20.757 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:54:20.759 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-12312_train.pkl
2022-12-29 19:54:21.589 DEBUG: Taking gradient step
2022-12-29 19:54:21.598 DEBUG: Loss 0: {'policy_loss': 0.04513059889872539, 'entropy_loss': -0.031779509503394365, 'vf_loss': 0.0001954855809031476, 'total_loss': 0.013546574976234169, 'approx_kl': 7.935644497081284e-08, 'clip_fraction': 0.0, 'grad_norm': 12.79920482635498}
2022-12-29 19:54:22.430 DEBUG: Taking gradient step
2022-12-29 19:54:22.445 DEBUG: Loss 1: {'policy_loss': 0.003970732850536229, 'entropy_loss': -0.029437449295073748, 'vf_loss': 0.00020334892796462203, 'total_loss': -0.025263367516572897, 'approx_kl': 0.03249363647773862, 'clip_fraction': 0.1236979179084301, 'grad_norm': 11.643041610717773}
2022-12-29 19:54:23.259 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:54:23.259 INFO: Optimization: policy loss=0.004, vf loss=0.000, entropy loss=-0.029, total loss=-0.025, num steps=2
2022-12-29 19:54:23.259 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:54:24.754 INFO: Evaluation rollout: return=0.779 (0.0), episode length=5.0
2022-12-29 19:54:24.755 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:54:24.757 INFO: Iteration: 58/137, steps: 12528
2022-12-29 19:54:40.802 DEBUG: Atoms are too close
2022-12-29 19:55:10.094 DEBUG: Atoms are too close
2022-12-29 19:55:22.376 INFO: Training rollout: return=-0.578 (4.6), episode length=5.0
2022-12-29 19:55:22.378 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:55:22.380 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-12528_train.pkl
2022-12-29 19:55:23.207 DEBUG: Taking gradient step
2022-12-29 19:55:23.217 DEBUG: Loss 0: {'policy_loss': -0.03632698686662541, 'entropy_loss': -0.02996005956083536, 'vf_loss': 0.015259301042911858, 'total_loss': -0.051027745384548906, 'approx_kl': -5.475400666199448e-08, 'clip_fraction': 0.0, 'grad_norm': 16.898929595947266}
2022-12-29 19:55:24.104 DEBUG: Taking gradient step
2022-12-29 19:55:24.114 DEBUG: Loss 1: {'policy_loss': 0.05310037135980093, 'entropy_loss': -0.028386696707457304, 'vf_loss': 0.022926222624510988, 'total_loss': 0.04763989727685461, 'approx_kl': 0.027137373748701066, 'clip_fraction': 0.1510416679084301, 'grad_norm': 20.313507080078125}
2022-12-29 19:55:24.923 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:55:24.923 INFO: Optimization: policy loss=0.053, vf loss=0.023, entropy loss=-0.028, total loss=0.048, num steps=2
2022-12-29 19:55:24.923 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:55:26.347 INFO: Evaluation rollout: return=0.774 (0.0), episode length=5.0
2022-12-29 19:55:26.349 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:55:26.351 INFO: Iteration: 59/137, steps: 12744
2022-12-29 19:55:58.628 DEBUG: Atoms are too close
2022-12-29 19:56:13.152 DEBUG: Atoms are too close
2022-12-29 19:56:23.975 INFO: Training rollout: return=-0.560 (4.6), episode length=5.0
2022-12-29 19:56:23.976 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:56:23.979 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-12744_train.pkl
2022-12-29 19:56:24.781 DEBUG: Taking gradient step
2022-12-29 19:56:24.790 DEBUG: Loss 0: {'policy_loss': -0.03525473367269822, 'entropy_loss': -0.028562423307448626, 'vf_loss': 0.015479081096881987, 'total_loss': -0.04833807588326486, 'approx_kl': 9.794409194796572e-08, 'clip_fraction': 0.0, 'grad_norm': 12.128089904785156}
2022-12-29 19:56:25.596 DEBUG: Early stopping at step 1 for reaching max KL.
2022-12-29 19:56:25.596 INFO: Optimization: policy loss=-0.035, vf loss=0.015, entropy loss=-0.029, total loss=-0.048, num steps=1
2022-12-29 19:56:25.597 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:56:26.903 INFO: Evaluation rollout: return=0.776 (0.0), episode length=5.0
2022-12-29 19:56:26.904 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:56:26.906 INFO: Iteration: 60/137, steps: 12960
2022-12-29 19:56:38.375 DEBUG: Atoms are too close
2022-12-29 19:57:23.907 INFO: Training rollout: return=0.015 (3.3), episode length=5.0
2022-12-29 19:57:23.909 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:57:23.913 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-12960_train.pkl
2022-12-29 19:57:24.792 DEBUG: Taking gradient step
2022-12-29 19:57:24.806 DEBUG: Loss 0: {'policy_loss': -0.022485652105166936, 'entropy_loss': -0.028404213022440672, 'vf_loss': 0.006257800536202859, 'total_loss': -0.04463206459140475, 'approx_kl': -2.7939677238464355e-08, 'clip_fraction': 0.0, 'grad_norm': 6.676003456115723}
2022-12-29 19:57:25.650 DEBUG: Taking gradient step
2022-12-29 19:57:25.660 DEBUG: Loss 1: {'policy_loss': -0.03095615521837272, 'entropy_loss': -0.025799109134823084, 'vf_loss': 0.006258417600608144, 'total_loss': -0.05049684675258766, 'approx_kl': 0.013081125798635185, 'clip_fraction': 0.22265625, 'grad_norm': 4.146410942077637}
2022-12-29 19:57:26.546 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:57:26.546 INFO: Optimization: policy loss=-0.031, vf loss=0.006, entropy loss=-0.026, total loss=-0.050, num steps=2
2022-12-29 19:57:26.547 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:57:27.978 INFO: Evaluation rollout: return=0.777 (0.0), episode length=5.0
2022-12-29 19:57:27.979 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:57:27.981 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-11016.model
2022-12-29 19:57:28.014 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-13176.model
2022-12-29 19:57:28.049 INFO: Iteration: 61/137, steps: 13176
2022-12-29 19:57:54.369 DEBUG: Atoms are too close
2022-12-29 19:58:25.995 INFO: Training rollout: return=0.014 (3.3), episode length=5.0
2022-12-29 19:58:25.996 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:58:25.999 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-13176_train.pkl
2022-12-29 19:58:26.909 DEBUG: Taking gradient step
2022-12-29 19:58:26.923 DEBUG: Loss 0: {'policy_loss': -0.020044613372813304, 'entropy_loss': -0.024790297728031874, 'vf_loss': 0.006349739512606208, 'total_loss': -0.03848517158823897, 'approx_kl': -1.8345115648799037e-08, 'clip_fraction': 0.0, 'grad_norm': 13.077912330627441}
2022-12-29 19:58:27.737 DEBUG: Taking gradient step
2022-12-29 19:58:27.749 DEBUG: Loss 1: {'policy_loss': 0.018732616763932702, 'entropy_loss': -0.023672526702284813, 'vf_loss': 0.00882539994056768, 'total_loss': 0.0038854900022155595, 'approx_kl': 0.013018013560213149, 'clip_fraction': 0.2473958358168602, 'grad_norm': 1.7015233039855957}
2022-12-29 19:58:28.522 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 19:58:28.522 INFO: Optimization: policy loss=0.019, vf loss=0.009, entropy loss=-0.024, total loss=0.004, num steps=2
2022-12-29 19:58:28.523 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:58:30.066 INFO: Evaluation rollout: return=0.765 (0.0), episode length=5.0
2022-12-29 19:58:30.067 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:58:30.070 INFO: Iteration: 62/137, steps: 13392
2022-12-29 19:58:39.059 DEBUG: Atoms are too close
2022-12-29 19:58:52.606 DEBUG: Atoms are too close
2022-12-29 19:59:14.117 DEBUG: Atoms are too close
2022-12-29 19:59:27.408 INFO: Training rollout: return=-1.082 (5.6), episode length=4.9
2022-12-29 19:59:27.410 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 19:59:27.413 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-13392_train.pkl
2022-12-29 19:59:28.248 DEBUG: Taking gradient step
2022-12-29 19:59:28.257 DEBUG: Loss 0: {'policy_loss': 0.022756637418571354, 'entropy_loss': -0.02375122904777527, 'vf_loss': 0.02207907211974926, 'total_loss': 0.021084480490545343, 'approx_kl': -3.866929176865597e-08, 'clip_fraction': 0.0, 'grad_norm': 21.36749839782715}
2022-12-29 19:59:29.086 DEBUG: Taking gradient step
2022-12-29 19:59:29.101 DEBUG: Loss 1: {'policy_loss': -0.039469815234228496, 'entropy_loss': -0.024555787444114685, 'vf_loss': 0.017096810070252582, 'total_loss': -0.04692879260809059, 'approx_kl': 0.017349678790196776, 'clip_fraction': 0.12239583395421505, 'grad_norm': 6.774111270904541}
2022-12-29 19:59:30.017 DEBUG: Taking gradient step
2022-12-29 19:59:30.027 DEBUG: Loss 2: {'policy_loss': 0.06965881301324922, 'entropy_loss': -0.022412160877138376, 'vf_loss': 0.027424910407206897, 'total_loss': 0.07467156254331773, 'approx_kl': 0.029193888418376446, 'clip_fraction': 0.2447916716337204, 'grad_norm': 7.1695475578308105}
2022-12-29 19:59:30.846 DEBUG: Taking gradient step
2022-12-29 19:59:30.855 DEBUG: Loss 3: {'policy_loss': -0.015693102739284315, 'entropy_loss': -0.02290842030197382, 'vf_loss': 0.019653475137411083, 'total_loss': -0.018948047903847055, 'approx_kl': 0.03994204965420067, 'clip_fraction': 0.2955729216337204, 'grad_norm': 5.58592414855957}
2022-12-29 19:59:31.658 DEBUG: Taking gradient step
2022-12-29 19:59:31.668 DEBUG: Loss 4: {'policy_loss': 0.019936579376726646, 'entropy_loss': -0.02199911093339324, 'vf_loss': 0.022281587954799918, 'total_loss': 0.02021905639813332, 'approx_kl': 0.016837267205119133, 'clip_fraction': 0.2018229179084301, 'grad_norm': 3.9141573905944824}
2022-12-29 19:59:32.462 DEBUG: Taking gradient step
2022-12-29 19:59:32.472 DEBUG: Loss 5: {'policy_loss': -0.021072555044919037, 'entropy_loss': -0.02166929841041565, 'vf_loss': 0.019574655952901993, 'total_loss': -0.023167197502432693, 'approx_kl': 0.020970761077478528, 'clip_fraction': 0.2122395858168602, 'grad_norm': 3.0507779121398926}
2022-12-29 19:59:33.301 DEBUG: Taking gradient step
2022-12-29 19:59:33.317 DEBUG: Loss 6: {'policy_loss': -0.027000320531547887, 'entropy_loss': -0.022797031328082085, 'vf_loss': 0.019534101112262465, 'total_loss': -0.0302632507473675, 'approx_kl': 0.02574020065367222, 'clip_fraction': 0.2955729216337204, 'grad_norm': 4.011425971984863}
2022-12-29 19:59:34.234 DEBUG: Early stopping at step 7 for reaching max KL.
2022-12-29 19:59:34.234 INFO: Optimization: policy loss=-0.027, vf loss=0.020, entropy loss=-0.023, total loss=-0.030, num steps=7
2022-12-29 19:59:34.235 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 19:59:35.555 INFO: Evaluation rollout: return=0.778 (0.0), episode length=5.0
2022-12-29 19:59:35.556 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 19:59:35.559 INFO: Iteration: 63/137, steps: 13608
2022-12-29 20:00:20.943 DEBUG: Atoms are too close
2022-12-29 20:00:33.222 INFO: Training rollout: return=0.044 (3.3), episode length=5.0
2022-12-29 20:00:33.224 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:00:33.226 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-13608_train.pkl
2022-12-29 20:00:34.074 DEBUG: Taking gradient step
2022-12-29 20:00:34.088 DEBUG: Loss 0: {'policy_loss': 0.019397111115941935, 'entropy_loss': -0.02326571149751544, 'vf_loss': 0.010341320527996863, 'total_loss': 0.006472720146423354, 'approx_kl': 6.359186954796314e-08, 'clip_fraction': 0.0, 'grad_norm': 29.63785743713379}
2022-12-29 20:00:34.926 DEBUG: Taking gradient step
2022-12-29 20:00:34.940 DEBUG: Loss 1: {'policy_loss': -0.03670885229598868, 'entropy_loss': -0.02287590689957142, 'vf_loss': 0.007850953781987692, 'total_loss': -0.051733805413572406, 'approx_kl': 0.007521950639784336, 'clip_fraction': 0.12630208395421505, 'grad_norm': 1.726678729057312}
2022-12-29 20:00:35.810 DEBUG: Taking gradient step
2022-12-29 20:00:35.820 DEBUG: Loss 2: {'policy_loss': 0.00474041250947721, 'entropy_loss': -0.024437953252345324, 'vf_loss': 0.010369150986134468, 'total_loss': -0.009328389756733647, 'approx_kl': 0.031529605854302645, 'clip_fraction': 0.265625, 'grad_norm': 3.5855774879455566}
2022-12-29 20:00:36.623 DEBUG: Taking gradient step
2022-12-29 20:00:36.632 DEBUG: Loss 3: {'policy_loss': -0.0326139783194936, 'entropy_loss': -0.024072641506791115, 'vf_loss': 0.007864445952962944, 'total_loss': -0.04882217387332177, 'approx_kl': 0.03634563717059791, 'clip_fraction': 0.31640625, 'grad_norm': 3.609623432159424}
2022-12-29 20:00:37.434 DEBUG: Taking gradient step
2022-12-29 20:00:37.444 DEBUG: Loss 4: {'policy_loss': 0.0037993293834262115, 'entropy_loss': -0.02576090767979622, 'vf_loss': 0.010360119104449499, 'total_loss': -0.011601459191920505, 'approx_kl': 0.03610972594469786, 'clip_fraction': 0.2747395858168602, 'grad_norm': 4.227927207946777}
2022-12-29 20:00:38.258 DEBUG: Taking gradient step
2022-12-29 20:00:38.268 DEBUG: Loss 5: {'policy_loss': -0.034553559555617994, 'entropy_loss': -0.027915813494473696, 'vf_loss': 0.007891745425898211, 'total_loss': -0.05457762762419348, 'approx_kl': 0.04374916246160865, 'clip_fraction': 0.3190104216337204, 'grad_norm': 2.6923744678497314}
2022-12-29 20:00:39.134 DEBUG: Early stopping at step 6 for reaching max KL.
2022-12-29 20:00:39.134 INFO: Optimization: policy loss=-0.035, vf loss=0.008, entropy loss=-0.028, total loss=-0.055, num steps=6
2022-12-29 20:00:39.134 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:00:40.532 INFO: Evaluation rollout: return=0.787 (0.0), episode length=5.0
2022-12-29 20:00:40.533 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:00:40.536 INFO: Iteration: 64/137, steps: 13824
2022-12-29 20:01:06.331 DEBUG: Atoms are too close
2022-12-29 20:01:24.798 DEBUG: Atoms are too close
2022-12-29 20:01:38.156 INFO: Training rollout: return=-0.569 (4.6), episode length=4.9
2022-12-29 20:01:38.157 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:01:38.160 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-13824_train.pkl
2022-12-29 20:01:38.991 DEBUG: Taking gradient step
2022-12-29 20:01:39.002 DEBUG: Loss 0: {'policy_loss': -0.0005136740398200973, 'entropy_loss': -0.02664309274405241, 'vf_loss': 0.014830235990357689, 'total_loss': -0.012326530793514819, 'approx_kl': 7.520429790019989e-08, 'clip_fraction': 0.0, 'grad_norm': 8.512991905212402}
2022-12-29 20:01:39.830 DEBUG: Taking gradient step
2022-12-29 20:01:39.839 DEBUG: Loss 1: {'policy_loss': -0.003380206906452994, 'entropy_loss': -0.02860051952302456, 'vf_loss': 0.014833430846881828, 'total_loss': -0.01714729558259572, 'approx_kl': -0.0009840050097409403, 'clip_fraction': 0.03125, 'grad_norm': 7.304683685302734}
2022-12-29 20:01:40.727 DEBUG: Taking gradient step
2022-12-29 20:01:40.741 DEBUG: Loss 2: {'policy_loss': -0.002794749407938009, 'entropy_loss': -0.02878065360710025, 'vf_loss': 0.014829626894222611, 'total_loss': -0.01674577612081565, 'approx_kl': 0.00767003046348691, 'clip_fraction': 0.1575520858168602, 'grad_norm': 6.060319900512695}
2022-12-29 20:01:41.593 DEBUG: Taking gradient step
2022-12-29 20:01:41.604 DEBUG: Loss 3: {'policy_loss': -0.039665712931774934, 'entropy_loss': -0.029097843449562788, 'vf_loss': 0.012341211645041884, 'total_loss': -0.05642234473629584, 'approx_kl': 0.01451515732333064, 'clip_fraction': 0.1731770858168602, 'grad_norm': 4.07139253616333}
2022-12-29 20:01:42.502 DEBUG: Taking gradient step
2022-12-29 20:01:42.513 DEBUG: Loss 4: {'policy_loss': -0.038243124479787, 'entropy_loss': -0.02938229264691472, 'vf_loss': 0.012326076031269177, 'total_loss': -0.05529934109543254, 'approx_kl': 0.02108971239067614, 'clip_fraction': 0.07291666697710752, 'grad_norm': 4.772128105163574}
2022-12-29 20:01:43.280 DEBUG: Early stopping at step 5 for reaching max KL.
2022-12-29 20:01:43.282 INFO: Optimization: policy loss=-0.038, vf loss=0.012, entropy loss=-0.029, total loss=-0.055, num steps=5
2022-12-29 20:01:43.283 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:01:44.849 INFO: Evaluation rollout: return=0.784 (0.0), episode length=5.0
2022-12-29 20:01:44.850 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:01:44.853 INFO: Iteration: 65/137, steps: 14040
2022-12-29 20:02:11.023 DEBUG: Atoms are too close
2022-12-29 20:02:43.024 INFO: Training rollout: return=-0.012 (3.3), episode length=5.0
2022-12-29 20:02:43.026 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:02:43.029 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-14040_train.pkl
2022-12-29 20:02:43.845 DEBUG: Taking gradient step
2022-12-29 20:02:43.856 DEBUG: Loss 0: {'policy_loss': 0.027699700909476363, 'entropy_loss': -0.030319889076054096, 'vf_loss': 0.008895697313326414, 'total_loss': 0.006275509146748681, 'approx_kl': -3.589470054521371e-09, 'clip_fraction': 0.0, 'grad_norm': 8.694458961486816}
2022-12-29 20:02:44.664 DEBUG: Taking gradient step
2022-12-29 20:02:44.674 DEBUG: Loss 1: {'policy_loss': 0.01638614029221083, 'entropy_loss': -0.030373866204172373, 'vf_loss': 0.008873407196475825, 'total_loss': -0.005114318715485726, 'approx_kl': 0.011405094846850261, 'clip_fraction': 0.046875, 'grad_norm': 4.058241844177246}
2022-12-29 20:02:45.527 DEBUG: Taking gradient step
2022-12-29 20:02:45.540 DEBUG: Loss 2: {'policy_loss': -0.029897022075239286, 'entropy_loss': -0.030777353793382645, 'vf_loss': 0.0063728844771424676, 'total_loss': -0.05430149139147946, 'approx_kl': 0.010211698710918427, 'clip_fraction': 0.1966145858168602, 'grad_norm': 1.3200337886810303}
2022-12-29 20:02:46.379 DEBUG: Taking gradient step
2022-12-29 20:02:46.388 DEBUG: Loss 3: {'policy_loss': -0.035331119822568195, 'entropy_loss': -0.03152162302285433, 'vf_loss': 0.006374625500079762, 'total_loss': -0.060478117345342766, 'approx_kl': 0.005251495400443673, 'clip_fraction': 0.3567708358168602, 'grad_norm': 1.6407753229141235}
2022-12-29 20:02:47.184 DEBUG: Taking gradient step
2022-12-29 20:02:47.194 DEBUG: Loss 4: {'policy_loss': 0.012222572435440036, 'entropy_loss': -0.03155548218637705, 'vf_loss': 0.008853870442472132, 'total_loss': -0.010479039308464881, 'approx_kl': 0.02113404613919556, 'clip_fraction': 0.4296875, 'grad_norm': 7.646482467651367}
2022-12-29 20:02:48.036 DEBUG: Taking gradient step
2022-12-29 20:02:48.045 DEBUG: Loss 5: {'policy_loss': -0.03454874259634021, 'entropy_loss': -0.032282507978379726, 'vf_loss': 0.006368776898641179, 'total_loss': -0.06046247367607875, 'approx_kl': 0.041004644706845284, 'clip_fraction': 0.4778645858168602, 'grad_norm': 1.9412542581558228}
2022-12-29 20:02:48.867 DEBUG: Taking gradient step
2022-12-29 20:02:48.877 DEBUG: Loss 6: {'policy_loss': -0.035071988012261114, 'entropy_loss': -0.0337065071798861, 'vf_loss': 0.006365285198698834, 'total_loss': -0.062413209993448376, 'approx_kl': 0.027294994040858, 'clip_fraction': 0.5078125, 'grad_norm': 1.5779776573181152}
2022-12-29 20:02:49.777 DEBUG: Taking gradient step
2022-12-29 20:02:49.786 DEBUG: Loss 7: {'policy_loss': -0.0364548408468912, 'entropy_loss': -0.035231141839176416, 'vf_loss': 0.006352960148414074, 'total_loss': -0.06533302253765354, 'approx_kl': -0.006930834148079157, 'clip_fraction': 0.5260416716337204, 'grad_norm': 1.8613916635513306}
2022-12-29 20:02:50.593 DEBUG: Taking gradient step
2022-12-29 20:02:50.604 DEBUG: Loss 8: {'policy_loss': -0.03146627959493556, 'entropy_loss': -0.035273490473628044, 'vf_loss': 0.00634273085619465, 'total_loss': -0.060397039212368955, 'approx_kl': -0.014571520499885082, 'clip_fraction': 0.4973958358168602, 'grad_norm': 8.640911102294922}
2022-12-29 20:02:51.442 DEBUG: Taking gradient step
2022-12-29 20:02:51.451 DEBUG: Loss 9: {'policy_loss': -0.03844009826555645, 'entropy_loss': -0.033968355506658554, 'vf_loss': 0.006389000788978968, 'total_loss': -0.06601945298323604, 'approx_kl': 0.013529651798307896, 'clip_fraction': 0.484375, 'grad_norm': 10.091713905334473}
2022-12-29 20:02:51.452 INFO: Optimization: policy loss=-0.038, vf loss=0.006, entropy loss=-0.034, total loss=-0.066, num steps=10
2022-12-29 20:02:51.452 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:02:52.763 INFO: Evaluation rollout: return=0.785 (0.0), episode length=5.0
2022-12-29 20:02:52.764 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:02:52.767 INFO: Iteration: 66/137, steps: 14256
2022-12-29 20:03:50.936 INFO: Training rollout: return=0.510 (0.2), episode length=5.0
2022-12-29 20:03:50.937 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:03:50.939 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-14256_train.pkl
2022-12-29 20:03:51.774 DEBUG: Taking gradient step
2022-12-29 20:03:51.783 DEBUG: Loss 0: {'policy_loss': 0.07862465173404642, 'entropy_loss': -0.03718710131943226, 'vf_loss': 0.0002877817936488143, 'total_loss': 0.04172533220826298, 'approx_kl': -4.7012388648681736e-08, 'clip_fraction': 0.0, 'grad_norm': 16.460039138793945}
2022-12-29 20:03:52.618 DEBUG: Taking gradient step
2022-12-29 20:03:52.630 DEBUG: Loss 1: {'policy_loss': -0.022996665456341443, 'entropy_loss': -0.03576524741947651, 'vf_loss': 0.0002889934041175032, 'total_loss': -0.05847291947170044, 'approx_kl': 0.009983681957237422, 'clip_fraction': 0.078125, 'grad_norm': 9.846418380737305}
2022-12-29 20:03:53.456 DEBUG: Taking gradient step
2022-12-29 20:03:53.470 DEBUG: Loss 2: {'policy_loss': -0.0192393152199103, 'entropy_loss': -0.03560895752161741, 'vf_loss': 0.00026548924792266665, 'total_loss': -0.05458278349360505, 'approx_kl': 0.0432894176337868, 'clip_fraction': 0.2643229216337204, 'grad_norm': 6.875475883483887}
2022-12-29 20:03:54.314 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 20:03:54.314 INFO: Optimization: policy loss=-0.019, vf loss=0.000, entropy loss=-0.036, total loss=-0.055, num steps=3
2022-12-29 20:03:54.315 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:03:55.795 INFO: Evaluation rollout: return=0.786 (0.0), episode length=5.0
2022-12-29 20:03:55.796 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:03:55.798 INFO: Iteration: 67/137, steps: 14472
2022-12-29 20:04:24.546 DEBUG: Atoms are too close
2022-12-29 20:04:53.499 INFO: Training rollout: return=-0.002 (3.3), episode length=5.0
2022-12-29 20:04:53.501 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:04:53.503 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-14472_train.pkl
2022-12-29 20:04:54.372 DEBUG: Taking gradient step
2022-12-29 20:04:54.381 DEBUG: Loss 0: {'policy_loss': -0.028979582048298116, 'entropy_loss': -0.03449856769293547, 'vf_loss': 0.007950710682620666, 'total_loss': -0.05552743905861292, 'approx_kl': 5.1028717251710987e-08, 'clip_fraction': 0.0, 'grad_norm': 6.966517448425293}
2022-12-29 20:04:55.187 DEBUG: Taking gradient step
2022-12-29 20:04:55.198 DEBUG: Loss 1: {'policy_loss': 0.0030937460762379618, 'entropy_loss': -0.03490308299660683, 'vf_loss': 0.010502625863036467, 'total_loss': -0.021306711057332395, 'approx_kl': 0.04221425158903003, 'clip_fraction': 0.1484375, 'grad_norm': 8.259063720703125}
2022-12-29 20:04:56.056 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 20:04:56.056 INFO: Optimization: policy loss=0.003, vf loss=0.011, entropy loss=-0.035, total loss=-0.021, num steps=2
2022-12-29 20:04:56.056 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:04:57.502 INFO: Evaluation rollout: return=0.783 (0.0), episode length=5.0
2022-12-29 20:04:57.503 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:04:57.506 INFO: Iteration: 68/137, steps: 14688
2022-12-29 20:05:06.841 DEBUG: Atoms are too close
2022-12-29 20:05:10.713 DEBUG: Atoms are too close
2022-12-29 20:05:39.520 DEBUG: Atoms are too close
2022-12-29 20:05:54.312 INFO: Training rollout: return=-1.191 (5.6), episode length=4.9
2022-12-29 20:05:54.313 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:05:54.316 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-14688_train.pkl
2022-12-29 20:05:55.147 DEBUG: Taking gradient step
2022-12-29 20:05:55.157 DEBUG: Loss 0: {'policy_loss': 0.014692890962604125, 'entropy_loss': -0.03265899047255516, 'vf_loss': 0.02513404607915689, 'total_loss': 0.007167946569205844, 'approx_kl': -1.098184565506699e-08, 'clip_fraction': 0.0, 'grad_norm': 14.406147003173828}
2022-12-29 20:05:55.974 DEBUG: Early stopping at step 1 for reaching max KL.
2022-12-29 20:05:55.975 INFO: Optimization: policy loss=0.015, vf loss=0.025, entropy loss=-0.033, total loss=0.007, num steps=1
2022-12-29 20:05:55.975 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:05:57.390 INFO: Evaluation rollout: return=0.782 (0.0), episode length=5.0
2022-12-29 20:05:57.391 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:05:57.394 INFO: Iteration: 69/137, steps: 14904
2022-12-29 20:06:41.198 DEBUG: Atoms are too close
2022-12-29 20:06:54.443 INFO: Training rollout: return=0.014 (3.3), episode length=5.0
2022-12-29 20:06:54.444 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:06:54.447 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-14904_train.pkl
2022-12-29 20:06:55.303 DEBUG: Taking gradient step
2022-12-29 20:06:55.314 DEBUG: Loss 0: {'policy_loss': 0.025579318012937256, 'entropy_loss': -0.03155024582520127, 'vf_loss': 0.008882299103643247, 'total_loss': 0.0029113712913792222, 'approx_kl': 6.713284150805521e-09, 'clip_fraction': 0.0, 'grad_norm': 11.872392654418945}
2022-12-29 20:06:56.173 DEBUG: Taking gradient step
2022-12-29 20:06:56.187 DEBUG: Loss 1: {'policy_loss': -0.028152393295601527, 'entropy_loss': -0.030820649582892656, 'vf_loss': 0.006328432852635591, 'total_loss': -0.0526446100258586, 'approx_kl': 0.008361662388779223, 'clip_fraction': 0.09375, 'grad_norm': 6.4857683181762695}
2022-12-29 20:06:57.037 DEBUG: Taking gradient step
2022-12-29 20:06:57.046 DEBUG: Loss 2: {'policy_loss': 0.01467321888274439, 'entropy_loss': -0.028700869996100664, 'vf_loss': 0.008837308039318426, 'total_loss': -0.005190343074037845, 'approx_kl': 0.023384870728477836, 'clip_fraction': 0.17578125, 'grad_norm': 1.6094621419906616}
2022-12-29 20:06:57.872 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 20:06:57.873 INFO: Optimization: policy loss=0.015, vf loss=0.009, entropy loss=-0.029, total loss=-0.005, num steps=3
2022-12-29 20:06:57.873 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:06:59.318 INFO: Evaluation rollout: return=0.787 (0.0), episode length=5.0
2022-12-29 20:06:59.319 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:06:59.323 INFO: Iteration: 70/137, steps: 15120
2022-12-29 20:07:56.799 INFO: Training rollout: return=0.662 (0.1), episode length=5.0
2022-12-29 20:07:56.801 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:07:56.803 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-15120_train.pkl
2022-12-29 20:07:57.622 DEBUG: Taking gradient step
2022-12-29 20:07:57.632 DEBUG: Loss 0: {'policy_loss': 0.06416297888661805, 'entropy_loss': -0.028464872855693102, 'vf_loss': 0.0003232171951394261, 'total_loss': 0.036021323226064376, 'approx_kl': 1.0384246706962585e-07, 'clip_fraction': 0.0, 'grad_norm': 12.0358304977417}
2022-12-29 20:07:58.446 DEBUG: Taking gradient step
2022-12-29 20:07:58.455 DEBUG: Loss 1: {'policy_loss': -0.02103204590944922, 'entropy_loss': -0.0257750921882689, 'vf_loss': 0.0003347201571602897, 'total_loss': -0.04647241794055783, 'approx_kl': 0.0169427078217268, 'clip_fraction': 0.1276041679084301, 'grad_norm': 8.72706413269043}
2022-12-29 20:07:59.337 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 20:07:59.337 INFO: Optimization: policy loss=-0.021, vf loss=0.000, entropy loss=-0.026, total loss=-0.046, num steps=2
2022-12-29 20:07:59.338 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:08:00.764 INFO: Evaluation rollout: return=0.796 (0.0), episode length=5.0
2022-12-29 20:08:00.765 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:08:00.768 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-13176.model
2022-12-29 20:08:00.774 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-15336.model
2022-12-29 20:08:00.803 INFO: Iteration: 71/137, steps: 15336
2022-12-29 20:08:58.615 INFO: Training rollout: return=0.614 (0.1), episode length=5.0
2022-12-29 20:08:58.617 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:08:58.620 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-15336_train.pkl
2022-12-29 20:08:59.460 DEBUG: Taking gradient step
2022-12-29 20:08:59.469 DEBUG: Loss 0: {'policy_loss': 0.01731725534728507, 'entropy_loss': -0.024355475790798664, 'vf_loss': 0.00029191499801666106, 'total_loss': -0.006746305445496939, 'approx_kl': 7.2953589835833554e-09, 'clip_fraction': 0.0, 'grad_norm': 10.05988883972168}
2022-12-29 20:09:00.274 DEBUG: Taking gradient step
2022-12-29 20:09:00.283 DEBUG: Loss 1: {'policy_loss': 0.008340614922137314, 'entropy_loss': -0.023211441934108734, 'vf_loss': 0.00027989695791483547, 'total_loss': -0.014590930054056584, 'approx_kl': 0.03069808939471841, 'clip_fraction': 0.2395833358168602, 'grad_norm': 20.65736961364746}
2022-12-29 20:09:01.108 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 20:09:01.109 INFO: Optimization: policy loss=0.008, vf loss=0.000, entropy loss=-0.023, total loss=-0.015, num steps=2
2022-12-29 20:09:01.109 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:09:02.565 INFO: Evaluation rollout: return=0.796 (0.0), episode length=5.0
2022-12-29 20:09:02.567 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:09:02.570 INFO: Iteration: 72/137, steps: 15552
2022-12-29 20:10:00.799 INFO: Training rollout: return=0.613 (0.1), episode length=5.0
2022-12-29 20:10:00.801 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:10:00.804 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-15552_train.pkl
2022-12-29 20:10:01.663 DEBUG: Taking gradient step
2022-12-29 20:10:01.673 DEBUG: Loss 0: {'policy_loss': 0.025163982933581644, 'entropy_loss': -0.0227911788970232, 'vf_loss': 0.00026581592629704924, 'total_loss': 0.0026386199628554985, 'approx_kl': 6.111804395914078e-09, 'clip_fraction': 0.0, 'grad_norm': 19.026142120361328}
2022-12-29 20:10:02.588 DEBUG: Taking gradient step
2022-12-29 20:10:02.602 DEBUG: Loss 1: {'policy_loss': -0.015433329309603911, 'entropy_loss': -0.020964570343494415, 'vf_loss': 0.0002563313379907335, 'total_loss': -0.0361415683151076, 'approx_kl': 0.03628662880510092, 'clip_fraction': 0.1953125, 'grad_norm': 10.799972534179688}
2022-12-29 20:10:03.428 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 20:10:03.428 INFO: Optimization: policy loss=-0.015, vf loss=0.000, entropy loss=-0.021, total loss=-0.036, num steps=2
2022-12-29 20:10:03.429 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:10:04.921 INFO: Evaluation rollout: return=0.787 (0.0), episode length=5.0
2022-12-29 20:10:04.922 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:10:04.925 INFO: Iteration: 73/137, steps: 15768
2022-12-29 20:11:02.528 INFO: Training rollout: return=0.670 (0.1), episode length=5.0
2022-12-29 20:11:02.529 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:11:02.531 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-15768_train.pkl
2022-12-29 20:11:03.353 DEBUG: Taking gradient step
2022-12-29 20:11:03.362 DEBUG: Loss 0: {'policy_loss': 0.03153308387733061, 'entropy_loss': -0.020798746030777693, 'vf_loss': 0.0002746531488496492, 'total_loss': 0.011008990995402562, 'approx_kl': -3.012246452271938e-08, 'clip_fraction': 0.0, 'grad_norm': 15.581098556518555}
2022-12-29 20:11:04.171 DEBUG: Taking gradient step
2022-12-29 20:11:04.181 DEBUG: Loss 1: {'policy_loss': -0.038272837595548184, 'entropy_loss': -0.019913630094379187, 'vf_loss': 0.00027049077924612056, 'total_loss': -0.057915976910681254, 'approx_kl': 0.01568856416270137, 'clip_fraction': 0.09895833395421505, 'grad_norm': 10.158109664916992}
2022-12-29 20:11:04.991 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 20:11:04.991 INFO: Optimization: policy loss=-0.038, vf loss=0.000, entropy loss=-0.020, total loss=-0.058, num steps=2
2022-12-29 20:11:04.992 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:11:06.383 INFO: Evaluation rollout: return=0.777 (0.0), episode length=5.0
2022-12-29 20:11:06.384 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:11:06.387 INFO: Iteration: 74/137, steps: 15984
2022-12-29 20:12:03.755 INFO: Training rollout: return=0.668 (0.1), episode length=5.0
2022-12-29 20:12:03.756 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:12:03.759 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-15984_train.pkl
2022-12-29 20:12:04.566 DEBUG: Taking gradient step
2022-12-29 20:12:04.577 DEBUG: Loss 0: {'policy_loss': -0.03998744832509036, 'entropy_loss': -0.018711745738983154, 'vf_loss': 0.0002606092088854836, 'total_loss': -0.05843858485518803, 'approx_kl': 5.327941199340103e-08, 'clip_fraction': 0.0, 'grad_norm': 13.721860885620117}
2022-12-29 20:12:05.392 DEBUG: Taking gradient step
2022-12-29 20:12:05.402 DEBUG: Loss 1: {'policy_loss': -0.017597188827070287, 'entropy_loss': -0.01897812867537141, 'vf_loss': 0.0002412297449049574, 'total_loss': -0.03633408775753674, 'approx_kl': 0.024252122617326677, 'clip_fraction': 0.234375, 'grad_norm': 8.996575355529785}
2022-12-29 20:12:06.198 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 20:12:06.198 INFO: Optimization: policy loss=-0.018, vf loss=0.000, entropy loss=-0.019, total loss=-0.036, num steps=2
2022-12-29 20:12:06.199 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:12:07.617 INFO: Evaluation rollout: return=0.759 (0.0), episode length=5.0
2022-12-29 20:12:07.618 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:12:07.621 INFO: Iteration: 75/137, steps: 16200
2022-12-29 20:13:06.294 INFO: Training rollout: return=0.662 (0.1), episode length=5.0
2022-12-29 20:13:06.296 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:13:06.299 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-16200_train.pkl
2022-12-29 20:13:07.140 DEBUG: Taking gradient step
2022-12-29 20:13:07.150 DEBUG: Loss 0: {'policy_loss': -0.036869709658048755, 'entropy_loss': -0.01816896442323923, 'vf_loss': 0.0002447802330154882, 'total_loss': -0.05479389384827249, 'approx_kl': 1.9577178989038657e-08, 'clip_fraction': 0.0, 'grad_norm': 7.102445125579834}
2022-12-29 20:13:07.955 DEBUG: Taking gradient step
2022-12-29 20:13:07.965 DEBUG: Loss 1: {'policy_loss': 0.020979266632986396, 'entropy_loss': -0.017955733463168144, 'vf_loss': 0.00022143641808101662, 'total_loss': 0.003244969587899261, 'approx_kl': 0.005808206740766764, 'clip_fraction': 0.09375, 'grad_norm': 16.4207820892334}
2022-12-29 20:13:08.792 DEBUG: Taking gradient step
2022-12-29 20:13:08.801 DEBUG: Loss 2: {'policy_loss': -0.02787132495825858, 'entropy_loss': -0.0170390154235065, 'vf_loss': 0.0002252781690056988, 'total_loss': -0.04468506221275938, 'approx_kl': 0.029868825571611524, 'clip_fraction': 0.2591145858168602, 'grad_norm': 19.795774459838867}
2022-12-29 20:13:09.678 DEBUG: Taking gradient step
2022-12-29 20:13:09.687 DEBUG: Loss 3: {'policy_loss': -0.017938918066890838, 'entropy_loss': -0.01749839447438717, 'vf_loss': 0.000212562915346963, 'total_loss': -0.03522474962593104, 'approx_kl': 0.044107158901169896, 'clip_fraction': 0.2825520858168602, 'grad_norm': 17.88668441772461}
2022-12-29 20:13:10.497 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 20:13:10.497 INFO: Optimization: policy loss=-0.018, vf loss=0.000, entropy loss=-0.017, total loss=-0.035, num steps=4
2022-12-29 20:13:10.498 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:13:11.854 INFO: Evaluation rollout: return=0.748 (0.0), episode length=5.0
2022-12-29 20:13:11.855 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:13:11.858 INFO: Iteration: 76/137, steps: 16416
2022-12-29 20:13:37.522 DEBUG: There is a single atom floating around
2022-12-29 20:13:38.085 DEBUG: There is a single atom floating around
2022-12-29 20:13:56.876 DEBUG: Atoms are too close
2022-12-29 20:14:08.722 INFO: Training rollout: return=-1.017 (5.6), episode length=4.9
2022-12-29 20:14:08.724 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:14:08.726 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-16416_train.pkl
2022-12-29 20:14:09.625 DEBUG: Taking gradient step
2022-12-29 20:14:09.634 DEBUG: Loss 0: {'policy_loss': -0.0405645223117504, 'entropy_loss': -0.017122598364949226, 'vf_loss': 0.020270781843036458, 'total_loss': -0.03741633883366317, 'approx_kl': 5.9333011392936896e-08, 'clip_fraction': 0.0, 'grad_norm': 9.397316932678223}
2022-12-29 20:14:10.522 DEBUG: Taking gradient step
2022-12-29 20:14:10.531 DEBUG: Loss 1: {'policy_loss': -0.017050159648927584, 'entropy_loss': -0.016791151138022542, 'vf_loss': 0.022804526675768287, 'total_loss': -0.011036784111181842, 'approx_kl': 0.008343523601070046, 'clip_fraction': 0.033854166977107525, 'grad_norm': 8.698507308959961}
2022-12-29 20:14:11.357 DEBUG: Taking gradient step
2022-12-29 20:14:11.366 DEBUG: Loss 2: {'policy_loss': 0.03517892598931567, 'entropy_loss': -0.01659844210371375, 'vf_loss': 0.027945968169380567, 'total_loss': 0.04652645205498248, 'approx_kl': 0.02266967497416772, 'clip_fraction': 0.1666666679084301, 'grad_norm': 7.896734714508057}
2022-12-29 20:14:12.196 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 20:14:12.196 INFO: Optimization: policy loss=0.035, vf loss=0.028, entropy loss=-0.017, total loss=0.047, num steps=3
2022-12-29 20:14:12.197 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:14:13.639 INFO: Evaluation rollout: return=0.732 (0.0), episode length=5.0
2022-12-29 20:14:13.640 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:14:13.642 INFO: Iteration: 77/137, steps: 16632
2022-12-29 20:15:01.434 DEBUG: Atoms are too close
2022-12-29 20:15:11.460 INFO: Training rollout: return=0.107 (3.3), episode length=5.0
2022-12-29 20:15:11.461 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:15:11.464 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-16632_train.pkl
2022-12-29 20:15:12.277 DEBUG: Taking gradient step
2022-12-29 20:15:12.286 DEBUG: Loss 0: {'policy_loss': -0.020210987778813926, 'entropy_loss': -0.016293804859742522, 'vf_loss': 0.007848243880573596, 'total_loss': -0.02865654875798285, 'approx_kl': 7.256555001333709e-08, 'clip_fraction': 0.0, 'grad_norm': 5.737361431121826}
2022-12-29 20:15:13.093 DEBUG: Taking gradient step
2022-12-29 20:15:13.103 DEBUG: Loss 1: {'policy_loss': -0.029101951472792556, 'entropy_loss': -0.01593470457009971, 'vf_loss': 0.00783284570618018, 'total_loss': -0.037203810336712086, 'approx_kl': 0.01697155530564487, 'clip_fraction': 0.1158854179084301, 'grad_norm': 4.756685256958008}
2022-12-29 20:15:13.902 DEBUG: Taking gradient step
2022-12-29 20:15:13.911 DEBUG: Loss 2: {'policy_loss': -0.040549170462729094, 'entropy_loss': -0.015029451111331582, 'vf_loss': 0.007829015945120397, 'total_loss': -0.047749605628940275, 'approx_kl': 0.04489268735051155, 'clip_fraction': 0.33203125, 'grad_norm': 2.6205241680145264}
2022-12-29 20:15:14.714 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 20:15:14.714 INFO: Optimization: policy loss=-0.041, vf loss=0.008, entropy loss=-0.015, total loss=-0.048, num steps=3
2022-12-29 20:15:14.715 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:15:16.207 INFO: Evaluation rollout: return=0.688 (0.0), episode length=5.0
2022-12-29 20:15:16.208 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:15:16.210 INFO: Iteration: 78/137, steps: 16848
2022-12-29 20:15:29.513 DEBUG: Atoms are too close
2022-12-29 20:15:45.143 DEBUG: Atoms are too close
2022-12-29 20:15:46.933 DEBUG: Atoms are too close
2022-12-29 20:15:46.934 DEBUG: Atoms are too close
2022-12-29 20:16:00.337 DEBUG: Atoms are too close
2022-12-29 20:16:12.699 INFO: Training rollout: return=-2.158 (6.9), episode length=5.0
2022-12-29 20:16:12.700 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:16:12.702 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-16848_train.pkl
2022-12-29 20:16:13.529 DEBUG: Taking gradient step
2022-12-29 20:16:13.538 DEBUG: Loss 0: {'policy_loss': -0.014285712905252552, 'entropy_loss': -0.01592804561369121, 'vf_loss': 0.04331880793221808, 'total_loss': 0.013105049413274318, 'approx_kl': 2.382633601527573e-08, 'clip_fraction': 0.0, 'grad_norm': 14.117608070373535}
2022-12-29 20:16:14.419 DEBUG: Taking gradient step
2022-12-29 20:16:14.430 DEBUG: Loss 1: {'policy_loss': -0.023152603279331534, 'entropy_loss': -0.015185564756393433, 'vf_loss': 0.045704353746801214, 'total_loss': 0.007366185711076251, 'approx_kl': 0.009306526568252593, 'clip_fraction': 0.2877604216337204, 'grad_norm': 4.330095291137695}
2022-12-29 20:16:15.226 DEBUG: Taking gradient step
2022-12-29 20:16:15.236 DEBUG: Loss 2: {'policy_loss': -0.04164973279486618, 'entropy_loss': -0.015957862371578813, 'vf_loss': 0.04288750360186835, 'total_loss': -0.014720091564576647, 'approx_kl': 0.035558207891881466, 'clip_fraction': 0.41015625, 'grad_norm': 6.427465915679932}
2022-12-29 20:16:16.036 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 20:16:16.036 INFO: Optimization: policy loss=-0.042, vf loss=0.043, entropy loss=-0.016, total loss=-0.015, num steps=3
2022-12-29 20:16:16.037 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:16:17.444 INFO: Evaluation rollout: return=0.651 (0.0), episode length=5.0
2022-12-29 20:16:17.445 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:16:17.448 INFO: Iteration: 79/137, steps: 17064
2022-12-29 20:16:32.977 DEBUG: Atoms are too close
2022-12-29 20:16:46.121 DEBUG: Atoms are too close
2022-12-29 20:16:46.397 DEBUG: Atoms are too close
2022-12-29 20:17:14.304 INFO: Training rollout: return=-1.050 (5.6), episode length=5.0
2022-12-29 20:17:14.306 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:17:14.308 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-17064_train.pkl
2022-12-29 20:17:15.220 DEBUG: Taking gradient step
2022-12-29 20:17:15.230 DEBUG: Loss 0: {'policy_loss': -0.04487352081995773, 'entropy_loss': -0.016607115976512432, 'vf_loss': 0.022636682585523853, 'total_loss': -0.038843954210946316, 'approx_kl': 3.7796176854953956e-08, 'clip_fraction': 0.0, 'grad_norm': 10.064959526062012}
2022-12-29 20:17:16.033 DEBUG: Taking gradient step
2022-12-29 20:17:16.043 DEBUG: Loss 1: {'policy_loss': -0.04948922838151186, 'entropy_loss': -0.01706811017356813, 'vf_loss': 0.022519784871813525, 'total_loss': -0.044037553683266464, 'approx_kl': 0.0007329131476581097, 'clip_fraction': 0.01953125, 'grad_norm': 7.6423821449279785}
2022-12-29 20:17:16.850 DEBUG: Taking gradient step
2022-12-29 20:17:16.865 DEBUG: Loss 2: {'policy_loss': -0.0544077755105208, 'entropy_loss': -0.016389584401622415, 'vf_loss': 0.022426344164310587, 'total_loss': -0.04837101574783262, 'approx_kl': 0.0026341788470745087, 'clip_fraction': 0.05598958395421505, 'grad_norm': 6.049891471862793}
2022-12-29 20:17:17.684 DEBUG: Taking gradient step
2022-12-29 20:17:17.695 DEBUG: Loss 3: {'policy_loss': -0.038092452194161956, 'entropy_loss': -0.01745639112778008, 'vf_loss': 0.024826484015944818, 'total_loss': -0.030722359305997222, 'approx_kl': 0.027548715006560087, 'clip_fraction': 0.30859375, 'grad_norm': 7.146559238433838}
2022-12-29 20:17:18.502 DEBUG: Taking gradient step
2022-12-29 20:17:18.513 DEBUG: Loss 4: {'policy_loss': 0.011389221874577589, 'entropy_loss': -0.018464035354554653, 'vf_loss': 0.029609573460610036, 'total_loss': 0.022534759980632972, 'approx_kl': 0.02818747889250517, 'clip_fraction': 0.3606770858168602, 'grad_norm': 3.502364158630371}
2022-12-29 20:17:19.365 DEBUG: Early stopping at step 5 for reaching max KL.
2022-12-29 20:17:19.366 INFO: Optimization: policy loss=0.011, vf loss=0.030, entropy loss=-0.018, total loss=0.023, num steps=5
2022-12-29 20:17:19.366 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:17:20.743 INFO: Evaluation rollout: return=0.687 (0.0), episode length=5.0
2022-12-29 20:17:20.745 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:17:20.748 INFO: Iteration: 80/137, steps: 17280
2022-12-29 20:18:05.610 DEBUG: Atoms are too close
2022-12-29 20:18:18.113 INFO: Training rollout: return=0.067 (3.3), episode length=5.0
2022-12-29 20:18:18.114 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:18:18.116 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-17280_train.pkl
2022-12-29 20:18:18.968 DEBUG: Taking gradient step
2022-12-29 20:18:18.977 DEBUG: Loss 0: {'policy_loss': -0.02658090076242766, 'entropy_loss': -0.019118182361125946, 'vf_loss': 0.007937292468093663, 'total_loss': -0.03776179065545994, 'approx_kl': -9.423335356473217e-08, 'clip_fraction': 0.0, 'grad_norm': 15.034123420715332}
2022-12-29 20:18:19.804 DEBUG: Taking gradient step
2022-12-29 20:18:19.813 DEBUG: Loss 1: {'policy_loss': 0.010790531929379187, 'entropy_loss': -0.019413204863667488, 'vf_loss': 0.010258449614333474, 'total_loss': 0.0016357766800451715, 'approx_kl': 0.009130466962233186, 'clip_fraction': 0.12630208395421505, 'grad_norm': 3.796987771987915}
2022-12-29 20:18:20.645 DEBUG: Taking gradient step
2022-12-29 20:18:20.655 DEBUG: Loss 2: {'policy_loss': -0.03120590208237867, 'entropy_loss': -0.021148521918803453, 'vf_loss': 0.008070661549491695, 'total_loss': -0.044283762451690425, 'approx_kl': 0.04483144043479115, 'clip_fraction': 0.3151041716337204, 'grad_norm': 4.461005210876465}
2022-12-29 20:18:21.454 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 20:18:21.454 INFO: Optimization: policy loss=-0.031, vf loss=0.008, entropy loss=-0.021, total loss=-0.044, num steps=3
2022-12-29 20:18:21.455 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:18:22.844 INFO: Evaluation rollout: return=0.724 (0.0), episode length=5.0
2022-12-29 20:18:22.847 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:18:22.849 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-15336.model
2022-12-29 20:18:22.852 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-17496.model
2022-12-29 20:18:22.881 INFO: Iteration: 81/137, steps: 17496
2022-12-29 20:18:54.128 DEBUG: There is a single atom floating around
2022-12-29 20:19:20.274 INFO: Training rollout: return=0.110 (3.3), episode length=5.0
2022-12-29 20:19:20.276 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:19:20.278 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-17496_train.pkl
2022-12-29 20:19:21.122 DEBUG: Taking gradient step
2022-12-29 20:19:21.131 DEBUG: Loss 0: {'policy_loss': -0.02243508730100355, 'entropy_loss': -0.021821168716996908, 'vf_loss': 0.008142678099466612, 'total_loss': -0.03611357791853384, 'approx_kl': 4.123042707249169e-08, 'clip_fraction': 0.0, 'grad_norm': 14.874796867370605}
2022-12-29 20:19:21.935 DEBUG: Taking gradient step
2022-12-29 20:19:21.944 DEBUG: Loss 1: {'policy_loss': -0.02544603020410786, 'entropy_loss': -0.022245487663894892, 'vf_loss': 0.008181226879289677, 'total_loss': -0.03951029098871307, 'approx_kl': 0.0026503289118409157, 'clip_fraction': 0.12369791697710752, 'grad_norm': 7.072183132171631}
2022-12-29 20:19:22.836 DEBUG: Taking gradient step
2022-12-29 20:19:22.850 DEBUG: Loss 2: {'policy_loss': -0.015100053467077627, 'entropy_loss': -0.023155592381954193, 'vf_loss': 0.008170607727471085, 'total_loss': -0.030085038121560735, 'approx_kl': 0.012505001155659556, 'clip_fraction': 0.19140625, 'grad_norm': 8.035177230834961}
2022-12-29 20:19:23.738 DEBUG: Taking gradient step
2022-12-29 20:19:23.747 DEBUG: Loss 3: {'policy_loss': -0.03224744338481681, 'entropy_loss': -0.023656456731259823, 'vf_loss': 0.008235315174927153, 'total_loss': -0.047668584941149485, 'approx_kl': 0.03972145589068532, 'clip_fraction': 0.2356770858168602, 'grad_norm': 7.279563903808594}
2022-12-29 20:19:24.545 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 20:19:24.545 INFO: Optimization: policy loss=-0.032, vf loss=0.008, entropy loss=-0.024, total loss=-0.048, num steps=4
2022-12-29 20:19:24.545 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:19:26.098 INFO: Evaluation rollout: return=0.743 (0.0), episode length=5.0
2022-12-29 20:19:26.099 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:19:26.101 INFO: Iteration: 82/137, steps: 17712
2022-12-29 20:20:23.652 INFO: Training rollout: return=0.647 (0.1), episode length=5.0
2022-12-29 20:20:23.653 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:20:23.657 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-17712_train.pkl
2022-12-29 20:20:24.501 DEBUG: Taking gradient step
2022-12-29 20:20:24.511 DEBUG: Loss 0: {'policy_loss': 0.04339553258365106, 'entropy_loss': -0.023445095401257277, 'vf_loss': 0.0011902424136359553, 'total_loss': 0.021140679596029735, 'approx_kl': -8.847564458847046e-08, 'clip_fraction': 0.0, 'grad_norm': 11.103107452392578}
2022-12-29 20:20:25.315 DEBUG: Taking gradient step
2022-12-29 20:20:25.324 DEBUG: Loss 1: {'policy_loss': -0.01899116172005666, 'entropy_loss': -0.02463885396718979, 'vf_loss': 0.0012077002028839073, 'total_loss': -0.042422315484362545, 'approx_kl': 0.010279236128553748, 'clip_fraction': 0.14322916697710752, 'grad_norm': 19.354602813720703}
2022-12-29 20:20:26.157 DEBUG: Taking gradient step
2022-12-29 20:20:26.171 DEBUG: Loss 2: {'policy_loss': 0.003187107802626356, 'entropy_loss': -0.02483480703085661, 'vf_loss': 0.0011838452160557735, 'total_loss': -0.020463854012174477, 'approx_kl': 0.014689362375065684, 'clip_fraction': 0.2734375, 'grad_norm': 20.8774471282959}
2022-12-29 20:20:26.994 DEBUG: Taking gradient step
2022-12-29 20:20:27.003 DEBUG: Loss 3: {'policy_loss': -0.02362939074179005, 'entropy_loss': -0.025549113750457764, 'vf_loss': 0.00114547902213451, 'total_loss': -0.04803302547011331, 'approx_kl': 0.02966302027925849, 'clip_fraction': 0.3541666716337204, 'grad_norm': 12.30681037902832}
2022-12-29 20:20:27.894 DEBUG: Taking gradient step
2022-12-29 20:20:27.903 DEBUG: Loss 4: {'policy_loss': 0.10548122717453366, 'entropy_loss': -0.024844651110470295, 'vf_loss': 0.0010399678546841639, 'total_loss': 0.08167654391874754, 'approx_kl': 0.029744924046099186, 'clip_fraction': 0.3958333358168602, 'grad_norm': 8.858254432678223}
2022-12-29 20:20:28.710 DEBUG: Taking gradient step
2022-12-29 20:20:28.720 DEBUG: Loss 5: {'policy_loss': 0.03896448606040881, 'entropy_loss': -0.025894918013364077, 'vf_loss': 0.001021911692851505, 'total_loss': 0.014091479739896242, 'approx_kl': 0.029670883901417255, 'clip_fraction': 0.3776041716337204, 'grad_norm': 15.189568519592285}
2022-12-29 20:20:29.534 DEBUG: Taking gradient step
2022-12-29 20:20:29.544 DEBUG: Loss 6: {'policy_loss': 0.016446320067070745, 'entropy_loss': -0.025385865475982428, 'vf_loss': 0.0009763747266286063, 'total_loss': -0.007963170682283074, 'approx_kl': 0.0023239515721797943, 'clip_fraction': 0.3268229216337204, 'grad_norm': 12.168813705444336}
2022-12-29 20:20:30.392 DEBUG: Taking gradient step
2022-12-29 20:20:30.402 DEBUG: Loss 7: {'policy_loss': 0.002001567108778333, 'entropy_loss': -0.02522088447585702, 'vf_loss': 0.0009240704019587384, 'total_loss': -0.02229524696511995, 'approx_kl': -0.013400611467659473, 'clip_fraction': 0.2018229179084301, 'grad_norm': 12.31830883026123}
2022-12-29 20:20:31.218 DEBUG: Taking gradient step
2022-12-29 20:20:31.228 DEBUG: Loss 8: {'policy_loss': -0.03648247487290866, 'entropy_loss': -0.025503905024379492, 'vf_loss': 0.0008899816625379738, 'total_loss': -0.06109639823475018, 'approx_kl': -0.0052553650457412004, 'clip_fraction': 0.1588541716337204, 'grad_norm': 12.93599796295166}
2022-12-29 20:20:32.078 DEBUG: Taking gradient step
2022-12-29 20:20:32.089 DEBUG: Loss 9: {'policy_loss': 0.046771806772536575, 'entropy_loss': -0.024509581737220287, 'vf_loss': 0.0008123849521525098, 'total_loss': 0.023074609987468804, 'approx_kl': 0.0027111413655802608, 'clip_fraction': 0.2096354216337204, 'grad_norm': 19.2489070892334}
2022-12-29 20:20:32.089 INFO: Optimization: policy loss=0.047, vf loss=0.001, entropy loss=-0.025, total loss=0.023, num steps=10
2022-12-29 20:20:32.090 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:20:33.542 INFO: Evaluation rollout: return=0.741 (0.0), episode length=5.0
2022-12-29 20:20:33.543 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:20:33.547 INFO: Iteration: 83/137, steps: 17928
2022-12-29 20:21:31.245 INFO: Training rollout: return=0.665 (0.1), episode length=5.0
2022-12-29 20:21:31.247 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:21:31.249 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-17928_train.pkl
2022-12-29 20:21:32.062 DEBUG: Taking gradient step
2022-12-29 20:21:32.071 DEBUG: Loss 0: {'policy_loss': 0.005450554019975344, 'entropy_loss': -0.024323666002601385, 'vf_loss': 0.0007424176777411412, 'total_loss': -0.018130694304884898, 'approx_kl': 7.877436836167817e-08, 'clip_fraction': 0.0, 'grad_norm': 15.951741218566895}
2022-12-29 20:21:32.883 DEBUG: Taking gradient step
2022-12-29 20:21:32.894 DEBUG: Loss 1: {'policy_loss': 0.014134077533128314, 'entropy_loss': -0.023017720319330692, 'vf_loss': 0.0006894470327697285, 'total_loss': -0.008194195753432644, 'approx_kl': 3.228208515793085e-05, 'clip_fraction': 0.041666666977107525, 'grad_norm': 5.919900417327881}
2022-12-29 20:21:33.759 DEBUG: Taking gradient step
2022-12-29 20:21:33.772 DEBUG: Loss 2: {'policy_loss': -0.006709566802863007, 'entropy_loss': -0.024023908656090498, 'vf_loss': 0.000644377599324482, 'total_loss': -0.030089097859629023, 'approx_kl': 0.014346482930704951, 'clip_fraction': 0.3346354216337204, 'grad_norm': 17.900514602661133}
2022-12-29 20:21:34.612 DEBUG: Taking gradient step
2022-12-29 20:21:34.623 DEBUG: Loss 3: {'policy_loss': 0.005319076161698235, 'entropy_loss': -0.023003313224762678, 'vf_loss': 0.0005977362278877226, 'total_loss': -0.017086500835176717, 'approx_kl': 0.03372034942731261, 'clip_fraction': 0.3307291716337204, 'grad_norm': 18.856304168701172}
2022-12-29 20:21:35.421 DEBUG: Taking gradient step
2022-12-29 20:21:35.431 DEBUG: Loss 4: {'policy_loss': 0.03376427385101469, 'entropy_loss': -0.02286642976105213, 'vf_loss': 0.0005490875870369677, 'total_loss': 0.01144693167699952, 'approx_kl': 0.012425380758941174, 'clip_fraction': 0.3255208358168602, 'grad_norm': 16.550874710083008}
2022-12-29 20:21:36.249 DEBUG: Taking gradient step
2022-12-29 20:21:36.259 DEBUG: Loss 5: {'policy_loss': -0.03372197681365545, 'entropy_loss': -0.0226517622359097, 'vf_loss': 0.0005364443475313066, 'total_loss': -0.05583729470203384, 'approx_kl': -0.0031434115953743458, 'clip_fraction': 0.2447916716337204, 'grad_norm': 7.258272171020508}
2022-12-29 20:21:37.066 DEBUG: Taking gradient step
2022-12-29 20:21:37.075 DEBUG: Loss 6: {'policy_loss': -0.016371996445460925, 'entropy_loss': -0.0222262404859066, 'vf_loss': 0.0004928184560709908, 'total_loss': -0.03810541847529654, 'approx_kl': -0.0001073833554983139, 'clip_fraction': 0.09505208395421505, 'grad_norm': 7.484789848327637}
2022-12-29 20:21:37.894 DEBUG: Taking gradient step
2022-12-29 20:21:37.903 DEBUG: Loss 7: {'policy_loss': -0.06463799514567929, 'entropy_loss': -0.02282423758879304, 'vf_loss': 0.000446432636310748, 'total_loss': -0.08701580009816157, 'approx_kl': -0.002379744197241962, 'clip_fraction': 0.05598958395421505, 'grad_norm': 16.853500366210938}
2022-12-29 20:21:38.721 DEBUG: Taking gradient step
2022-12-29 20:21:38.730 DEBUG: Loss 8: {'policy_loss': 0.007540314968535869, 'entropy_loss': -0.022632870357483625, 'vf_loss': 0.0004034866543103741, 'total_loss': -0.014689068734637382, 'approx_kl': -0.0004639171529561281, 'clip_fraction': 0.06380208395421505, 'grad_norm': 12.214844703674316}
2022-12-29 20:21:39.547 DEBUG: Taking gradient step
2022-12-29 20:21:39.558 DEBUG: Loss 9: {'policy_loss': 0.007913938910342613, 'entropy_loss': -0.02166223619133234, 'vf_loss': 0.00036928980206230104, 'total_loss': -0.013379007478927427, 'approx_kl': -0.005346674472093582, 'clip_fraction': 0.234375, 'grad_norm': 9.530384063720703}
2022-12-29 20:21:39.558 INFO: Optimization: policy loss=0.008, vf loss=0.000, entropy loss=-0.022, total loss=-0.013, num steps=10
2022-12-29 20:21:39.558 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:21:41.059 INFO: Evaluation rollout: return=0.743 (0.0), episode length=5.0
2022-12-29 20:21:41.060 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:21:41.063 INFO: Iteration: 84/137, steps: 18144
2022-12-29 20:22:08.615 DEBUG: There is a single atom floating around
2022-12-29 20:22:19.627 DEBUG: Atoms are too close
2022-12-29 20:22:38.583 INFO: Training rollout: return=-0.434 (4.6), episode length=4.9
2022-12-29 20:22:38.584 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:22:38.587 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-18144_train.pkl
2022-12-29 20:22:39.391 DEBUG: Taking gradient step
2022-12-29 20:22:39.400 DEBUG: Loss 0: {'policy_loss': 0.008253196599798222, 'entropy_loss': -0.02222923608496785, 'vf_loss': 0.013467045571578407, 'total_loss': -0.0005089939135912262, 'approx_kl': -3.41096892952919e-08, 'clip_fraction': 0.0, 'grad_norm': 20.911508560180664}
2022-12-29 20:22:40.216 DEBUG: Taking gradient step
2022-12-29 20:22:40.227 DEBUG: Loss 1: {'policy_loss': -0.0006059662657573088, 'entropy_loss': -0.021284181624650955, 'vf_loss': 0.013437492473314347, 'total_loss': -0.008452655417093918, 'approx_kl': 0.0022245447034947574, 'clip_fraction': 0.0078125, 'grad_norm': 13.692059516906738}
2022-12-29 20:22:41.066 DEBUG: Taking gradient step
2022-12-29 20:22:41.075 DEBUG: Loss 2: {'policy_loss': -0.006671055852109464, 'entropy_loss': -0.021415138617157936, 'vf_loss': 0.013405956304055215, 'total_loss': -0.014680238165212186, 'approx_kl': -0.004696846939623356, 'clip_fraction': 0.171875, 'grad_norm': 10.648459434509277}
2022-12-29 20:22:41.938 DEBUG: Taking gradient step
2022-12-29 20:22:41.952 DEBUG: Loss 3: {'policy_loss': -0.011116775650825219, 'entropy_loss': -0.0212287581525743, 'vf_loss': 0.013447174647332808, 'total_loss': -0.018898359156066715, 'approx_kl': 0.029606914147734642, 'clip_fraction': 0.2786458358168602, 'grad_norm': 5.131566047668457}
2022-12-29 20:22:42.875 DEBUG: Taking gradient step
2022-12-29 20:22:42.885 DEBUG: Loss 4: {'policy_loss': 0.028845190704953325, 'entropy_loss': -0.020703180693089962, 'vf_loss': 0.015977921557871477, 'total_loss': 0.024119931569734833, 'approx_kl': 0.03828726848587394, 'clip_fraction': 0.2552083358168602, 'grad_norm': 5.468882083892822}
2022-12-29 20:22:43.685 DEBUG: Taking gradient step
2022-12-29 20:22:43.694 DEBUG: Loss 5: {'policy_loss': -0.04526875844772905, 'entropy_loss': -0.02060422534123063, 'vf_loss': 0.010927545373826572, 'total_loss': -0.05494543841513311, 'approx_kl': 0.010715631768107414, 'clip_fraction': 0.1783854179084301, 'grad_norm': 1.6763190031051636}
2022-12-29 20:22:44.469 DEBUG: Taking gradient step
2022-12-29 20:22:44.479 DEBUG: Loss 6: {'policy_loss': -0.0033234617413387714, 'entropy_loss': -0.02117849700152874, 'vf_loss': 0.013409870246324996, 'total_loss': -0.011092088496542512, 'approx_kl': 0.023505744757130742, 'clip_fraction': 0.2044270858168602, 'grad_norm': 1.9794708490371704}
2022-12-29 20:22:45.303 DEBUG: Taking gradient step
2022-12-29 20:22:45.317 DEBUG: Loss 7: {'policy_loss': -0.045514012589090325, 'entropy_loss': -0.02092422591522336, 'vf_loss': 0.010919502135233563, 'total_loss': -0.05551873636908012, 'approx_kl': 0.025204127188771963, 'clip_fraction': 0.2447916716337204, 'grad_norm': 2.1470839977264404}
2022-12-29 20:22:46.165 DEBUG: Taking gradient step
2022-12-29 20:22:46.174 DEBUG: Loss 8: {'policy_loss': -0.008895930516531383, 'entropy_loss': -0.020186617504805326, 'vf_loss': 0.01350810401982141, 'total_loss': -0.0155744440015153, 'approx_kl': 0.024557163007557392, 'clip_fraction': 0.1875, 'grad_norm': 24.013601303100586}
2022-12-29 20:22:46.980 DEBUG: Taking gradient step
2022-12-29 20:22:46.989 DEBUG: Loss 9: {'policy_loss': -0.04358789325795574, 'entropy_loss': -0.02115053730085492, 'vf_loss': 0.010925228419843029, 'total_loss': -0.053813202138967633, 'approx_kl': 0.037273941561579704, 'clip_fraction': 0.2513020858168602, 'grad_norm': 3.749195098876953}
2022-12-29 20:22:46.989 INFO: Optimization: policy loss=-0.044, vf loss=0.011, entropy loss=-0.021, total loss=-0.054, num steps=10
2022-12-29 20:22:46.989 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:22:48.446 INFO: Evaluation rollout: return=0.751 (0.0), episode length=5.0
2022-12-29 20:22:48.447 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:22:48.451 INFO: Iteration: 85/137, steps: 18360
2022-12-29 20:23:00.662 DEBUG: There is a single atom floating around
2022-12-29 20:23:46.412 INFO: Training rollout: return=0.107 (3.3), episode length=5.0
2022-12-29 20:23:46.414 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:23:46.417 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-18360_train.pkl
2022-12-29 20:23:47.269 DEBUG: Taking gradient step
2022-12-29 20:23:47.279 DEBUG: Loss 0: {'policy_loss': 0.026845921506257715, 'entropy_loss': -0.02075898926705122, 'vf_loss': 0.008849041971425449, 'total_loss': 0.014935974210631947, 'approx_kl': -5.975987704687213e-09, 'clip_fraction': 0.0, 'grad_norm': 20.13604736328125}
2022-12-29 20:23:48.104 DEBUG: Taking gradient step
2022-12-29 20:23:48.113 DEBUG: Loss 1: {'policy_loss': 0.012464047698409808, 'entropy_loss': -0.020885229110717773, 'vf_loss': 0.008859934200352138, 'total_loss': 0.00043875278804417217, 'approx_kl': 0.006532728322781622, 'clip_fraction': 0.033854166977107525, 'grad_norm': 13.978394508361816}
2022-12-29 20:23:48.928 DEBUG: Taking gradient step
2022-12-29 20:23:48.937 DEBUG: Loss 2: {'policy_loss': -0.03711033753689544, 'entropy_loss': -0.02118393825367093, 'vf_loss': 0.006364250641396674, 'total_loss': -0.051930025149169695, 'approx_kl': 0.025930706411600113, 'clip_fraction': 0.23046875, 'grad_norm': 3.1948800086975098}
2022-12-29 20:23:49.801 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 20:23:49.801 INFO: Optimization: policy loss=-0.037, vf loss=0.006, entropy loss=-0.021, total loss=-0.052, num steps=3
2022-12-29 20:23:49.802 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:23:51.206 INFO: Evaluation rollout: return=0.761 (0.0), episode length=5.0
2022-12-29 20:23:51.207 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:23:51.210 INFO: Iteration: 86/137, steps: 18576
2022-12-29 20:24:49.585 INFO: Training rollout: return=0.667 (0.1), episode length=5.0
2022-12-29 20:24:49.586 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:24:49.589 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-18576_train.pkl
2022-12-29 20:24:50.396 DEBUG: Taking gradient step
2022-12-29 20:24:50.406 DEBUG: Loss 0: {'policy_loss': 0.01931064720868774, 'entropy_loss': -0.021841897163540125, 'vf_loss': 0.00029812256221756596, 'total_loss': -0.0022331273926348184, 'approx_kl': -4.550868837327471e-08, 'clip_fraction': 0.0, 'grad_norm': 9.664886474609375}
2022-12-29 20:24:51.267 DEBUG: Taking gradient step
2022-12-29 20:24:51.276 DEBUG: Loss 1: {'policy_loss': 0.042790220178320196, 'entropy_loss': -0.022802086547017097, 'vf_loss': 0.00029535365575208923, 'total_loss': 0.020283487287055195, 'approx_kl': 0.01223425380885601, 'clip_fraction': 0.1302083358168602, 'grad_norm': 13.215744972229004}
2022-12-29 20:24:52.080 DEBUG: Taking gradient step
2022-12-29 20:24:52.090 DEBUG: Loss 2: {'policy_loss': 0.055292238610961816, 'entropy_loss': -0.022689474746584892, 'vf_loss': 0.0002843710947168951, 'total_loss': 0.03288713495909381, 'approx_kl': 0.03462776355445385, 'clip_fraction': 0.265625, 'grad_norm': 16.02849006652832}
2022-12-29 20:24:52.889 DEBUG: Taking gradient step
2022-12-29 20:24:52.898 DEBUG: Loss 3: {'policy_loss': -0.0021309169618762332, 'entropy_loss': -0.021808921359479427, 'vf_loss': 0.0003001450592838252, 'total_loss': -0.023639693262071833, 'approx_kl': 0.03959304839372635, 'clip_fraction': 0.2578125, 'grad_norm': 19.524608612060547}
2022-12-29 20:24:53.719 DEBUG: Taking gradient step
2022-12-29 20:24:53.728 DEBUG: Loss 4: {'policy_loss': 0.01929230215763571, 'entropy_loss': -0.02205170365050435, 'vf_loss': 0.00028866507274525244, 'total_loss': -0.002470736420123385, 'approx_kl': 0.029657701335963793, 'clip_fraction': 0.1888020858168602, 'grad_norm': 15.480563163757324}
2022-12-29 20:24:54.559 DEBUG: Early stopping at step 5 for reaching max KL.
2022-12-29 20:24:54.559 INFO: Optimization: policy loss=0.019, vf loss=0.000, entropy loss=-0.022, total loss=-0.002, num steps=5
2022-12-29 20:24:54.560 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:24:56.047 INFO: Evaluation rollout: return=0.772 (0.0), episode length=5.0
2022-12-29 20:24:56.048 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:24:56.051 INFO: Iteration: 87/137, steps: 18792
2022-12-29 20:25:54.372 INFO: Training rollout: return=0.684 (0.1), episode length=5.0
2022-12-29 20:25:54.374 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:25:54.376 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-18792_train.pkl
2022-12-29 20:25:55.194 DEBUG: Taking gradient step
2022-12-29 20:25:55.203 DEBUG: Loss 0: {'policy_loss': -0.024744029899261777, 'entropy_loss': -0.021841467823833227, 'vf_loss': 0.0003004299389099107, 'total_loss': -0.04628506778418509, 'approx_kl': -4.15408685228158e-08, 'clip_fraction': 0.0, 'grad_norm': 7.571956634521484}
2022-12-29 20:25:56.023 DEBUG: Taking gradient step
2022-12-29 20:25:56.033 DEBUG: Loss 1: {'policy_loss': -0.0950205908011197, 'entropy_loss': -0.021343727130442858, 'vf_loss': 0.00030559472767557574, 'total_loss': -0.11605872320388697, 'approx_kl': 0.00967614515684545, 'clip_fraction': 0.05078125, 'grad_norm': 5.440293788909912}
2022-12-29 20:25:56.875 DEBUG: Taking gradient step
2022-12-29 20:25:56.884 DEBUG: Loss 2: {'policy_loss': -0.034759723877074704, 'entropy_loss': -0.021126090548932552, 'vf_loss': 0.0002740761435687934, 'total_loss': -0.05561173828243847, 'approx_kl': 0.021796117071062326, 'clip_fraction': 0.2734375, 'grad_norm': 12.105205535888672}
2022-12-29 20:25:57.701 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 20:25:57.701 INFO: Optimization: policy loss=-0.035, vf loss=0.000, entropy loss=-0.021, total loss=-0.056, num steps=3
2022-12-29 20:25:57.702 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:25:59.063 INFO: Evaluation rollout: return=0.759 (0.0), episode length=5.0
2022-12-29 20:25:59.064 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:25:59.066 INFO: Iteration: 88/137, steps: 19008
2022-12-29 20:26:56.961 INFO: Training rollout: return=0.671 (0.1), episode length=5.0
2022-12-29 20:26:56.962 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:26:56.965 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-19008_train.pkl
2022-12-29 20:26:57.840 DEBUG: Taking gradient step
2022-12-29 20:26:57.849 DEBUG: Loss 0: {'policy_loss': 0.00852107799674267, 'entropy_loss': -0.020945385564118624, 'vf_loss': 0.0002510946816174157, 'total_loss': -0.01217321288575854, 'approx_kl': 5.7334545999765396e-08, 'clip_fraction': 0.0, 'grad_norm': 10.593693733215332}
2022-12-29 20:26:58.665 DEBUG: Taking gradient step
2022-12-29 20:26:58.675 DEBUG: Loss 1: {'policy_loss': -0.021302065548449475, 'entropy_loss': -0.02078697318211198, 'vf_loss': 0.00024172431311604353, 'total_loss': -0.04184731441744541, 'approx_kl': 0.005545839434489608, 'clip_fraction': 0.03125, 'grad_norm': 9.728779792785645}
2022-12-29 20:26:59.489 DEBUG: Taking gradient step
2022-12-29 20:26:59.498 DEBUG: Loss 2: {'policy_loss': -0.027955320804529053, 'entropy_loss': -0.020303789526224136, 'vf_loss': 0.0002377878550960381, 'total_loss': -0.04802132247565716, 'approx_kl': 0.02532897109631449, 'clip_fraction': 0.2473958358168602, 'grad_norm': 14.745438575744629}
2022-12-29 20:27:00.293 DEBUG: Taking gradient step
2022-12-29 20:27:00.306 DEBUG: Loss 3: {'policy_loss': 0.0049297958847574555, 'entropy_loss': -0.019037501886487007, 'vf_loss': 0.00021525403588935094, 'total_loss': -0.01389245196584019, 'approx_kl': 0.026120093651115894, 'clip_fraction': 0.30078125, 'grad_norm': 15.763803482055664}
2022-12-29 20:27:01.137 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 20:27:01.137 INFO: Optimization: policy loss=0.005, vf loss=0.000, entropy loss=-0.019, total loss=-0.014, num steps=4
2022-12-29 20:27:01.138 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:27:02.577 INFO: Evaluation rollout: return=0.747 (0.0), episode length=5.0
2022-12-29 20:27:02.578 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:27:02.580 INFO: Iteration: 89/137, steps: 19224
2022-12-29 20:27:15.597 DEBUG: There is a single atom floating around
2022-12-29 20:27:50.532 DEBUG: Atoms are too close
2022-12-29 20:28:00.105 INFO: Training rollout: return=-0.444 (4.6), episode length=5.0
2022-12-29 20:28:00.106 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:28:00.109 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-19224_train.pkl
2022-12-29 20:28:00.957 DEBUG: Taking gradient step
2022-12-29 20:28:00.966 DEBUG: Loss 0: {'policy_loss': -0.004248468708316113, 'entropy_loss': -0.019639243837445974, 'vf_loss': 0.018232000263809264, 'total_loss': -0.005655712281952826, 'approx_kl': 2.896801376550684e-08, 'clip_fraction': 0.0, 'grad_norm': 23.5305233001709}
2022-12-29 20:28:01.762 DEBUG: Taking gradient step
2022-12-29 20:28:01.772 DEBUG: Loss 1: {'policy_loss': -0.007178702491869753, 'entropy_loss': -0.019518780522048473, 'vf_loss': 0.018493406115336203, 'total_loss': -0.008204076898582022, 'approx_kl': 0.0046924930065870285, 'clip_fraction': 0.0234375, 'grad_norm': 15.452244758605957}
2022-12-29 20:28:02.575 DEBUG: Taking gradient step
2022-12-29 20:28:02.587 DEBUG: Loss 2: {'policy_loss': 0.04597495006826813, 'entropy_loss': -0.019312112126499414, 'vf_loss': 0.023557508867442685, 'total_loss': 0.050220346809211405, 'approx_kl': 0.013901000143960118, 'clip_fraction': 0.0885416679084301, 'grad_norm': 21.718189239501953}
2022-12-29 20:28:03.383 DEBUG: Taking gradient step
2022-12-29 20:28:03.393 DEBUG: Loss 3: {'policy_loss': -0.053764434920681246, 'entropy_loss': -0.018068430479615927, 'vf_loss': 0.015831774168631903, 'total_loss': -0.05600109123166528, 'approx_kl': 0.017331471026409417, 'clip_fraction': 0.14322916697710752, 'grad_norm': 3.1503262519836426}
2022-12-29 20:28:04.190 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 20:28:04.190 INFO: Optimization: policy loss=-0.054, vf loss=0.016, entropy loss=-0.018, total loss=-0.056, num steps=4
2022-12-29 20:28:04.191 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:28:05.549 INFO: Evaluation rollout: return=0.760 (0.0), episode length=5.0
2022-12-29 20:28:05.550 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:28:05.552 INFO: Iteration: 90/137, steps: 19440
2022-12-29 20:29:02.690 INFO: Training rollout: return=0.653 (0.1), episode length=5.0
2022-12-29 20:29:02.693 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:29:02.695 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-19440_train.pkl
2022-12-29 20:29:03.530 DEBUG: Taking gradient step
2022-12-29 20:29:03.539 DEBUG: Loss 0: {'policy_loss': -0.012004691826926636, 'entropy_loss': -0.018912496976554394, 'vf_loss': 0.00026324613852534226, 'total_loss': -0.030653942664955695, 'approx_kl': -3.047171048820019e-08, 'clip_fraction': 0.0, 'grad_norm': 17.951862335205078}
2022-12-29 20:29:04.340 DEBUG: Taking gradient step
2022-12-29 20:29:04.351 DEBUG: Loss 1: {'policy_loss': -0.005629289325779068, 'entropy_loss': -0.018329271115362644, 'vf_loss': 0.00023875109591157495, 'total_loss': -0.023719809345230138, 'approx_kl': 0.030476499581709504, 'clip_fraction': 0.1575520858168602, 'grad_norm': 14.135445594787598}
2022-12-29 20:29:05.156 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 20:29:05.156 INFO: Optimization: policy loss=-0.006, vf loss=0.000, entropy loss=-0.018, total loss=-0.024, num steps=2
2022-12-29 20:29:05.157 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:29:06.621 INFO: Evaluation rollout: return=0.758 (0.0), episode length=5.0
2022-12-29 20:29:06.623 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:29:06.626 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-17496.model
2022-12-29 20:29:06.631 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-19656.model
2022-12-29 20:29:06.662 INFO: Iteration: 91/137, steps: 19656
2022-12-29 20:30:04.676 INFO: Training rollout: return=0.647 (0.1), episode length=5.0
2022-12-29 20:30:04.678 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:30:04.680 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-19656_train.pkl
2022-12-29 20:30:05.595 DEBUG: Taking gradient step
2022-12-29 20:30:05.604 DEBUG: Loss 0: {'policy_loss': -0.030763020415928852, 'entropy_loss': -0.017918015364557505, 'vf_loss': 0.0002222896369638297, 'total_loss': -0.04845874614352253, 'approx_kl': 2.518451402977462e-08, 'clip_fraction': 0.0, 'grad_norm': 14.729525566101074}
2022-12-29 20:30:06.443 DEBUG: Taking gradient step
2022-12-29 20:30:06.453 DEBUG: Loss 1: {'policy_loss': -0.047026717903307966, 'entropy_loss': -0.01746470737271011, 'vf_loss': 0.00021124048017468839, 'total_loss': -0.06428018479584338, 'approx_kl': 0.004967479093465954, 'clip_fraction': 0.01171875, 'grad_norm': 14.31824016571045}
2022-12-29 20:30:07.240 DEBUG: Taking gradient step
2022-12-29 20:30:07.250 DEBUG: Loss 2: {'policy_loss': -0.03018806408904326, 'entropy_loss': -0.016595537308603525, 'vf_loss': 0.00019711710567658363, 'total_loss': -0.04658648429197021, 'approx_kl': 0.012444306164979935, 'clip_fraction': 0.12760416697710752, 'grad_norm': 22.658321380615234}
2022-12-29 20:30:08.069 DEBUG: Taking gradient step
2022-12-29 20:30:08.078 DEBUG: Loss 3: {'policy_loss': -0.022626919261003398, 'entropy_loss': -0.017070268280804157, 'vf_loss': 0.00019172523910952744, 'total_loss': -0.039505462302698025, 'approx_kl': 0.02088098879903555, 'clip_fraction': 0.1848958358168602, 'grad_norm': 13.169792175292969}
2022-12-29 20:30:08.914 DEBUG: Taking gradient step
2022-12-29 20:30:08.925 DEBUG: Loss 4: {'policy_loss': -0.05314665920407004, 'entropy_loss': -0.016941037494689226, 'vf_loss': 0.0001968467967716987, 'total_loss': -0.06989084990198757, 'approx_kl': 0.03628442715853453, 'clip_fraction': 0.11848958395421505, 'grad_norm': 11.185511589050293}
2022-12-29 20:30:09.749 DEBUG: Early stopping at step 5 for reaching max KL.
2022-12-29 20:30:09.749 INFO: Optimization: policy loss=-0.053, vf loss=0.000, entropy loss=-0.017, total loss=-0.070, num steps=5
2022-12-29 20:30:09.750 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:30:11.207 INFO: Evaluation rollout: return=0.764 (0.0), episode length=5.0
2022-12-29 20:30:11.207 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:30:11.210 INFO: Iteration: 92/137, steps: 19872
2022-12-29 20:31:09.484 INFO: Training rollout: return=0.678 (0.1), episode length=5.0
2022-12-29 20:31:09.485 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:31:09.488 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-19872_train.pkl
2022-12-29 20:31:10.312 DEBUG: Taking gradient step
2022-12-29 20:31:10.322 DEBUG: Loss 0: {'policy_loss': 0.010847675641592521, 'entropy_loss': -0.01692489068955183, 'vf_loss': 0.00021026099788678163, 'total_loss': -0.005866954050072527, 'approx_kl': 1.1753097162170434e-08, 'clip_fraction': 0.0, 'grad_norm': 14.884572982788086}
2022-12-29 20:31:11.142 DEBUG: Taking gradient step
2022-12-29 20:31:11.154 DEBUG: Loss 1: {'policy_loss': 0.007298981180483614, 'entropy_loss': -0.016290548257529736, 'vf_loss': 0.0002026018629162713, 'total_loss': -0.008788965214129852, 'approx_kl': 0.014501040801405907, 'clip_fraction': 0.07161458395421505, 'grad_norm': 5.292454719543457}
2022-12-29 20:31:11.992 DEBUG: Taking gradient step
2022-12-29 20:31:12.001 DEBUG: Loss 2: {'policy_loss': 0.010225235969330636, 'entropy_loss': -0.017225997056812048, 'vf_loss': 0.000200547670506709, 'total_loss': -0.006800213416974704, 'approx_kl': 0.03600404458120465, 'clip_fraction': 0.2916666716337204, 'grad_norm': 23.22473907470703}
2022-12-29 20:31:12.875 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 20:31:12.876 INFO: Optimization: policy loss=0.010, vf loss=0.000, entropy loss=-0.017, total loss=-0.007, num steps=3
2022-12-29 20:31:12.876 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:31:14.255 INFO: Evaluation rollout: return=0.773 (0.0), episode length=5.0
2022-12-29 20:31:14.256 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:31:14.259 INFO: Iteration: 93/137, steps: 20088
2022-12-29 20:32:12.188 INFO: Training rollout: return=0.670 (0.1), episode length=5.0
2022-12-29 20:32:12.190 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:32:12.192 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-20088_train.pkl
2022-12-29 20:32:13.005 DEBUG: Taking gradient step
2022-12-29 20:32:13.020 DEBUG: Loss 0: {'policy_loss': -0.01870114613436843, 'entropy_loss': -0.017816292122006416, 'vf_loss': 0.00019658714655079008, 'total_loss': -0.036320851109824054, 'approx_kl': 9.313225746154785e-09, 'clip_fraction': 0.0, 'grad_norm': 11.743897438049316}
2022-12-29 20:32:13.832 DEBUG: Taking gradient step
2022-12-29 20:32:13.841 DEBUG: Loss 1: {'policy_loss': -0.03884995345359343, 'entropy_loss': -0.017021744279190898, 'vf_loss': 0.00019364009674985143, 'total_loss': -0.05567805763603448, 'approx_kl': 0.011097019305452704, 'clip_fraction': 0.0390625, 'grad_norm': 8.442076683044434}
2022-12-29 20:32:14.695 DEBUG: Taking gradient step
2022-12-29 20:32:14.709 DEBUG: Loss 2: {'policy_loss': -0.010954296089987678, 'entropy_loss': -0.01846868498250842, 'vf_loss': 0.0001857577427448044, 'total_loss': -0.029237223329751306, 'approx_kl': 0.02917861906462349, 'clip_fraction': 0.2395833358168602, 'grad_norm': 10.165811538696289}
2022-12-29 20:32:15.567 DEBUG: Taking gradient step
2022-12-29 20:32:15.576 DEBUG: Loss 3: {'policy_loss': -0.02435675028620432, 'entropy_loss': -0.018225420266389847, 'vf_loss': 0.0001813242340118895, 'total_loss': -0.04240084631858227, 'approx_kl': 0.04032860603183508, 'clip_fraction': 0.3268229216337204, 'grad_norm': 12.012276649475098}
2022-12-29 20:32:16.422 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 20:32:16.422 INFO: Optimization: policy loss=-0.024, vf loss=0.000, entropy loss=-0.018, total loss=-0.042, num steps=4
2022-12-29 20:32:16.422 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:32:17.846 INFO: Evaluation rollout: return=0.767 (0.0), episode length=5.0
2022-12-29 20:32:17.847 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:32:17.850 INFO: Iteration: 94/137, steps: 20304
2022-12-29 20:33:15.049 INFO: Training rollout: return=0.675 (0.1), episode length=5.0
2022-12-29 20:33:15.050 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:33:15.053 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-20304_train.pkl
2022-12-29 20:33:15.884 DEBUG: Taking gradient step
2022-12-29 20:33:15.893 DEBUG: Loss 0: {'policy_loss': -0.02428397729365465, 'entropy_loss': -0.019614001736044884, 'vf_loss': 0.0001771300349180061, 'total_loss': -0.043720848994781535, 'approx_kl': 3.057842468123795e-08, 'clip_fraction': 0.0, 'grad_norm': 6.10136604309082}
2022-12-29 20:33:16.691 DEBUG: Taking gradient step
2022-12-29 20:33:16.701 DEBUG: Loss 1: {'policy_loss': -0.03982254351351677, 'entropy_loss': -0.020386340096592903, 'vf_loss': 0.0001746671967184673, 'total_loss': -0.060034216413391205, 'approx_kl': 0.009220895590260625, 'clip_fraction': 0.022135416977107525, 'grad_norm': 7.491610050201416}
2022-12-29 20:33:17.486 DEBUG: Taking gradient step
2022-12-29 20:33:17.495 DEBUG: Loss 2: {'policy_loss': 0.003642831191670271, 'entropy_loss': -0.02024157950654626, 'vf_loss': 0.00015885463711869195, 'total_loss': -0.0164398936777573, 'approx_kl': 0.015695634996518493, 'clip_fraction': 0.04427083395421505, 'grad_norm': 6.074794769287109}
2022-12-29 20:33:18.282 DEBUG: Taking gradient step
2022-12-29 20:33:18.292 DEBUG: Loss 3: {'policy_loss': -0.023232086936554205, 'entropy_loss': -0.021629691123962402, 'vf_loss': 0.000156851521095578, 'total_loss': -0.04470492653942103, 'approx_kl': 0.01678982749581337, 'clip_fraction': 0.1979166716337204, 'grad_norm': 23.832120895385742}
2022-12-29 20:33:19.109 DEBUG: Taking gradient step
2022-12-29 20:33:19.122 DEBUG: Loss 4: {'policy_loss': -0.0001753718413645758, 'entropy_loss': -0.02218604227527976, 'vf_loss': 0.00014108950857539835, 'total_loss': -0.022220324608068935, 'approx_kl': 0.03834692668169737, 'clip_fraction': 0.2239583358168602, 'grad_norm': 13.786216735839844}
2022-12-29 20:33:20.017 DEBUG: Early stopping at step 5 for reaching max KL.
2022-12-29 20:33:20.018 INFO: Optimization: policy loss=-0.000, vf loss=0.000, entropy loss=-0.022, total loss=-0.022, num steps=5
2022-12-29 20:33:20.018 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:33:21.395 INFO: Evaluation rollout: return=0.772 (0.0), episode length=5.0
2022-12-29 20:33:21.396 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:33:21.399 INFO: Iteration: 95/137, steps: 20520
2022-12-29 20:34:19.177 INFO: Training rollout: return=0.664 (0.1), episode length=5.0
2022-12-29 20:34:19.178 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:34:19.181 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-20520_train.pkl
2022-12-29 20:34:20.014 DEBUG: Taking gradient step
2022-12-29 20:34:20.024 DEBUG: Loss 0: {'policy_loss': 0.021017468349424185, 'entropy_loss': -0.02227534493431449, 'vf_loss': 0.0001520046335481637, 'total_loss': -0.0011058719513421418, 'approx_kl': -3.279031579594971e-08, 'clip_fraction': 0.0, 'grad_norm': 8.932297706604004}
2022-12-29 20:34:20.852 DEBUG: Taking gradient step
2022-12-29 20:34:20.862 DEBUG: Loss 1: {'policy_loss': -0.012374284230565143, 'entropy_loss': -0.022618107963353395, 'vf_loss': 0.00015245212916835583, 'total_loss': -0.034839940064750184, 'approx_kl': 0.018008995917625725, 'clip_fraction': 0.11979166697710752, 'grad_norm': 8.776665687561035}
2022-12-29 20:34:21.673 DEBUG: Taking gradient step
2022-12-29 20:34:21.682 DEBUG: Loss 2: {'policy_loss': -0.03693361120986391, 'entropy_loss': -0.022665355820208788, 'vf_loss': 0.0001471661371711292, 'total_loss': -0.059451800892901564, 'approx_kl': 0.03397034830413759, 'clip_fraction': 0.2096354216337204, 'grad_norm': 13.888053894042969}
2022-12-29 20:34:22.516 DEBUG: Taking gradient step
2022-12-29 20:34:22.525 DEBUG: Loss 3: {'policy_loss': 0.0020411206331822085, 'entropy_loss': -0.024085892364382744, 'vf_loss': 0.0001361324219128526, 'total_loss': -0.021908639309287686, 'approx_kl': 0.04131422936916351, 'clip_fraction': 0.16015625, 'grad_norm': 5.599504470825195}
2022-12-29 20:34:23.348 DEBUG: Taking gradient step
2022-12-29 20:34:23.359 DEBUG: Loss 4: {'policy_loss': 0.00869237405020135, 'entropy_loss': -0.024328398518264294, 'vf_loss': 0.00012985236668941347, 'total_loss': -0.015506172101373533, 'approx_kl': 0.03916932875290513, 'clip_fraction': 0.171875, 'grad_norm': 10.149667739868164}
2022-12-29 20:34:24.186 DEBUG: Early stopping at step 5 for reaching max KL.
2022-12-29 20:34:24.187 INFO: Optimization: policy loss=0.009, vf loss=0.000, entropy loss=-0.024, total loss=-0.016, num steps=5
2022-12-29 20:34:24.187 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:34:25.714 INFO: Evaluation rollout: return=0.770 (0.0), episode length=5.0
2022-12-29 20:34:25.715 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:34:25.717 INFO: Iteration: 96/137, steps: 20736
2022-12-29 20:35:19.015 DEBUG: Atoms are too close
2022-12-29 20:35:22.472 INFO: Training rollout: return=0.117 (3.3), episode length=4.9
2022-12-29 20:35:22.473 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:35:22.476 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-20736_train.pkl
2022-12-29 20:35:23.345 DEBUG: Taking gradient step
2022-12-29 20:35:23.355 DEBUG: Loss 0: {'policy_loss': -0.022405833300048347, 'entropy_loss': -0.02414529398083687, 'vf_loss': 0.004854924657441654, 'total_loss': -0.04169620262344357, 'approx_kl': -6.173892597161057e-08, 'clip_fraction': 0.0, 'grad_norm': 17.393430709838867}
2022-12-29 20:35:24.188 DEBUG: Taking gradient step
2022-12-29 20:35:24.198 DEBUG: Loss 1: {'policy_loss': -0.02748094452600099, 'entropy_loss': -0.02536009158939123, 'vf_loss': 0.004852732545981821, 'total_loss': -0.047988303569410404, 'approx_kl': 0.0006296661449596286, 'clip_fraction': 0.02734375, 'grad_norm': 4.327188491821289}
2022-12-29 20:35:25.060 DEBUG: Taking gradient step
2022-12-29 20:35:25.073 DEBUG: Loss 2: {'policy_loss': 0.013583809087416729, 'entropy_loss': -0.024969169404357672, 'vf_loss': 0.0074971753828367935, 'total_loss': -0.003888184934104158, 'approx_kl': 0.015155619126744568, 'clip_fraction': 0.1119791679084301, 'grad_norm': 3.380080461502075}
2022-12-29 20:35:25.873 DEBUG: Taking gradient step
2022-12-29 20:35:25.887 DEBUG: Loss 3: {'policy_loss': -0.02640780832861927, 'entropy_loss': -0.026409673038870096, 'vf_loss': 0.004847507593298183, 'total_loss': -0.04796997377419118, 'approx_kl': 0.01472837058827281, 'clip_fraction': 0.3333333358168602, 'grad_norm': 1.7855802774429321}
2022-12-29 20:35:26.786 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 20:35:26.786 INFO: Optimization: policy loss=-0.026, vf loss=0.005, entropy loss=-0.026, total loss=-0.048, num steps=4
2022-12-29 20:35:26.787 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:35:28.252 INFO: Evaluation rollout: return=0.790 (0.0), episode length=5.0
2022-12-29 20:35:28.253 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:35:28.256 INFO: Iteration: 97/137, steps: 20952
2022-12-29 20:36:26.209 INFO: Training rollout: return=0.648 (0.1), episode length=5.0
2022-12-29 20:36:26.210 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:36:26.213 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-20952_train.pkl
2022-12-29 20:36:27.109 DEBUG: Taking gradient step
2022-12-29 20:36:27.120 DEBUG: Loss 0: {'policy_loss': -0.00916775075030134, 'entropy_loss': -0.0266839019022882, 'vf_loss': 0.00010558752812155655, 'total_loss': -0.035746065124467985, 'approx_kl': 4.5353470312647914e-08, 'clip_fraction': 0.0, 'grad_norm': 10.0120267868042}
2022-12-29 20:36:27.950 DEBUG: Taking gradient step
2022-12-29 20:36:27.966 DEBUG: Loss 1: {'policy_loss': -0.00596092846321411, 'entropy_loss': -0.02724013803526759, 'vf_loss': 0.00010606499294044543, 'total_loss': -0.03309500150554127, 'approx_kl': -0.00254213809967041, 'clip_fraction': 0.1979166679084301, 'grad_norm': 15.69503402709961}
2022-12-29 20:36:28.874 DEBUG: Taking gradient step
2022-12-29 20:36:28.883 DEBUG: Loss 2: {'policy_loss': 0.02276168410097668, 'entropy_loss': -0.027335043996572495, 'vf_loss': 9.670383424186798e-05, 'total_loss': -0.004476656061353949, 'approx_kl': 0.007201269268989563, 'clip_fraction': 0.3802083358168602, 'grad_norm': 13.846002578735352}
2022-12-29 20:36:29.705 DEBUG: Taking gradient step
2022-12-29 20:36:29.714 DEBUG: Loss 3: {'policy_loss': -0.04531676745905988, 'entropy_loss': -0.02652442641556263, 'vf_loss': 0.00010268378854834177, 'total_loss': -0.07173851008607417, 'approx_kl': 0.009763500420376658, 'clip_fraction': 0.4361979216337204, 'grad_norm': 8.851985931396484}
2022-12-29 20:36:30.551 DEBUG: Taking gradient step
2022-12-29 20:36:30.561 DEBUG: Loss 4: {'policy_loss': 0.01871050160982346, 'entropy_loss': -0.02656201785430312, 'vf_loss': 9.692152519296681e-05, 'total_loss': -0.0077545947192866945, 'approx_kl': 0.040854171849787235, 'clip_fraction': 0.5130208432674408, 'grad_norm': 10.136909484863281}
2022-12-29 20:36:31.385 DEBUG: Early stopping at step 5 for reaching max KL.
2022-12-29 20:36:31.385 INFO: Optimization: policy loss=0.019, vf loss=0.000, entropy loss=-0.027, total loss=-0.008, num steps=5
2022-12-29 20:36:31.386 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:36:32.774 INFO: Evaluation rollout: return=0.797 (0.0), episode length=5.0
2022-12-29 20:36:32.775 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:36:32.777 INFO: Iteration: 98/137, steps: 21168
2022-12-29 20:37:30.080 INFO: Training rollout: return=0.649 (0.1), episode length=5.0
2022-12-29 20:37:30.081 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:37:30.084 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-21168_train.pkl
2022-12-29 20:37:30.922 DEBUG: Taking gradient step
2022-12-29 20:37:30.933 DEBUG: Loss 0: {'policy_loss': -0.0013548812656008881, 'entropy_loss': -0.026715016458183527, 'vf_loss': 9.974791946484311e-05, 'total_loss': -0.02797014980431957, 'approx_kl': -5.3939093191957e-09, 'clip_fraction': 0.0, 'grad_norm': 12.008757591247559}
2022-12-29 20:37:31.719 DEBUG: Taking gradient step
2022-12-29 20:37:31.728 DEBUG: Loss 1: {'policy_loss': -0.04352612644449669, 'entropy_loss': -0.02573795709758997, 'vf_loss': 0.00010530700522538841, 'total_loss': -0.06915877653686126, 'approx_kl': 0.00800350250210613, 'clip_fraction': 0.0078125, 'grad_norm': 12.898407936096191}
2022-12-29 20:37:32.534 DEBUG: Taking gradient step
2022-12-29 20:37:32.543 DEBUG: Loss 2: {'policy_loss': -0.018925128697910625, 'entropy_loss': -0.025765857193619013, 'vf_loss': 9.73312665729156e-05, 'total_loss': -0.04459365462495672, 'approx_kl': 0.02749229595065117, 'clip_fraction': 0.1653645858168602, 'grad_norm': 13.648314476013184}
2022-12-29 20:37:33.340 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 20:37:33.340 INFO: Optimization: policy loss=-0.019, vf loss=0.000, entropy loss=-0.026, total loss=-0.045, num steps=3
2022-12-29 20:37:33.341 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:37:34.754 INFO: Evaluation rollout: return=0.795 (0.0), episode length=5.0
2022-12-29 20:37:34.755 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:37:34.757 INFO: Iteration: 99/137, steps: 21384
2022-12-29 20:38:32.297 INFO: Training rollout: return=0.653 (0.1), episode length=5.0
2022-12-29 20:38:32.299 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:38:32.302 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-21384_train.pkl
2022-12-29 20:38:33.255 DEBUG: Taking gradient step
2022-12-29 20:38:33.264 DEBUG: Loss 0: {'policy_loss': 0.03183729356122553, 'entropy_loss': -0.026284780353307724, 'vf_loss': 8.704631775535074e-05, 'total_loss': 0.005639559525673146, 'approx_kl': 6.449408829212189e-08, 'clip_fraction': 0.0, 'grad_norm': 9.135987281799316}
2022-12-29 20:38:34.081 DEBUG: Taking gradient step
2022-12-29 20:38:34.091 DEBUG: Loss 1: {'policy_loss': -0.006743445064072016, 'entropy_loss': -0.024986709468066692, 'vf_loss': 8.572358346349673e-05, 'total_loss': -0.031644430948675216, 'approx_kl': 0.006691087779472582, 'clip_fraction': 0.05208333395421505, 'grad_norm': 10.22713565826416}
2022-12-29 20:38:34.898 DEBUG: Taking gradient step
2022-12-29 20:38:34.908 DEBUG: Loss 2: {'policy_loss': -0.02326135996089145, 'entropy_loss': -0.024131629150360823, 'vf_loss': 8.73153586923945e-05, 'total_loss': -0.047305673752559875, 'approx_kl': 0.021601594518870115, 'clip_fraction': 0.19661458395421505, 'grad_norm': 14.074718475341797}
2022-12-29 20:38:35.716 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 20:38:35.716 INFO: Optimization: policy loss=-0.023, vf loss=0.000, entropy loss=-0.024, total loss=-0.047, num steps=3
2022-12-29 20:38:35.717 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:38:37.228 INFO: Evaluation rollout: return=0.790 (0.0), episode length=5.0
2022-12-29 20:38:37.230 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:38:37.232 INFO: Iteration: 100/137, steps: 21600
2022-12-29 20:39:35.081 INFO: Training rollout: return=0.662 (0.1), episode length=5.0
2022-12-29 20:39:35.085 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:39:35.088 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-21600_train.pkl
2022-12-29 20:39:35.972 DEBUG: Taking gradient step
2022-12-29 20:39:35.986 DEBUG: Loss 0: {'policy_loss': -0.002794354294662689, 'entropy_loss': -0.02526377374306321, 'vf_loss': 9.55361361170379e-05, 'total_loss': -0.02796259190160886, 'approx_kl': -2.9288154734796734e-08, 'clip_fraction': 0.0, 'grad_norm': 21.203310012817383}
2022-12-29 20:39:36.844 DEBUG: Taking gradient step
2022-12-29 20:39:36.859 DEBUG: Loss 1: {'policy_loss': -0.023178039014526912, 'entropy_loss': -0.023570983204990625, 'vf_loss': 8.81984037436966e-05, 'total_loss': -0.04666082381577384, 'approx_kl': 0.009425051470316248, 'clip_fraction': 0.04817708395421505, 'grad_norm': 7.73635196685791}
2022-12-29 20:39:37.750 DEBUG: Taking gradient step
2022-12-29 20:39:37.760 DEBUG: Loss 2: {'policy_loss': -0.009637817966891742, 'entropy_loss': -0.023465663194656372, 'vf_loss': 9.012546220638407e-05, 'total_loss': -0.03301335569934173, 'approx_kl': 0.024628792190924287, 'clip_fraction': 0.19140625, 'grad_norm': 11.34798812866211}
2022-12-29 20:39:38.572 DEBUG: Taking gradient step
2022-12-29 20:39:38.582 DEBUG: Loss 3: {'policy_loss': -0.0023981517869583364, 'entropy_loss': -0.022757406812161207, 'vf_loss': 8.217388451962908e-05, 'total_loss': -0.025073384714599918, 'approx_kl': 0.039982898626476526, 'clip_fraction': 0.2434895858168602, 'grad_norm': 10.027379035949707}
2022-12-29 20:39:39.483 DEBUG: Taking gradient step
2022-12-29 20:39:39.494 DEBUG: Loss 4: {'policy_loss': 0.01100319863707374, 'entropy_loss': -0.02220105193555355, 'vf_loss': 8.199090662624579e-05, 'total_loss': -0.011115862391853568, 'approx_kl': 0.03801613952964544, 'clip_fraction': 0.2265625, 'grad_norm': 8.881780624389648}
2022-12-29 20:39:40.333 DEBUG: Taking gradient step
2022-12-29 20:39:40.343 DEBUG: Loss 5: {'policy_loss': 0.0013398929709842675, 'entropy_loss': -0.02276319731026888, 'vf_loss': 7.598479780692935e-05, 'total_loss': -0.021347319541477686, 'approx_kl': 0.03254362661391497, 'clip_fraction': 0.2278645858168602, 'grad_norm': 9.497313499450684}
2022-12-29 20:39:41.180 DEBUG: Taking gradient step
2022-12-29 20:39:41.189 DEBUG: Loss 6: {'policy_loss': -0.011889628336601852, 'entropy_loss': -0.022298942785710096, 'vf_loss': 7.79059879965122e-05, 'total_loss': -0.03411066513431543, 'approx_kl': 0.022301160730421543, 'clip_fraction': 0.11067708395421505, 'grad_norm': 7.841458320617676}
2022-12-29 20:39:42.006 DEBUG: Taking gradient step
2022-12-29 20:39:42.016 DEBUG: Loss 7: {'policy_loss': -0.03655552757844828, 'entropy_loss': -0.021340780425816774, 'vf_loss': 7.583236070136292e-05, 'total_loss': -0.05782047564356369, 'approx_kl': 0.015600008191540837, 'clip_fraction': 0.10677083395421505, 'grad_norm': 17.068384170532227}
2022-12-29 20:39:42.823 DEBUG: Taking gradient step
2022-12-29 20:39:42.833 DEBUG: Loss 8: {'policy_loss': -0.02047969331963393, 'entropy_loss': -0.0212174691259861, 'vf_loss': 7.25913601745726e-05, 'total_loss': -0.041624571085445455, 'approx_kl': 0.02305845101363957, 'clip_fraction': 0.10677083395421505, 'grad_norm': 13.825172424316406}
2022-12-29 20:39:43.650 DEBUG: Taking gradient step
2022-12-29 20:39:43.659 DEBUG: Loss 9: {'policy_loss': -0.020003176473948166, 'entropy_loss': -0.021337437443435192, 'vf_loss': 6.698968486296707e-05, 'total_loss': -0.041273624232520394, 'approx_kl': 0.02555067278444767, 'clip_fraction': 0.1744791679084301, 'grad_norm': 13.734260559082031}
2022-12-29 20:39:43.660 INFO: Optimization: policy loss=-0.020, vf loss=0.000, entropy loss=-0.021, total loss=-0.041, num steps=10
2022-12-29 20:39:43.660 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:39:45.069 INFO: Evaluation rollout: return=0.793 (0.0), episode length=5.0
2022-12-29 20:39:45.070 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:39:45.073 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-19656.model
2022-12-29 20:39:45.076 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-21816.model
2022-12-29 20:39:45.107 INFO: Iteration: 101/137, steps: 21816
2022-12-29 20:40:14.445 DEBUG: There is a single atom floating around
2022-12-29 20:40:43.027 INFO: Training rollout: return=0.114 (3.3), episode length=5.0
2022-12-29 20:40:43.029 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:40:43.032 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-21816_train.pkl
2022-12-29 20:40:43.885 DEBUG: Taking gradient step
2022-12-29 20:40:43.895 DEBUG: Loss 0: {'policy_loss': 0.10369498395531439, 'entropy_loss': -0.02075451659038663, 'vf_loss': 0.015946593879709463, 'total_loss': 0.09888706124463721, 'approx_kl': 1.0011717677116394e-08, 'clip_fraction': 0.0, 'grad_norm': 24.91898536682129}
2022-12-29 20:40:44.713 DEBUG: Taking gradient step
2022-12-29 20:40:44.724 DEBUG: Loss 1: {'policy_loss': -0.035229942930901814, 'entropy_loss': -0.020007609855383635, 'vf_loss': 0.00799021082156415, 'total_loss': -0.0472473419647213, 'approx_kl': 0.010086713824421167, 'clip_fraction': 0.05989583395421505, 'grad_norm': 12.891225814819336}
2022-12-29 20:40:45.606 DEBUG: Taking gradient step
2022-12-29 20:40:45.615 DEBUG: Loss 2: {'policy_loss': -0.006993491899629349, 'entropy_loss': -0.02037117723375559, 'vf_loss': 0.010608528211123205, 'total_loss': -0.016756140922261735, 'approx_kl': 0.044412252493202686, 'clip_fraction': 0.2369791716337204, 'grad_norm': 3.371044635772705}
2022-12-29 20:40:46.424 DEBUG: Taking gradient step
2022-12-29 20:40:46.434 DEBUG: Loss 3: {'policy_loss': -0.004528339418987549, 'entropy_loss': -0.02104468271136284, 'vf_loss': 0.010662193552066029, 'total_loss': -0.014910828578284359, 'approx_kl': 0.038661541999317706, 'clip_fraction': 0.25, 'grad_norm': 1.813110113143921}
2022-12-29 20:40:47.262 DEBUG: Taking gradient step
2022-12-29 20:40:47.275 DEBUG: Loss 4: {'policy_loss': -0.04119294506536417, 'entropy_loss': -0.02119760075584054, 'vf_loss': 0.007960518025523612, 'total_loss': -0.054430027795681096, 'approx_kl': 0.04088874376611784, 'clip_fraction': 0.20572916697710752, 'grad_norm': 1.1964051723480225}
2022-12-29 20:40:48.119 DEBUG: Taking gradient step
2022-12-29 20:40:48.128 DEBUG: Loss 5: {'policy_loss': -0.03794456590200811, 'entropy_loss': -0.0214075674302876, 'vf_loss': 0.007930462135334215, 'total_loss': -0.05142167119696149, 'approx_kl': 0.044589378172531724, 'clip_fraction': 0.3333333358168602, 'grad_norm': 2.511016607284546}
2022-12-29 20:40:48.968 DEBUG: Taking gradient step
2022-12-29 20:40:48.978 DEBUG: Loss 6: {'policy_loss': 0.03550135113910767, 'entropy_loss': -0.022975058294832706, 'vf_loss': 0.013141439035912927, 'total_loss': 0.025667731880187894, 'approx_kl': 0.039795870427042246, 'clip_fraction': 0.4153645858168602, 'grad_norm': 6.868782043457031}
2022-12-29 20:40:49.822 DEBUG: Taking gradient step
2022-12-29 20:40:49.831 DEBUG: Loss 7: {'policy_loss': -0.037995696417909265, 'entropy_loss': -0.023006737232208252, 'vf_loss': 0.00790146590424743, 'total_loss': -0.05310096774587009, 'approx_kl': 0.039339710492640734, 'clip_fraction': 0.4114583358168602, 'grad_norm': 6.897913932800293}
2022-12-29 20:40:50.657 DEBUG: Taking gradient step
2022-12-29 20:40:50.672 DEBUG: Loss 8: {'policy_loss': -0.03879895020675613, 'entropy_loss': -0.02321817772462964, 'vf_loss': 0.00789425924766529, 'total_loss': -0.05412286868372048, 'approx_kl': 0.043616005685180426, 'clip_fraction': 0.3958333358168602, 'grad_norm': 3.6183278560638428}
2022-12-29 20:40:51.529 DEBUG: Taking gradient step
2022-12-29 20:40:51.538 DEBUG: Loss 9: {'policy_loss': -0.04427354721793357, 'entropy_loss': -0.02398773143067956, 'vf_loss': 0.007885407056071162, 'total_loss': -0.06037587159254198, 'approx_kl': 0.015842710621654987, 'clip_fraction': 0.2981770858168602, 'grad_norm': 5.141448974609375}
2022-12-29 20:40:51.538 INFO: Optimization: policy loss=-0.044, vf loss=0.008, entropy loss=-0.024, total loss=-0.060, num steps=10
2022-12-29 20:40:51.538 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:40:53.008 INFO: Evaluation rollout: return=0.793 (0.0), episode length=5.0
2022-12-29 20:40:53.009 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:40:53.011 INFO: Iteration: 102/137, steps: 22032
2022-12-29 20:41:50.377 INFO: Training rollout: return=0.654 (0.1), episode length=5.0
2022-12-29 20:41:50.379 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:41:50.382 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-22032_train.pkl
2022-12-29 20:41:51.220 DEBUG: Taking gradient step
2022-12-29 20:41:51.230 DEBUG: Loss 0: {'policy_loss': 0.019762979452154248, 'entropy_loss': -0.025245685130357742, 'vf_loss': 0.00018930278681331223, 'total_loss': -0.005293402891390183, 'approx_kl': 1.6569780658670652e-08, 'clip_fraction': 0.0, 'grad_norm': 10.430700302124023}
2022-12-29 20:41:52.040 DEBUG: Taking gradient step
2022-12-29 20:41:52.049 DEBUG: Loss 1: {'policy_loss': 0.029787951684124972, 'entropy_loss': -0.026318590622395277, 'vf_loss': 0.0002112646656510125, 'total_loss': 0.003680625727380711, 'approx_kl': -0.005725574359530583, 'clip_fraction': 0.06770833395421505, 'grad_norm': 13.967341423034668}
2022-12-29 20:41:52.853 DEBUG: Taking gradient step
2022-12-29 20:41:52.862 DEBUG: Loss 2: {'policy_loss': -0.023482020008969505, 'entropy_loss': -0.025799467228353024, 'vf_loss': 0.00023993193066731473, 'total_loss': -0.049041555306655216, 'approx_kl': -0.013987591140903533, 'clip_fraction': 0.12630208395421505, 'grad_norm': 6.85319709777832}
2022-12-29 20:41:53.652 DEBUG: Taking gradient step
2022-12-29 20:41:53.661 DEBUG: Loss 3: {'policy_loss': -0.01586769854191051, 'entropy_loss': -0.02585027227178216, 'vf_loss': 0.00025644143611575857, 'total_loss': -0.04146152937757691, 'approx_kl': -0.004105875268578529, 'clip_fraction': 0.1328125, 'grad_norm': 11.143397331237793}
2022-12-29 20:41:54.556 DEBUG: Taking gradient step
2022-12-29 20:41:54.566 DEBUG: Loss 4: {'policy_loss': 0.06923501141929643, 'entropy_loss': -0.02558121783658862, 'vf_loss': 0.000248769282397094, 'total_loss': 0.04390256286510491, 'approx_kl': 0.0006815652595832944, 'clip_fraction': 0.17838541697710752, 'grad_norm': 14.86550235748291}
2022-12-29 20:41:55.405 DEBUG: Taking gradient step
2022-12-29 20:41:55.414 DEBUG: Loss 5: {'policy_loss': 0.037252383800272536, 'entropy_loss': -0.025747438427060843, 'vf_loss': 0.00026664474610428395, 'total_loss': 0.01177159011931598, 'approx_kl': 0.0020129792392253876, 'clip_fraction': 0.2434895858168602, 'grad_norm': 35.48937225341797}
2022-12-29 20:41:56.212 DEBUG: Taking gradient step
2022-12-29 20:41:56.222 DEBUG: Loss 6: {'policy_loss': 0.024960379414248737, 'entropy_loss': -0.026401834096759558, 'vf_loss': 0.0002715582276699842, 'total_loss': -0.0011698964548408283, 'approx_kl': 0.012268532067537308, 'clip_fraction': 0.2252604216337204, 'grad_norm': 15.706379890441895}
2022-12-29 20:41:57.094 DEBUG: Taking gradient step
2022-12-29 20:41:57.108 DEBUG: Loss 7: {'policy_loss': -0.04055349994036893, 'entropy_loss': -0.025594611652195454, 'vf_loss': 0.0002837950652767865, 'total_loss': -0.06586431652728761, 'approx_kl': -0.0017838417552411556, 'clip_fraction': 0.2708333358168602, 'grad_norm': 13.87963581085205}
2022-12-29 20:41:57.996 DEBUG: Taking gradient step
2022-12-29 20:41:58.006 DEBUG: Loss 8: {'policy_loss': -0.008437528617921361, 'entropy_loss': -0.024536955170333385, 'vf_loss': 0.00027328947577605334, 'total_loss': -0.032701194312478685, 'approx_kl': 0.024042313918471336, 'clip_fraction': 0.2708333358168602, 'grad_norm': 17.480955123901367}
2022-12-29 20:41:58.808 DEBUG: Taking gradient step
2022-12-29 20:41:58.817 DEBUG: Loss 9: {'policy_loss': 0.000854289566287799, 'entropy_loss': -0.0250917156226933, 'vf_loss': 0.00026317646718169703, 'total_loss': -0.023974249589223805, 'approx_kl': 0.00831600558012724, 'clip_fraction': 0.21875, 'grad_norm': 12.932405471801758}
2022-12-29 20:41:58.817 INFO: Optimization: policy loss=0.001, vf loss=0.000, entropy loss=-0.025, total loss=-0.024, num steps=10
2022-12-29 20:41:58.818 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:42:00.233 INFO: Evaluation rollout: return=0.795 (0.0), episode length=5.0
2022-12-29 20:42:00.234 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:42:00.236 INFO: Iteration: 103/137, steps: 22248
2022-12-29 20:42:58.620 INFO: Training rollout: return=0.657 (0.1), episode length=5.0
2022-12-29 20:42:58.622 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:42:58.624 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-22248_train.pkl
2022-12-29 20:42:59.468 DEBUG: Taking gradient step
2022-12-29 20:42:59.477 DEBUG: Loss 0: {'policy_loss': 0.019693272619036598, 'entropy_loss': -0.024209631141275167, 'vf_loss': 0.00025609068483843737, 'total_loss': -0.0042602678374001335, 'approx_kl': 1.5172798129015064e-08, 'clip_fraction': 0.0, 'grad_norm': 6.59596586227417}
2022-12-29 20:43:00.274 DEBUG: Taking gradient step
2022-12-29 20:43:00.285 DEBUG: Loss 1: {'policy_loss': 0.03196441421022835, 'entropy_loss': -0.023373147938400507, 'vf_loss': 0.00024293884405637633, 'total_loss': 0.008834205115884224, 'approx_kl': 0.003876113099977374, 'clip_fraction': 0.09635416697710752, 'grad_norm': 18.921255111694336}
2022-12-29 20:43:01.083 DEBUG: Taking gradient step
2022-12-29 20:43:01.092 DEBUG: Loss 2: {'policy_loss': -0.04175609233096842, 'entropy_loss': -0.023365276399999857, 'vf_loss': 0.00023662315303558156, 'total_loss': -0.06488474557793271, 'approx_kl': 0.009756168234162033, 'clip_fraction': 0.19140625, 'grad_norm': 16.40189552307129}
2022-12-29 20:43:01.919 DEBUG: Taking gradient step
2022-12-29 20:43:01.928 DEBUG: Loss 3: {'policy_loss': -0.021113937859890037, 'entropy_loss': -0.022150643169879913, 'vf_loss': 0.00022080106780203755, 'total_loss': -0.04304377996196791, 'approx_kl': 0.013550880365073681, 'clip_fraction': 0.1875, 'grad_norm': 15.447927474975586}
2022-12-29 20:43:02.763 DEBUG: Taking gradient step
2022-12-29 20:43:02.772 DEBUG: Loss 4: {'policy_loss': 0.01950732883758184, 'entropy_loss': -0.022039924282580614, 'vf_loss': 0.00020380543750071233, 'total_loss': -0.002328790007498062, 'approx_kl': 0.0015316188801079988, 'clip_fraction': 0.1119791679084301, 'grad_norm': 9.766532897949219}
2022-12-29 20:43:03.563 DEBUG: Taking gradient step
2022-12-29 20:43:03.577 DEBUG: Loss 5: {'policy_loss': -0.027878088236875653, 'entropy_loss': -0.02112560300156474, 'vf_loss': 0.0001941249563056475, 'total_loss': -0.048809566282134745, 'approx_kl': 0.004191098967567086, 'clip_fraction': 0.10546875, 'grad_norm': 8.807559967041016}
2022-12-29 20:43:04.420 DEBUG: Taking gradient step
2022-12-29 20:43:04.431 DEBUG: Loss 6: {'policy_loss': -0.0760279017560502, 'entropy_loss': -0.020794250536710024, 'vf_loss': 0.00018868455993441779, 'total_loss': -0.0966334677328258, 'approx_kl': 0.010880644898861647, 'clip_fraction': 0.2278645858168602, 'grad_norm': 21.964506149291992}
2022-12-29 20:43:05.252 DEBUG: Taking gradient step
2022-12-29 20:43:05.261 DEBUG: Loss 7: {'policy_loss': -0.0012360705689833072, 'entropy_loss': -0.021272385492920876, 'vf_loss': 0.00016223371194710104, 'total_loss': -0.022346222349957083, 'approx_kl': 0.01088999246712774, 'clip_fraction': 0.2356770858168602, 'grad_norm': 20.2305850982666}
2022-12-29 20:43:06.114 DEBUG: Taking gradient step
2022-12-29 20:43:06.129 DEBUG: Loss 8: {'policy_loss': -0.020932782051570998, 'entropy_loss': -0.02074116049334407, 'vf_loss': 0.00015091289764743575, 'total_loss': -0.04152302964726763, 'approx_kl': 0.014552908018231392, 'clip_fraction': 0.1471354179084301, 'grad_norm': 12.445541381835938}
2022-12-29 20:43:07.002 DEBUG: Taking gradient step
2022-12-29 20:43:07.011 DEBUG: Loss 9: {'policy_loss': 0.023657827148204448, 'entropy_loss': -0.01984456554055214, 'vf_loss': 0.00013713942047501496, 'total_loss': 0.003950401028127326, 'approx_kl': 0.01617606997024268, 'clip_fraction': 0.16796875, 'grad_norm': 14.985320091247559}
2022-12-29 20:43:07.012 INFO: Optimization: policy loss=0.024, vf loss=0.000, entropy loss=-0.020, total loss=0.004, num steps=10
2022-12-29 20:43:07.012 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:43:08.486 INFO: Evaluation rollout: return=0.792 (0.0), episode length=5.0
2022-12-29 20:43:08.487 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:43:08.489 INFO: Iteration: 104/137, steps: 22464
2022-12-29 20:44:05.977 INFO: Training rollout: return=0.695 (0.1), episode length=5.0
2022-12-29 20:44:05.979 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:44:05.982 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-22464_train.pkl
2022-12-29 20:44:06.821 DEBUG: Taking gradient step
2022-12-29 20:44:06.831 DEBUG: Loss 0: {'policy_loss': -0.03173724072767816, 'entropy_loss': -0.019236477557569742, 'vf_loss': 0.00014370893650162304, 'total_loss': -0.05083000934874628, 'approx_kl': -1.0787819748614424e-08, 'clip_fraction': 0.0, 'grad_norm': 6.996486663818359}
2022-12-29 20:44:07.674 DEBUG: Taking gradient step
2022-12-29 20:44:07.684 DEBUG: Loss 1: {'policy_loss': -0.050382544479184596, 'entropy_loss': -0.01903815846890211, 'vf_loss': 0.0001343803716919535, 'total_loss': -0.06928632257639475, 'approx_kl': -0.002716984017752111, 'clip_fraction': 0.09375, 'grad_norm': 5.113583564758301}
2022-12-29 20:44:08.521 DEBUG: Taking gradient step
2022-12-29 20:44:08.532 DEBUG: Loss 2: {'policy_loss': 0.0013336658781577838, 'entropy_loss': -0.01870626211166382, 'vf_loss': 0.0001145901924458009, 'total_loss': -0.017258006041060234, 'approx_kl': 0.00808408996090293, 'clip_fraction': 0.23046875, 'grad_norm': 10.564227104187012}
2022-12-29 20:44:09.363 DEBUG: Taking gradient step
2022-12-29 20:44:09.375 DEBUG: Loss 3: {'policy_loss': -0.03174821054158, 'entropy_loss': -0.01868837047368288, 'vf_loss': 0.00010672253758086865, 'total_loss': -0.05032985847768201, 'approx_kl': 0.00565876648761332, 'clip_fraction': 0.2630208358168602, 'grad_norm': 10.521219253540039}
2022-12-29 20:44:10.183 DEBUG: Taking gradient step
2022-12-29 20:44:10.193 DEBUG: Loss 4: {'policy_loss': -0.032678795464398, 'entropy_loss': -0.01807112619280815, 'vf_loss': 9.671407331283723e-05, 'total_loss': -0.050653207583893314, 'approx_kl': 0.005466870730742812, 'clip_fraction': 0.2513020858168602, 'grad_norm': 10.80945873260498}
2022-12-29 20:44:11.038 DEBUG: Taking gradient step
2022-12-29 20:44:11.049 DEBUG: Loss 5: {'policy_loss': -0.001164605291586563, 'entropy_loss': -0.01779310405254364, 'vf_loss': 8.458516241493551e-05, 'total_loss': -0.01887312418171527, 'approx_kl': 0.007678119814954698, 'clip_fraction': 0.23046875, 'grad_norm': 5.761590480804443}
2022-12-29 20:44:11.848 DEBUG: Taking gradient step
2022-12-29 20:44:11.857 DEBUG: Loss 6: {'policy_loss': 0.03950965266151437, 'entropy_loss': -0.016814043512567878, 'vf_loss': 7.579325747424963e-05, 'total_loss': 0.022771402406420754, 'approx_kl': 0.015676797658670694, 'clip_fraction': 0.0859375, 'grad_norm': 2.3732895851135254}
2022-12-29 20:44:12.673 DEBUG: Taking gradient step
2022-12-29 20:44:12.682 DEBUG: Loss 7: {'policy_loss': -0.025874915130385076, 'entropy_loss': -0.0170736750587821, 'vf_loss': 7.345110718428209e-05, 'total_loss': -0.0428751390819829, 'approx_kl': 0.02197042608167976, 'clip_fraction': 0.046875, 'grad_norm': 5.306346893310547}
2022-12-29 20:44:13.548 DEBUG: Taking gradient step
2022-12-29 20:44:13.562 DEBUG: Loss 8: {'policy_loss': 0.03878917480586724, 'entropy_loss': -0.017829146701842546, 'vf_loss': 6.356951065004166e-05, 'total_loss': 0.021023597614674733, 'approx_kl': 0.030147801619023085, 'clip_fraction': 0.1796875, 'grad_norm': 5.388863563537598}
2022-12-29 20:44:14.386 DEBUG: Taking gradient step
2022-12-29 20:44:14.396 DEBUG: Loss 9: {'policy_loss': -0.021315520189288548, 'entropy_loss': -0.017311030067503452, 'vf_loss': 6.61492715611335e-05, 'total_loss': -0.03856040098523087, 'approx_kl': 0.02825188683345914, 'clip_fraction': 0.2395833358168602, 'grad_norm': 5.704867839813232}
2022-12-29 20:44:14.396 INFO: Optimization: policy loss=-0.021, vf loss=0.000, entropy loss=-0.017, total loss=-0.039, num steps=10
2022-12-29 20:44:14.397 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:44:15.950 INFO: Evaluation rollout: return=0.795 (0.0), episode length=5.0
2022-12-29 20:44:15.951 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:44:15.953 INFO: Iteration: 105/137, steps: 22680
2022-12-29 20:45:13.880 INFO: Training rollout: return=0.705 (0.1), episode length=5.0
2022-12-29 20:45:13.882 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:45:13.884 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-22680_train.pkl
2022-12-29 20:45:14.736 DEBUG: Taking gradient step
2022-12-29 20:45:14.746 DEBUG: Loss 0: {'policy_loss': -0.003326247858047253, 'entropy_loss': -0.017451745457947254, 'vf_loss': 7.91741181893538e-05, 'total_loss': -0.020698819197805148, 'approx_kl': 4.268564168796729e-09, 'clip_fraction': 0.0, 'grad_norm': 25.210254669189453}
2022-12-29 20:45:15.573 DEBUG: Taking gradient step
2022-12-29 20:45:15.584 DEBUG: Loss 1: {'policy_loss': 0.046191972320625344, 'entropy_loss': -0.01832543988712132, 'vf_loss': 7.57367553749368e-05, 'total_loss': 0.02794226918887896, 'approx_kl': 0.010022666188888252, 'clip_fraction': 0.00390625, 'grad_norm': 15.828163146972656}
2022-12-29 20:45:16.379 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 20:45:16.380 INFO: Optimization: policy loss=0.046, vf loss=0.000, entropy loss=-0.018, total loss=0.028, num steps=2
2022-12-29 20:45:16.380 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:45:17.876 INFO: Evaluation rollout: return=0.796 (0.0), episode length=5.0
2022-12-29 20:45:17.878 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:45:17.881 INFO: Iteration: 106/137, steps: 22896
2022-12-29 20:46:16.687 INFO: Training rollout: return=0.680 (0.1), episode length=5.0
2022-12-29 20:46:16.689 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:46:16.692 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-22896_train.pkl
2022-12-29 20:46:17.524 DEBUG: Taking gradient step
2022-12-29 20:46:17.533 DEBUG: Loss 0: {'policy_loss': 0.05709607531276463, 'entropy_loss': -0.01762595819309354, 'vf_loss': 8.370311297117801e-05, 'total_loss': 0.03955382023264226, 'approx_kl': 3.57007223783512e-09, 'clip_fraction': 0.0, 'grad_norm': 25.120445251464844}
2022-12-29 20:46:18.373 DEBUG: Taking gradient step
2022-12-29 20:46:18.383 DEBUG: Loss 1: {'policy_loss': 0.0008277542042678752, 'entropy_loss': -0.01793017704039812, 'vf_loss': 7.964679483114813e-05, 'total_loss': -0.017022776041299096, 'approx_kl': 0.014215164352208376, 'clip_fraction': 0.17317708395421505, 'grad_norm': 15.510313987731934}
2022-12-29 20:46:19.202 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 20:46:19.202 INFO: Optimization: policy loss=0.001, vf loss=0.000, entropy loss=-0.018, total loss=-0.017, num steps=2
2022-12-29 20:46:19.202 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:46:20.604 INFO: Evaluation rollout: return=0.790 (0.0), episode length=5.0
2022-12-29 20:46:20.605 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:46:20.607 INFO: Iteration: 107/137, steps: 23112
2022-12-29 20:46:57.350 DEBUG: There is a single atom floating around
2022-12-29 20:47:17.853 INFO: Training rollout: return=0.144 (3.4), episode length=4.9
2022-12-29 20:47:17.855 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:47:17.858 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-23112_train.pkl
2022-12-29 20:47:18.719 DEBUG: Taking gradient step
2022-12-29 20:47:18.728 DEBUG: Loss 0: {'policy_loss': -0.012050523920311383, 'entropy_loss': -0.018776375334709883, 'vf_loss': 0.0033183753661069236, 'total_loss': -0.02750852388891434, 'approx_kl': -1.6298145055770874e-08, 'clip_fraction': 0.0, 'grad_norm': 3.0512778759002686}
2022-12-29 20:47:19.549 DEBUG: Taking gradient step
2022-12-29 20:47:19.558 DEBUG: Loss 1: {'policy_loss': -0.02715155274449188, 'entropy_loss': -0.018057528883218765, 'vf_loss': 0.0033195813839015597, 'total_loss': -0.04188950024380908, 'approx_kl': 0.007154153077863157, 'clip_fraction': 0.04296875, 'grad_norm': 1.4623732566833496}
2022-12-29 20:47:20.377 DEBUG: Taking gradient step
2022-12-29 20:47:20.388 DEBUG: Loss 2: {'policy_loss': -0.025898006569185665, 'entropy_loss': -0.01813353644683957, 'vf_loss': 0.0033127464527244617, 'total_loss': -0.040718796563300776, 'approx_kl': 0.0247065715957433, 'clip_fraction': 0.13932291697710752, 'grad_norm': 1.8691874742507935}
2022-12-29 20:47:21.186 DEBUG: Taking gradient step
2022-12-29 20:47:21.196 DEBUG: Loss 3: {'policy_loss': -0.019058120586475394, 'entropy_loss': -0.018660174682736397, 'vf_loss': 0.0033030354811928436, 'total_loss': -0.034415259788018945, 'approx_kl': 0.03114410489797592, 'clip_fraction': 0.203125, 'grad_norm': 2.0839853286743164}
2022-12-29 20:47:22.011 DEBUG: Taking gradient step
2022-12-29 20:47:22.022 DEBUG: Loss 4: {'policy_loss': -0.027230327893455475, 'entropy_loss': -0.01850429829210043, 'vf_loss': 0.0033008448903391976, 'total_loss': -0.042433781295216716, 'approx_kl': 0.03783058316912502, 'clip_fraction': 0.15234375, 'grad_norm': 1.5731921195983887}
2022-12-29 20:47:22.831 DEBUG: Taking gradient step
2022-12-29 20:47:22.840 DEBUG: Loss 5: {'policy_loss': -0.029218215028648766, 'entropy_loss': -0.01944822771474719, 'vf_loss': 0.0032935920459539057, 'total_loss': -0.04537285069744204, 'approx_kl': 0.019298374885693192, 'clip_fraction': 0.049479166977107525, 'grad_norm': 1.0689400434494019}
2022-12-29 20:47:23.651 DEBUG: Taking gradient step
2022-12-29 20:47:23.660 DEBUG: Loss 6: {'policy_loss': -0.025067636897242083, 'entropy_loss': -0.01972817676141858, 'vf_loss': 0.0032849216547764354, 'total_loss': -0.04151089200388423, 'approx_kl': 0.022319819312542677, 'clip_fraction': 0.14973958395421505, 'grad_norm': 1.8688374757766724}
2022-12-29 20:47:24.452 DEBUG: Early stopping at step 7 for reaching max KL.
2022-12-29 20:47:24.453 INFO: Optimization: policy loss=-0.025, vf loss=0.003, entropy loss=-0.020, total loss=-0.042, num steps=7
2022-12-29 20:47:24.453 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:47:25.734 INFO: Evaluation rollout: return=0.791 (0.0), episode length=5.0
2022-12-29 20:47:25.736 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:47:25.739 INFO: Iteration: 108/137, steps: 23328
2022-12-29 20:48:23.603 INFO: Training rollout: return=0.702 (0.1), episode length=5.0
2022-12-29 20:48:23.605 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:48:23.607 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-23328_train.pkl
2022-12-29 20:48:24.430 DEBUG: Taking gradient step
2022-12-29 20:48:24.440 DEBUG: Loss 0: {'policy_loss': 0.01292045707940536, 'entropy_loss': -0.020143714267760515, 'vf_loss': 5.217197191583156e-05, 'total_loss': -0.0071710852164393254, 'approx_kl': 3.20918238649881e-08, 'clip_fraction': 0.0, 'grad_norm': 9.382052421569824}
2022-12-29 20:48:25.251 DEBUG: Taking gradient step
2022-12-29 20:48:25.260 DEBUG: Loss 1: {'policy_loss': -0.004815014287203315, 'entropy_loss': -0.02174102421849966, 'vf_loss': 5.3884656133886665e-05, 'total_loss': -0.026502153849569096, 'approx_kl': 0.008224505465477705, 'clip_fraction': 0.04817708395421505, 'grad_norm': 16.07867431640625}
2022-12-29 20:48:26.065 DEBUG: Taking gradient step
2022-12-29 20:48:26.075 DEBUG: Loss 2: {'policy_loss': -0.026372389382890077, 'entropy_loss': -0.02120684552937746, 'vf_loss': 5.1984109114586137e-05, 'total_loss': -0.047527250803152954, 'approx_kl': 0.01823374198284, 'clip_fraction': 0.1966145858168602, 'grad_norm': 11.8799467086792}
2022-12-29 20:48:26.879 DEBUG: Taking gradient step
2022-12-29 20:48:26.889 DEBUG: Loss 3: {'policy_loss': -0.0073391258503263035, 'entropy_loss': -0.02249556966125965, 'vf_loss': 5.662570506841023e-05, 'total_loss': -0.029778069806517548, 'approx_kl': 0.029668180970475078, 'clip_fraction': 0.34765625, 'grad_norm': 11.929240226745605}
2022-12-29 20:48:27.710 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 20:48:27.711 INFO: Optimization: policy loss=-0.007, vf loss=0.000, entropy loss=-0.022, total loss=-0.030, num steps=4
2022-12-29 20:48:27.711 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:48:29.089 INFO: Evaluation rollout: return=0.791 (0.0), episode length=5.0
2022-12-29 20:48:29.090 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:48:29.093 INFO: Iteration: 109/137, steps: 23544
2022-12-29 20:49:26.965 INFO: Training rollout: return=0.692 (0.1), episode length=5.0
2022-12-29 20:49:26.967 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:49:26.969 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-23544_train.pkl
2022-12-29 20:49:27.783 DEBUG: Taking gradient step
2022-12-29 20:49:27.798 DEBUG: Loss 0: {'policy_loss': -0.012746426669817793, 'entropy_loss': -0.022981504909694195, 'vf_loss': 4.468171667511757e-05, 'total_loss': -0.03568324986283687, 'approx_kl': 2.9705311987981986e-08, 'clip_fraction': 0.0, 'grad_norm': 6.808571815490723}
2022-12-29 20:49:28.641 DEBUG: Taking gradient step
2022-12-29 20:49:28.651 DEBUG: Loss 1: {'policy_loss': 0.01796034164880979, 'entropy_loss': -0.023481848184019327, 'vf_loss': 4.386911422672502e-05, 'total_loss': -0.005477637420982815, 'approx_kl': 0.005677727283909917, 'clip_fraction': 0.1236979179084301, 'grad_norm': 12.549696922302246}
2022-12-29 20:49:29.574 DEBUG: Taking gradient step
2022-12-29 20:49:29.588 DEBUG: Loss 2: {'policy_loss': 0.022676884186543497, 'entropy_loss': -0.024645206052809954, 'vf_loss': 4.417951129881531e-05, 'total_loss': -0.0019241423549676388, 'approx_kl': 0.01652904786169529, 'clip_fraction': 0.2643229216337204, 'grad_norm': 14.323993682861328}
2022-12-29 20:49:30.472 DEBUG: Taking gradient step
2022-12-29 20:49:30.482 DEBUG: Loss 3: {'policy_loss': -0.03229584483838301, 'entropy_loss': -0.023422015365213156, 'vf_loss': 4.7851655249740114e-05, 'total_loss': -0.055670008548346424, 'approx_kl': 0.019105629762634635, 'clip_fraction': 0.1966145858168602, 'grad_norm': 11.003805160522461}
2022-12-29 20:49:31.308 DEBUG: Taking gradient step
2022-12-29 20:49:31.317 DEBUG: Loss 4: {'policy_loss': -0.00716589300955818, 'entropy_loss': -0.02384579088538885, 'vf_loss': 4.85549801524605e-05, 'total_loss': -0.030963128914794573, 'approx_kl': 0.02501791063696146, 'clip_fraction': 0.14583333395421505, 'grad_norm': 8.569633483886719}
2022-12-29 20:49:32.119 DEBUG: Taking gradient step
2022-12-29 20:49:32.129 DEBUG: Loss 5: {'policy_loss': 0.02948498653569974, 'entropy_loss': -0.02477457793429494, 'vf_loss': 4.4525285161605116e-05, 'total_loss': 0.0047549338865664065, 'approx_kl': 0.026386308018118143, 'clip_fraction': 0.140625, 'grad_norm': 9.375906944274902}
2022-12-29 20:49:32.929 DEBUG: Taking gradient step
2022-12-29 20:49:32.938 DEBUG: Loss 6: {'policy_loss': -0.028196469420369373, 'entropy_loss': -0.02409176854416728, 'vf_loss': 4.9290947991777604e-05, 'total_loss': -0.052238947016544864, 'approx_kl': 0.020242638420313597, 'clip_fraction': 0.09114583395421505, 'grad_norm': 7.770758628845215}
2022-12-29 20:49:33.785 DEBUG: Taking gradient step
2022-12-29 20:49:33.799 DEBUG: Loss 7: {'policy_loss': 0.0001299976674283736, 'entropy_loss': -0.02507784776389599, 'vf_loss': 4.775484056738016e-05, 'total_loss': -0.024900095255900232, 'approx_kl': 0.026714562671259046, 'clip_fraction': 0.12890625, 'grad_norm': 5.964628219604492}
2022-12-29 20:49:34.635 DEBUG: Taking gradient step
2022-12-29 20:49:34.650 DEBUG: Loss 8: {'policy_loss': -0.030824352948113764, 'entropy_loss': -0.02480002772063017, 'vf_loss': 4.544080615354157e-05, 'total_loss': -0.055578939862590396, 'approx_kl': 0.021988030755892396, 'clip_fraction': 0.13802083395421505, 'grad_norm': 5.216038227081299}
2022-12-29 20:49:35.542 DEBUG: Taking gradient step
2022-12-29 20:49:35.552 DEBUG: Loss 9: {'policy_loss': -0.06030581184678458, 'entropy_loss': -0.02566240495070815, 'vf_loss': 5.009807358701888e-05, 'total_loss': -0.08591811872390571, 'approx_kl': 0.02596573531627655, 'clip_fraction': 0.11197916697710752, 'grad_norm': 4.041182518005371}
2022-12-29 20:49:35.552 INFO: Optimization: policy loss=-0.060, vf loss=0.000, entropy loss=-0.026, total loss=-0.086, num steps=10
2022-12-29 20:49:35.553 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:49:36.978 INFO: Evaluation rollout: return=0.789 (0.0), episode length=5.0
2022-12-29 20:49:36.979 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:49:36.981 INFO: Iteration: 110/137, steps: 23760
2022-12-29 20:49:47.585 DEBUG: There is a single atom floating around
2022-12-29 20:50:34.406 INFO: Training rollout: return=0.078 (3.3), episode length=5.0
2022-12-29 20:50:34.407 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:50:34.409 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-23760_train.pkl
2022-12-29 20:50:35.252 DEBUG: Taking gradient step
2022-12-29 20:50:35.262 DEBUG: Loss 0: {'policy_loss': 0.027463101291808865, 'entropy_loss': -0.024952538777142763, 'vf_loss': 0.009146884066833297, 'total_loss': 0.011657446581499399, 'approx_kl': -3.40223778039217e-08, 'clip_fraction': 0.0, 'grad_norm': 21.60279655456543}
2022-12-29 20:50:36.062 DEBUG: Taking gradient step
2022-12-29 20:50:36.071 DEBUG: Loss 1: {'policy_loss': -0.025883384719993498, 'entropy_loss': -0.02496135700494051, 'vf_loss': 0.006468725122217834, 'total_loss': -0.044376016602716166, 'approx_kl': 0.006847727112472057, 'clip_fraction': 0.04817708395421505, 'grad_norm': 13.998820304870605}
2022-12-29 20:50:36.947 DEBUG: Taking gradient step
2022-12-29 20:50:36.956 DEBUG: Loss 2: {'policy_loss': -0.032552205651282806, 'entropy_loss': -0.024374359752982855, 'vf_loss': 0.006477942220092819, 'total_loss': -0.050448623184172835, 'approx_kl': 0.024884081911295652, 'clip_fraction': 0.2721354179084301, 'grad_norm': 5.863163471221924}
2022-12-29 20:50:37.821 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 20:50:37.822 INFO: Optimization: policy loss=-0.033, vf loss=0.006, entropy loss=-0.024, total loss=-0.050, num steps=3
2022-12-29 20:50:37.822 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:50:39.220 INFO: Evaluation rollout: return=0.785 (0.0), episode length=5.0
2022-12-29 20:50:39.221 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:50:39.223 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-21816.model
2022-12-29 20:50:39.230 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-23976.model
2022-12-29 20:50:39.260 INFO: Iteration: 111/137, steps: 23976
2022-12-29 20:51:37.022 INFO: Training rollout: return=0.671 (0.1), episode length=5.0
2022-12-29 20:51:37.023 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:51:37.026 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-23976_train.pkl
2022-12-29 20:51:37.870 DEBUG: Taking gradient step
2022-12-29 20:51:37.881 DEBUG: Loss 0: {'policy_loss': -0.01238193341002855, 'entropy_loss': -0.02554653026163578, 'vf_loss': 3.3843300648158185e-05, 'total_loss': -0.03789462037101618, 'approx_kl': -7.916241884231567e-09, 'clip_fraction': 0.0, 'grad_norm': 14.433419227600098}
2022-12-29 20:51:38.748 DEBUG: Taking gradient step
2022-12-29 20:51:38.763 DEBUG: Loss 1: {'policy_loss': 0.028983459046510826, 'entropy_loss': -0.025854854378849268, 'vf_loss': 3.2125110979268386e-05, 'total_loss': 0.0031607297786408334, 'approx_kl': 0.002931513823568821, 'clip_fraction': 0.0078125, 'grad_norm': 14.77173900604248}
2022-12-29 20:51:39.658 DEBUG: Taking gradient step
2022-12-29 20:51:39.667 DEBUG: Loss 2: {'policy_loss': 0.01773289128770579, 'entropy_loss': -0.025557418819516897, 'vf_loss': 3.3139583037962e-05, 'total_loss': -0.007791387948773144, 'approx_kl': 0.0036057736724615097, 'clip_fraction': 0.2278645858168602, 'grad_norm': 15.389243125915527}
2022-12-29 20:51:40.560 DEBUG: Taking gradient step
2022-12-29 20:51:40.570 DEBUG: Loss 3: {'policy_loss': 0.049665852935450616, 'entropy_loss': -0.02545434655621648, 'vf_loss': 3.391151966854678e-05, 'total_loss': 0.024245417898902673, 'approx_kl': 0.03525172499939799, 'clip_fraction': 0.30859375, 'grad_norm': 11.30419921875}
2022-12-29 20:51:41.426 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 20:51:41.426 INFO: Optimization: policy loss=0.050, vf loss=0.000, entropy loss=-0.025, total loss=0.024, num steps=4
2022-12-29 20:51:41.427 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:51:42.870 INFO: Evaluation rollout: return=0.796 (0.0), episode length=5.0
2022-12-29 20:51:42.871 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:51:42.874 INFO: Iteration: 112/137, steps: 24192
2022-12-29 20:52:26.414 DEBUG: There is a single atom floating around
2022-12-29 20:52:41.530 INFO: Training rollout: return=0.113 (3.3), episode length=5.0
2022-12-29 20:52:41.532 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:52:41.535 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-24192_train.pkl
2022-12-29 20:52:42.320 DEBUG: Taking gradient step
2022-12-29 20:52:42.329 DEBUG: Loss 0: {'policy_loss': 0.02674528669192336, 'entropy_loss': -0.02547294832766056, 'vf_loss': 0.009139500582730555, 'total_loss': 0.010411838946993358, 'approx_kl': -4.059014457880039e-08, 'clip_fraction': 0.0, 'grad_norm': 13.002376556396484}
2022-12-29 20:52:43.146 DEBUG: Taking gradient step
2022-12-29 20:52:43.157 DEBUG: Loss 1: {'policy_loss': 0.06775217252543013, 'entropy_loss': -0.02501987898722291, 'vf_loss': 0.01185904043729631, 'total_loss': 0.05459133397550353, 'approx_kl': -0.004475550027564168, 'clip_fraction': 0.0234375, 'grad_norm': 3.8168506622314453}
2022-12-29 20:52:44.012 DEBUG: Taking gradient step
2022-12-29 20:52:44.022 DEBUG: Loss 2: {'policy_loss': -0.03240375885694451, 'entropy_loss': -0.02520812675356865, 'vf_loss': 0.006478680588811098, 'total_loss': -0.051133205021702065, 'approx_kl': 0.002622708212584257, 'clip_fraction': 0.1731770858168602, 'grad_norm': 2.5006937980651855}
2022-12-29 20:52:44.811 DEBUG: Taking gradient step
2022-12-29 20:52:44.821 DEBUG: Loss 3: {'policy_loss': 0.014117279917975345, 'entropy_loss': -0.02549383044242859, 'vf_loss': 0.009111481418846075, 'total_loss': -0.0022650691056071703, 'approx_kl': 0.016348352190107107, 'clip_fraction': 0.171875, 'grad_norm': 2.863490343093872}
2022-12-29 20:52:45.616 DEBUG: Taking gradient step
2022-12-29 20:52:45.627 DEBUG: Loss 4: {'policy_loss': -0.03162003023180181, 'entropy_loss': -0.025510641280561686, 'vf_loss': 0.006451010438263252, 'total_loss': -0.05067966107410024, 'approx_kl': 0.01740737317595631, 'clip_fraction': 0.109375, 'grad_norm': 1.3993602991104126}
2022-12-29 20:52:46.443 DEBUG: Taking gradient step
2022-12-29 20:52:46.454 DEBUG: Loss 5: {'policy_loss': -0.02959826226246115, 'entropy_loss': -0.024656606372445822, 'vf_loss': 0.006434540575365332, 'total_loss': -0.04782032805954164, 'approx_kl': 0.020447252900339663, 'clip_fraction': 0.09765625, 'grad_norm': 1.2917697429656982}
2022-12-29 20:52:47.243 DEBUG: Taking gradient step
2022-12-29 20:52:47.253 DEBUG: Loss 6: {'policy_loss': 0.005717735834259788, 'entropy_loss': -0.0259701544418931, 'vf_loss': 0.009075937125603017, 'total_loss': -0.011176481482030303, 'approx_kl': 0.03921367973089218, 'clip_fraction': 0.1783854179084301, 'grad_norm': 1.027754306793213}
2022-12-29 20:52:48.061 DEBUG: Taking gradient step
2022-12-29 20:52:48.071 DEBUG: Loss 7: {'policy_loss': -0.0348587916509169, 'entropy_loss': -0.026562189683318138, 'vf_loss': 0.006407733387359221, 'total_loss': -0.055013247946875815, 'approx_kl': 0.028889512177556753, 'clip_fraction': 0.2239583358168602, 'grad_norm': 1.5403155088424683}
2022-12-29 20:52:48.867 DEBUG: Taking gradient step
2022-12-29 20:52:48.876 DEBUG: Loss 8: {'policy_loss': -0.030345332877803728, 'entropy_loss': -0.027345900423824787, 'vf_loss': 0.006397904662548041, 'total_loss': -0.051293328639080475, 'approx_kl': 0.024420868139714003, 'clip_fraction': 0.2994791716337204, 'grad_norm': 11.991691589355469}
2022-12-29 20:52:49.678 DEBUG: Taking gradient step
2022-12-29 20:52:49.687 DEBUG: Loss 9: {'policy_loss': -0.03098590025733572, 'entropy_loss': -0.028323456645011902, 'vf_loss': 0.006394625824511744, 'total_loss': -0.052914731077835876, 'approx_kl': 0.04393541580066085, 'clip_fraction': 0.3072916716337204, 'grad_norm': 11.774983406066895}
2022-12-29 20:52:49.688 INFO: Optimization: policy loss=-0.031, vf loss=0.006, entropy loss=-0.028, total loss=-0.053, num steps=10
2022-12-29 20:52:49.688 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:52:51.061 INFO: Evaluation rollout: return=0.795 (0.0), episode length=5.0
2022-12-29 20:52:51.062 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:52:51.065 INFO: Iteration: 113/137, steps: 24408
2022-12-29 20:53:33.899 DEBUG: There is a single atom floating around
2022-12-29 20:53:49.204 INFO: Training rollout: return=0.121 (3.3), episode length=5.0
2022-12-29 20:53:49.205 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:53:49.208 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-24408_train.pkl
2022-12-29 20:53:50.046 DEBUG: Taking gradient step
2022-12-29 20:53:50.057 DEBUG: Loss 0: {'policy_loss': -0.021766752719087526, 'entropy_loss': -0.029181193094700575, 'vf_loss': 0.006328929699253885, 'total_loss': -0.04461901611453421, 'approx_kl': -6.022552678786042e-08, 'clip_fraction': 0.0, 'grad_norm': 13.299112319946289}
2022-12-29 20:53:50.841 DEBUG: Taking gradient step
2022-12-29 20:53:50.852 DEBUG: Loss 1: {'policy_loss': -0.032962317870826124, 'entropy_loss': -0.028428131248801947, 'vf_loss': 0.006331330525362685, 'total_loss': -0.05505911859426539, 'approx_kl': 0.003211739007383585, 'clip_fraction': 0.03515625, 'grad_norm': 2.0221433639526367}
2022-12-29 20:53:51.653 DEBUG: Taking gradient step
2022-12-29 20:53:51.667 DEBUG: Loss 2: {'policy_loss': 0.003487897389872631, 'entropy_loss': -0.02789926389232278, 'vf_loss': 0.008876584507595646, 'total_loss': -0.015534781994854505, 'approx_kl': 0.025201168842613697, 'clip_fraction': 0.1796875, 'grad_norm': 2.504478693008423}
2022-12-29 20:53:52.509 DEBUG: Taking gradient step
2022-12-29 20:53:52.519 DEBUG: Loss 3: {'policy_loss': -0.03104474182285463, 'entropy_loss': -0.028653565794229507, 'vf_loss': 0.0063358506936143436, 'total_loss': -0.05336245692346979, 'approx_kl': 0.04067677352577448, 'clip_fraction': 0.2252604216337204, 'grad_norm': 2.3062691688537598}
2022-12-29 20:53:53.346 DEBUG: Taking gradient step
2022-12-29 20:53:53.356 DEBUG: Loss 4: {'policy_loss': -0.03418282083829696, 'entropy_loss': -0.02917648758739233, 'vf_loss': 0.0063406925981298, 'total_loss': -0.057018615827559495, 'approx_kl': 0.04307341389358044, 'clip_fraction': 0.2239583358168602, 'grad_norm': 1.8140677213668823}
2022-12-29 20:53:54.171 DEBUG: Taking gradient step
2022-12-29 20:53:54.180 DEBUG: Loss 5: {'policy_loss': 0.006264463450136561, 'entropy_loss': -0.0286342091858387, 'vf_loss': 0.008918802074751347, 'total_loss': -0.013450943660950794, 'approx_kl': 0.043923653196543455, 'clip_fraction': 0.17578125, 'grad_norm': 1.2010071277618408}
2022-12-29 20:53:54.970 DEBUG: Taking gradient step
2022-12-29 20:53:54.980 DEBUG: Loss 6: {'policy_loss': -0.03266707627103739, 'entropy_loss': -0.02927623363211751, 'vf_loss': 0.0063400436713764415, 'total_loss': -0.055603266231778456, 'approx_kl': 0.03448535222560167, 'clip_fraction': 0.2591145858168602, 'grad_norm': 1.6083455085754395}
2022-12-29 20:53:55.794 DEBUG: Taking gradient step
2022-12-29 20:53:55.806 DEBUG: Loss 7: {'policy_loss': -0.031879448169117724, 'entropy_loss': -0.030915357638150454, 'vf_loss': 0.006337081130609641, 'total_loss': -0.05645772467665854, 'approx_kl': 0.019120917189866304, 'clip_fraction': 0.2526041716337204, 'grad_norm': 2.140270233154297}
2022-12-29 20:53:56.725 DEBUG: Taking gradient step
2022-12-29 20:53:56.734 DEBUG: Loss 8: {'policy_loss': -0.02962129926490313, 'entropy_loss': -0.030747242737561464, 'vf_loss': 0.006339669433234173, 'total_loss': -0.05402887256923042, 'approx_kl': 0.009893468348309398, 'clip_fraction': 0.1861979179084301, 'grad_norm': 7.070334434509277}
2022-12-29 20:53:57.552 DEBUG: Taking gradient step
2022-12-29 20:53:57.563 DEBUG: Loss 9: {'policy_loss': -0.028137422418943764, 'entropy_loss': -0.03166783368214965, 'vf_loss': 0.0063338088131190615, 'total_loss': -0.053471447287974344, 'approx_kl': 0.0013666008599102497, 'clip_fraction': 0.2109375, 'grad_norm': 6.709105968475342}
2022-12-29 20:53:57.563 INFO: Optimization: policy loss=-0.028, vf loss=0.006, entropy loss=-0.032, total loss=-0.053, num steps=10
2022-12-29 20:53:57.563 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:53:59.000 INFO: Evaluation rollout: return=0.788 (0.0), episode length=5.0
2022-12-29 20:53:59.001 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:53:59.003 INFO: Iteration: 114/137, steps: 24624
2022-12-29 20:54:12.567 DEBUG: Atoms are too close
2022-12-29 20:54:57.313 INFO: Training rollout: return=0.026 (3.3), episode length=5.0
2022-12-29 20:54:57.315 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:54:57.317 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-24624_train.pkl
2022-12-29 20:54:58.114 DEBUG: Taking gradient step
2022-12-29 20:54:58.125 DEBUG: Loss 0: {'policy_loss': 0.020799239562150594, 'entropy_loss': -0.031521474942564964, 'vf_loss': 0.010196894334303989, 'total_loss': -0.000525341046110385, 'approx_kl': 2.157564082949648e-08, 'clip_fraction': 0.0, 'grad_norm': 23.9334659576416}
2022-12-29 20:54:58.938 DEBUG: Taking gradient step
2022-12-29 20:54:58.949 DEBUG: Loss 1: {'policy_loss': -0.030424736229948214, 'entropy_loss': -0.03154804231598973, 'vf_loss': 0.007689000883789442, 'total_loss': -0.05428377766214851, 'approx_kl': 0.004853057849686593, 'clip_fraction': 0.02864583395421505, 'grad_norm': 12.954835891723633}
2022-12-29 20:54:59.792 DEBUG: Taking gradient step
2022-12-29 20:54:59.802 DEBUG: Loss 2: {'policy_loss': -0.001236874443540377, 'entropy_loss': -0.03152280440554023, 'vf_loss': 0.010255466526428466, 'total_loss': -0.022504212322652137, 'approx_kl': 0.028294082498177886, 'clip_fraction': 0.2161458358168602, 'grad_norm': 2.70149302482605}
2022-12-29 20:55:00.604 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 20:55:00.604 INFO: Optimization: policy loss=-0.001, vf loss=0.010, entropy loss=-0.032, total loss=-0.023, num steps=3
2022-12-29 20:55:00.605 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:55:02.131 INFO: Evaluation rollout: return=0.779 (0.0), episode length=5.0
2022-12-29 20:55:02.132 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:55:02.134 INFO: Iteration: 115/137, steps: 24840
2022-12-29 20:55:15.153 DEBUG: There is a single atom floating around
2022-12-29 20:55:44.979 DEBUG: There is a single atom floating around
2022-12-29 20:55:59.419 INFO: Training rollout: return=-0.492 (4.7), episode length=5.0
2022-12-29 20:55:59.421 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:55:59.423 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-24840_train.pkl
2022-12-29 20:56:00.278 DEBUG: Taking gradient step
2022-12-29 20:56:00.287 DEBUG: Loss 0: {'policy_loss': 0.06530825309693786, 'entropy_loss': -0.03132863715291023, 'vf_loss': 0.02173729942831765, 'total_loss': 0.05571691537234527, 'approx_kl': 9.16188582777977e-08, 'clip_fraction': 0.0, 'grad_norm': 19.118824005126953}
2022-12-29 20:56:01.072 DEBUG: Taking gradient step
2022-12-29 20:56:01.081 DEBUG: Loss 1: {'policy_loss': -0.006035079902682279, 'entropy_loss': -0.0319978385232389, 'vf_loss': 0.016585203543066675, 'total_loss': -0.021447714882854504, 'approx_kl': 0.0017825060058385134, 'clip_fraction': 0.09765625, 'grad_norm': 14.725202560424805}
2022-12-29 20:56:01.927 DEBUG: Taking gradient step
2022-12-29 20:56:01.939 DEBUG: Loss 2: {'policy_loss': -0.04359972844201841, 'entropy_loss': -0.03255262551829219, 'vf_loss': 0.014029365111479538, 'total_loss': -0.062122988848831065, 'approx_kl': 0.01811220566742122, 'clip_fraction': 0.2864583358168602, 'grad_norm': 5.889950752258301}
2022-12-29 20:56:02.760 DEBUG: Taking gradient step
2022-12-29 20:56:02.769 DEBUG: Loss 3: {'policy_loss': -0.010731728856326625, 'entropy_loss': -0.03218268137425184, 'vf_loss': 0.016558605430048882, 'total_loss': -0.02635580480052959, 'approx_kl': 0.03341059386730194, 'clip_fraction': 0.3046875, 'grad_norm': 7.433414459228516}
2022-12-29 20:56:03.561 DEBUG: Taking gradient step
2022-12-29 20:56:03.574 DEBUG: Loss 4: {'policy_loss': 0.05316802575675979, 'entropy_loss': -0.031300841830670834, 'vf_loss': 0.021698016423789754, 'total_loss': 0.04356520034987871, 'approx_kl': 0.02986832894384861, 'clip_fraction': 0.3033854216337204, 'grad_norm': 6.546197414398193}
2022-12-29 20:56:04.378 DEBUG: Taking gradient step
2022-12-29 20:56:04.390 DEBUG: Loss 5: {'policy_loss': -0.012237324586189922, 'entropy_loss': -0.031203432474285364, 'vf_loss': 0.016578038428788005, 'total_loss': -0.026862718631687287, 'approx_kl': 0.03727690386585891, 'clip_fraction': 0.171875, 'grad_norm': 4.803326606750488}
2022-12-29 20:56:05.166 DEBUG: Taking gradient step
2022-12-29 20:56:05.177 DEBUG: Loss 6: {'policy_loss': -0.02273822185508793, 'entropy_loss': -0.03022771468386054, 'vf_loss': 0.01651833679026384, 'total_loss': -0.03644759974868463, 'approx_kl': 0.03908018907532096, 'clip_fraction': 0.11458333395421505, 'grad_norm': 4.244657516479492}
2022-12-29 20:56:05.960 DEBUG: Early stopping at step 7 for reaching max KL.
2022-12-29 20:56:05.960 INFO: Optimization: policy loss=-0.023, vf loss=0.017, entropy loss=-0.030, total loss=-0.036, num steps=7
2022-12-29 20:56:05.961 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:56:07.451 INFO: Evaluation rollout: return=0.780 (0.0), episode length=5.0
2022-12-29 20:56:07.452 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:56:07.455 INFO: Iteration: 116/137, steps: 25056
2022-12-29 20:57:05.960 INFO: Training rollout: return=0.643 (0.1), episode length=5.0
2022-12-29 20:57:05.961 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:57:05.964 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-25056_train.pkl
2022-12-29 20:57:06.789 DEBUG: Taking gradient step
2022-12-29 20:57:06.799 DEBUG: Loss 0: {'policy_loss': 0.003267986105957325, 'entropy_loss': -0.029359566513448954, 'vf_loss': 0.0002475576089860715, 'total_loss': -0.025844022798505558, 'approx_kl': 4.743924364447594e-08, 'clip_fraction': 0.0, 'grad_norm': 5.5827789306640625}
2022-12-29 20:57:07.591 DEBUG: Taking gradient step
2022-12-29 20:57:07.600 DEBUG: Loss 1: {'policy_loss': -0.016304195530418463, 'entropy_loss': -0.028281863778829575, 'vf_loss': 0.0002649835765445007, 'total_loss': -0.04432107573270354, 'approx_kl': 0.006641252897679806, 'clip_fraction': 0.02734375, 'grad_norm': 6.892417907714844}
2022-12-29 20:57:08.418 DEBUG: Taking gradient step
2022-12-29 20:57:08.427 DEBUG: Loss 2: {'policy_loss': -0.017842788076031367, 'entropy_loss': -0.02768823318183422, 'vf_loss': 0.0002772676886929443, 'total_loss': -0.04525375356917265, 'approx_kl': 0.02689171116799116, 'clip_fraction': 0.29817708395421505, 'grad_norm': 15.55415153503418}
2022-12-29 20:57:09.258 DEBUG: Taking gradient step
2022-12-29 20:57:09.267 DEBUG: Loss 3: {'policy_loss': 0.025140167965832357, 'entropy_loss': -0.0271946438588202, 'vf_loss': 0.00027766433824185114, 'total_loss': -0.0017768115547460023, 'approx_kl': 0.028341792058199644, 'clip_fraction': 0.4205729216337204, 'grad_norm': 17.120595932006836}
2022-12-29 20:57:10.084 DEBUG: Taking gradient step
2022-12-29 20:57:10.094 DEBUG: Loss 4: {'policy_loss': -0.041586888664255084, 'entropy_loss': -0.025869077537208796, 'vf_loss': 0.0002967148177501393, 'total_loss': -0.06715925138371374, 'approx_kl': 0.03978927060961723, 'clip_fraction': 0.4205729216337204, 'grad_norm': 17.491918563842773}
2022-12-29 20:57:10.890 DEBUG: Taking gradient step
2022-12-29 20:57:10.901 DEBUG: Loss 5: {'policy_loss': 0.0367585723258559, 'entropy_loss': -0.025575718376785517, 'vf_loss': 0.00028407243524337066, 'total_loss': 0.01146692638431375, 'approx_kl': 0.021426991559565067, 'clip_fraction': 0.33203125, 'grad_norm': 14.42846965789795}
2022-12-29 20:57:11.702 DEBUG: Taking gradient step
2022-12-29 20:57:11.711 DEBUG: Loss 6: {'policy_loss': -0.06151199863577152, 'entropy_loss': -0.024293603841215372, 'vf_loss': 0.0002973983705549072, 'total_loss': -0.08550820410643198, 'approx_kl': 0.01643730653449893, 'clip_fraction': 0.2799479216337204, 'grad_norm': 9.382838249206543}
2022-12-29 20:57:12.505 DEBUG: Taking gradient step
2022-12-29 20:57:12.515 DEBUG: Loss 7: {'policy_loss': -0.011716160003432452, 'entropy_loss': -0.023795054759830236, 'vf_loss': 0.0002886193864359359, 'total_loss': -0.03522259537682675, 'approx_kl': -0.0022716928215231746, 'clip_fraction': 0.2369791716337204, 'grad_norm': 14.03610610961914}
2022-12-29 20:57:13.304 DEBUG: Taking gradient step
2022-12-29 20:57:13.313 DEBUG: Loss 8: {'policy_loss': -0.043246444177824445, 'entropy_loss': -0.022625633981078863, 'vf_loss': 0.00029549420193328616, 'total_loss': -0.06557658395697001, 'approx_kl': -0.004735221853479743, 'clip_fraction': 0.2565104179084301, 'grad_norm': 12.494759559631348}
2022-12-29 20:57:14.115 DEBUG: Taking gradient step
2022-12-29 20:57:14.126 DEBUG: Loss 9: {'policy_loss': 0.019049226555890274, 'entropy_loss': -0.021979381795972586, 'vf_loss': 0.00027619805449588145, 'total_loss': -0.0026539571855864197, 'approx_kl': 0.017807757947593927, 'clip_fraction': 0.2825520858168602, 'grad_norm': 16.393938064575195}
2022-12-29 20:57:14.126 INFO: Optimization: policy loss=0.019, vf loss=0.000, entropy loss=-0.022, total loss=-0.003, num steps=10
2022-12-29 20:57:14.127 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:57:15.514 INFO: Evaluation rollout: return=0.782 (0.0), episode length=5.0
2022-12-29 20:57:15.515 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:57:15.518 INFO: Iteration: 117/137, steps: 25272
2022-12-29 20:58:14.467 INFO: Training rollout: return=0.685 (0.1), episode length=5.0
2022-12-29 20:58:14.469 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:58:14.472 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-25272_train.pkl
2022-12-29 20:58:15.312 DEBUG: Taking gradient step
2022-12-29 20:58:15.329 DEBUG: Loss 0: {'policy_loss': 0.014778746242628392, 'entropy_loss': -0.02127152495086193, 'vf_loss': 0.0002989461270434099, 'total_loss': -0.006193832581190134, 'approx_kl': 5.2755543322735576e-08, 'clip_fraction': 0.0, 'grad_norm': 8.71194076538086}
2022-12-29 20:58:16.130 DEBUG: Taking gradient step
2022-12-29 20:58:16.139 DEBUG: Loss 1: {'policy_loss': -0.020933893544530534, 'entropy_loss': -0.022265059873461723, 'vf_loss': 0.0002933223965233748, 'total_loss': -0.042905631021468875, 'approx_kl': -0.002101571415551007, 'clip_fraction': 0.046875, 'grad_norm': 4.950621604919434}
2022-12-29 20:58:16.947 DEBUG: Taking gradient step
2022-12-29 20:58:16.958 DEBUG: Loss 2: {'policy_loss': -0.02879977930331551, 'entropy_loss': -0.020528790075331926, 'vf_loss': 0.00027858658372076036, 'total_loss': -0.04904998279492667, 'approx_kl': -0.0035688806092366576, 'clip_fraction': 0.171875, 'grad_norm': 8.236244201660156}
2022-12-29 20:58:17.777 DEBUG: Taking gradient step
2022-12-29 20:58:17.791 DEBUG: Loss 3: {'policy_loss': 0.0008930892060013199, 'entropy_loss': -0.021918004844337702, 'vf_loss': 0.00025869911292750457, 'total_loss': -0.020766216525408873, 'approx_kl': -0.0011929066386073828, 'clip_fraction': 0.19270833395421505, 'grad_norm': 3.927006244659424}
2022-12-29 20:58:18.594 DEBUG: Taking gradient step
2022-12-29 20:58:18.604 DEBUG: Loss 4: {'policy_loss': -0.03521639893185198, 'entropy_loss': -0.020708774216473103, 'vf_loss': 0.0002545564759291801, 'total_loss': -0.0556706166723959, 'approx_kl': 0.014422249048948288, 'clip_fraction': 0.2252604179084301, 'grad_norm': 8.921935081481934}
2022-12-29 20:58:19.382 DEBUG: Taking gradient step
2022-12-29 20:58:19.391 DEBUG: Loss 5: {'policy_loss': -0.06054837392135481, 'entropy_loss': -0.02001489093527198, 'vf_loss': 0.00023604687313088417, 'total_loss': -0.0803272179834959, 'approx_kl': 0.025418913923203945, 'clip_fraction': 0.25390625, 'grad_norm': 9.448996543884277}
2022-12-29 20:58:20.235 DEBUG: Taking gradient step
2022-12-29 20:58:20.246 DEBUG: Loss 6: {'policy_loss': 0.030130397712464806, 'entropy_loss': -0.019984790589660406, 'vf_loss': 0.00021149144864803616, 'total_loss': 0.010357098571452443, 'approx_kl': 0.015766869764775038, 'clip_fraction': 0.20703125, 'grad_norm': 5.037123203277588}
2022-12-29 20:58:21.012 DEBUG: Taking gradient step
2022-12-29 20:58:21.021 DEBUG: Loss 7: {'policy_loss': 0.0026315613606887472, 'entropy_loss': -0.019194008316844702, 'vf_loss': 0.00020041395548867053, 'total_loss': -0.016362033000667284, 'approx_kl': 0.013821479864418507, 'clip_fraction': 0.1979166679084301, 'grad_norm': 8.702866554260254}
2022-12-29 20:58:21.795 DEBUG: Taking gradient step
2022-12-29 20:58:21.806 DEBUG: Loss 8: {'policy_loss': 0.012922048319651047, 'entropy_loss': -0.018251437693834305, 'vf_loss': 0.0001865415570587304, 'total_loss': -0.00514284781712452, 'approx_kl': 0.013110914034768939, 'clip_fraction': 0.12369791697710752, 'grad_norm': 6.47284460067749}
2022-12-29 20:58:22.599 DEBUG: Taking gradient step
2022-12-29 20:58:22.609 DEBUG: Loss 9: {'policy_loss': -0.006061613326021674, 'entropy_loss': -0.018583675380796194, 'vf_loss': 0.00016968867794848585, 'total_loss': -0.024475600028869383, 'approx_kl': 0.033951398683711886, 'clip_fraction': 0.1484375, 'grad_norm': 12.307769775390625}
2022-12-29 20:58:22.609 INFO: Optimization: policy loss=-0.006, vf loss=0.000, entropy loss=-0.019, total loss=-0.024, num steps=10
2022-12-29 20:58:22.609 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:58:24.024 INFO: Evaluation rollout: return=0.784 (0.0), episode length=5.0
2022-12-29 20:58:24.025 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:58:24.028 INFO: Iteration: 118/137, steps: 25488
2022-12-29 20:59:22.317 INFO: Training rollout: return=0.690 (0.1), episode length=5.0
2022-12-29 20:59:22.318 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 20:59:22.320 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-25488_train.pkl
2022-12-29 20:59:23.114 DEBUG: Taking gradient step
2022-12-29 20:59:23.124 DEBUG: Loss 0: {'policy_loss': 0.036912747726246524, 'entropy_loss': -0.01824418967589736, 'vf_loss': 0.00016169223120311363, 'total_loss': 0.01883025028155228, 'approx_kl': 1.0617077350616455e-07, 'clip_fraction': 0.0, 'grad_norm': 11.668190002441406}
2022-12-29 20:59:23.954 DEBUG: Taking gradient step
2022-12-29 20:59:23.966 DEBUG: Loss 1: {'policy_loss': -0.023140911872657707, 'entropy_loss': -0.018074309453368187, 'vf_loss': 0.00015743846976112063, 'total_loss': -0.04105778285626477, 'approx_kl': 0.00790004272130318, 'clip_fraction': 0.03515625, 'grad_norm': 4.227306842803955}
2022-12-29 20:59:24.719 DEBUG: Taking gradient step
2022-12-29 20:59:24.728 DEBUG: Loss 2: {'policy_loss': -0.06318238682613819, 'entropy_loss': -0.017280272906646132, 'vf_loss': 0.00015221668709370365, 'total_loss': -0.08031044304569063, 'approx_kl': 0.02905670623295009, 'clip_fraction': 0.1549479179084301, 'grad_norm': 8.274210929870605}
2022-12-29 20:59:25.545 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 20:59:25.545 INFO: Optimization: policy loss=-0.063, vf loss=0.000, entropy loss=-0.017, total loss=-0.080, num steps=3
2022-12-29 20:59:25.546 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 20:59:26.937 INFO: Evaluation rollout: return=0.793 (0.0), episode length=5.0
2022-12-29 20:59:26.938 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 20:59:26.941 INFO: Iteration: 119/137, steps: 25704
2022-12-29 21:00:25.227 INFO: Training rollout: return=0.696 (0.1), episode length=5.0
2022-12-29 21:00:25.228 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 21:00:25.232 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-25704_train.pkl
2022-12-29 21:00:26.084 DEBUG: Taking gradient step
2022-12-29 21:00:26.093 DEBUG: Loss 0: {'policy_loss': 0.06105136482979027, 'entropy_loss': -0.01891308627091348, 'vf_loss': 0.0001388689898680345, 'total_loss': 0.04227714754874483, 'approx_kl': -4.035731393514652e-08, 'clip_fraction': 0.0, 'grad_norm': 21.029123306274414}
2022-12-29 21:00:26.906 DEBUG: Taking gradient step
2022-12-29 21:00:26.916 DEBUG: Loss 1: {'policy_loss': 0.024158688414243693, 'entropy_loss': -0.018148076254874468, 'vf_loss': 0.0001336742249851246, 'total_loss': 0.006144286384354353, 'approx_kl': 0.01715516031254083, 'clip_fraction': 0.1171875, 'grad_norm': 7.3870368003845215}
2022-12-29 21:00:27.797 DEBUG: Taking gradient step
2022-12-29 21:00:27.806 DEBUG: Loss 2: {'policy_loss': -0.022198049935193358, 'entropy_loss': -0.017965123523026705, 'vf_loss': 0.00013346050346045268, 'total_loss': -0.04002971295475961, 'approx_kl': 0.03414063295349479, 'clip_fraction': 0.2526041679084301, 'grad_norm': 16.111915588378906}
2022-12-29 21:00:28.586 DEBUG: Taking gradient step
2022-12-29 21:00:28.601 DEBUG: Loss 3: {'policy_loss': -0.046543340076633015, 'entropy_loss': -0.017215083818882704, 'vf_loss': 0.0001271267479461939, 'total_loss': -0.06363129714756953, 'approx_kl': 0.038224704563617706, 'clip_fraction': 0.3333333358168602, 'grad_norm': 9.762810707092285}
2022-12-29 21:00:29.406 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 21:00:29.407 INFO: Optimization: policy loss=-0.047, vf loss=0.000, entropy loss=-0.017, total loss=-0.064, num steps=4
2022-12-29 21:00:29.407 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 21:00:30.879 INFO: Evaluation rollout: return=0.796 (0.0), episode length=5.0
2022-12-29 21:00:30.880 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 21:00:30.884 INFO: Iteration: 120/137, steps: 25920
2022-12-29 21:00:41.113 DEBUG: Atoms are too close
2022-12-29 21:01:28.594 INFO: Training rollout: return=0.126 (3.3), episode length=5.0
2022-12-29 21:01:28.595 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 21:01:28.598 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-25920_train.pkl
2022-12-29 21:01:29.405 DEBUG: Taking gradient step
2022-12-29 21:01:29.416 DEBUG: Loss 0: {'policy_loss': -0.025001677688198328, 'entropy_loss': -0.016896680928766727, 'vf_loss': 0.006362577906496953, 'total_loss': -0.035535780710468104, 'approx_kl': -3.837825346408863e-08, 'clip_fraction': 0.0, 'grad_norm': 11.957015037536621}
2022-12-29 21:01:30.182 DEBUG: Taking gradient step
2022-12-29 21:01:30.191 DEBUG: Loss 1: {'policy_loss': 0.07554400954070875, 'entropy_loss': -0.017526770010590553, 'vf_loss': 0.011505432925471827, 'total_loss': 0.06952267245559002, 'approx_kl': -0.0006874060491099954, 'clip_fraction': 0.0, 'grad_norm': 12.136005401611328}
2022-12-29 21:01:30.994 DEBUG: Taking gradient step
2022-12-29 21:01:31.003 DEBUG: Loss 2: {'policy_loss': -0.02693783791517567, 'entropy_loss': -0.016846749931573868, 'vf_loss': 0.006364457718968788, 'total_loss': -0.037420130127780754, 'approx_kl': -0.00017131568165495992, 'clip_fraction': 0.04427083395421505, 'grad_norm': 4.565256118774414}
2022-12-29 21:01:31.773 DEBUG: Taking gradient step
2022-12-29 21:01:31.782 DEBUG: Loss 3: {'policy_loss': 0.00821716932087892, 'entropy_loss': -0.01693475851789117, 'vf_loss': 0.009027139374679248, 'total_loss': 0.0003095501776670048, 'approx_kl': 0.017898132326081395, 'clip_fraction': 0.06510416697710752, 'grad_norm': 4.327093601226807}
2022-12-29 21:01:32.567 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 21:01:32.567 INFO: Optimization: policy loss=0.008, vf loss=0.009, entropy loss=-0.017, total loss=0.000, num steps=4
2022-12-29 21:01:32.567 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 21:01:33.982 INFO: Evaluation rollout: return=0.800 (0.0), episode length=5.0
2022-12-29 21:01:33.983 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 21:01:33.986 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-23976.model
2022-12-29 21:01:33.991 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-26136.model
2022-12-29 21:01:34.025 INFO: Iteration: 121/137, steps: 26136
2022-12-29 21:02:32.366 INFO: Training rollout: return=0.684 (0.1), episode length=5.0
2022-12-29 21:02:32.367 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 21:02:32.369 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-26136_train.pkl
2022-12-29 21:02:33.179 DEBUG: Taking gradient step
2022-12-29 21:02:33.190 DEBUG: Loss 0: {'policy_loss': 0.04736044627532462, 'entropy_loss': -0.017118492862209678, 'vf_loss': 0.00011691733650690533, 'total_loss': 0.030358870749621847, 'approx_kl': 3.709768137127867e-08, 'clip_fraction': 0.0, 'grad_norm': 26.86322021484375}
2022-12-29 21:02:33.994 DEBUG: Taking gradient step
2022-12-29 21:02:34.010 DEBUG: Loss 1: {'policy_loss': 0.007468034480214733, 'entropy_loss': -0.016504851635545492, 'vf_loss': 0.00011876493742237433, 'total_loss': -0.008918052217908388, 'approx_kl': 0.020617676433175802, 'clip_fraction': 0.09375, 'grad_norm': 20.234575271606445}
2022-12-29 21:02:34.793 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 21:02:34.793 INFO: Optimization: policy loss=0.007, vf loss=0.000, entropy loss=-0.017, total loss=-0.009, num steps=2
2022-12-29 21:02:34.793 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 21:02:36.253 INFO: Evaluation rollout: return=0.801 (0.0), episode length=5.0
2022-12-29 21:02:36.254 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 21:02:36.256 INFO: Iteration: 122/137, steps: 26352
2022-12-29 21:03:34.619 INFO: Training rollout: return=0.683 (0.1), episode length=5.0
2022-12-29 21:03:34.620 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 21:03:34.623 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-26352_train.pkl
2022-12-29 21:03:35.467 DEBUG: Taking gradient step
2022-12-29 21:03:35.481 DEBUG: Loss 0: {'policy_loss': -0.011554464614071065, 'entropy_loss': -0.01702638971619308, 'vf_loss': 0.00012530155574690566, 'total_loss': -0.028455552774517247, 'approx_kl': -4.45094574530458e-08, 'clip_fraction': 0.0, 'grad_norm': 33.02157211303711}
2022-12-29 21:03:36.339 DEBUG: Taking gradient step
2022-12-29 21:03:36.348 DEBUG: Loss 1: {'policy_loss': -0.014299394278285579, 'entropy_loss': -0.01696448028087616, 'vf_loss': 0.0001262216103030174, 'total_loss': -0.031137652948858716, 'approx_kl': 0.022166633047163486, 'clip_fraction': 0.12760416697710752, 'grad_norm': 20.32855796813965}
2022-12-29 21:03:37.129 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 21:03:37.130 INFO: Optimization: policy loss=-0.014, vf loss=0.000, entropy loss=-0.017, total loss=-0.031, num steps=2
2022-12-29 21:03:37.130 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 21:03:38.626 INFO: Evaluation rollout: return=0.798 (0.0), episode length=5.0
2022-12-29 21:03:38.627 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 21:03:38.632 INFO: Iteration: 123/137, steps: 26568
2022-12-29 21:04:36.797 INFO: Training rollout: return=0.701 (0.1), episode length=5.0
2022-12-29 21:04:36.800 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 21:04:36.803 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-26568_train.pkl
2022-12-29 21:04:37.593 DEBUG: Taking gradient step
2022-12-29 21:04:37.603 DEBUG: Loss 0: {'policy_loss': 0.016187411100301845, 'entropy_loss': -0.01659927051514387, 'vf_loss': 0.00012088634967068228, 'total_loss': -0.00029097306517134394, 'approx_kl': 2.8172507882118225e-08, 'clip_fraction': 0.0, 'grad_norm': 25.686195373535156}
2022-12-29 21:04:38.454 DEBUG: Taking gradient step
2022-12-29 21:04:38.464 DEBUG: Loss 1: {'policy_loss': -0.029468491994092853, 'entropy_loss': -0.01620997814461589, 'vf_loss': 0.00012174274430645953, 'total_loss': -0.04555672739440228, 'approx_kl': 0.004467969760298729, 'clip_fraction': 0.06510416697710752, 'grad_norm': 22.323261260986328}
2022-12-29 21:04:39.284 DEBUG: Taking gradient step
2022-12-29 21:04:39.294 DEBUG: Loss 2: {'policy_loss': -0.00524328476308613, 'entropy_loss': -0.016722518019378185, 'vf_loss': 0.00012201348892382777, 'total_loss': -0.02184378929354049, 'approx_kl': 0.008222526870667934, 'clip_fraction': 0.11979166697710752, 'grad_norm': 24.455074310302734}
2022-12-29 21:04:40.100 DEBUG: Taking gradient step
2022-12-29 21:04:40.110 DEBUG: Loss 3: {'policy_loss': -0.02178859176536954, 'entropy_loss': -0.016181365819647908, 'vf_loss': 0.00011729293658806596, 'total_loss': -0.03785266464842939, 'approx_kl': 0.029257139656692743, 'clip_fraction': 0.1653645858168602, 'grad_norm': 14.645098686218262}
2022-12-29 21:04:40.908 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 21:04:40.908 INFO: Optimization: policy loss=-0.022, vf loss=0.000, entropy loss=-0.016, total loss=-0.038, num steps=4
2022-12-29 21:04:40.908 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 21:04:42.376 INFO: Evaluation rollout: return=0.801 (0.0), episode length=5.0
2022-12-29 21:04:42.377 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 21:04:42.380 INFO: Iteration: 124/137, steps: 26784
2022-12-29 21:05:40.730 INFO: Training rollout: return=0.712 (0.1), episode length=5.0
2022-12-29 21:05:40.732 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 21:05:40.735 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-26784_train.pkl
2022-12-29 21:05:41.525 DEBUG: Taking gradient step
2022-12-29 21:05:41.534 DEBUG: Loss 0: {'policy_loss': -0.018447450213227058, 'entropy_loss': -0.015043588820844889, 'vf_loss': 0.00012565576976957096, 'total_loss': -0.033365383264302384, 'approx_kl': 1.3527460396289825e-07, 'clip_fraction': 0.0, 'grad_norm': 20.723194122314453}
2022-12-29 21:05:42.349 DEBUG: Taking gradient step
2022-12-29 21:05:42.358 DEBUG: Loss 1: {'policy_loss': -0.04470478160082698, 'entropy_loss': -0.014542192220687866, 'vf_loss': 0.0001251533135093346, 'total_loss': -0.05912182050800551, 'approx_kl': 0.006389411981217563, 'clip_fraction': 0.08333333395421505, 'grad_norm': 16.78045082092285}
2022-12-29 21:05:43.201 DEBUG: Taking gradient step
2022-12-29 21:05:43.215 DEBUG: Loss 2: {'policy_loss': -0.03044932729761843, 'entropy_loss': -0.01514605083502829, 'vf_loss': 0.00012136018143139003, 'total_loss': -0.045474017951215334, 'approx_kl': 0.017915015574544668, 'clip_fraction': 0.13020833395421505, 'grad_norm': 19.74796485900879}
2022-12-29 21:05:44.110 DEBUG: Taking gradient step
2022-12-29 21:05:44.121 DEBUG: Loss 3: {'policy_loss': 0.016964489064806, 'entropy_loss': -0.01597564318217337, 'vf_loss': 0.00011851921795551524, 'total_loss': 0.0011073651005881445, 'approx_kl': 0.009397054869623389, 'clip_fraction': 0.1653645858168602, 'grad_norm': 22.2062931060791}
2022-12-29 21:05:44.892 DEBUG: Taking gradient step
2022-12-29 21:05:44.902 DEBUG: Loss 4: {'policy_loss': 0.011504204959512824, 'entropy_loss': -0.01604812522418797, 'vf_loss': 0.00011613393672117515, 'total_loss': -0.0044277863279539725, 'approx_kl': 0.007970511331222951, 'clip_fraction': 0.09765625, 'grad_norm': 5.531317710876465}
2022-12-29 21:05:45.669 DEBUG: Taking gradient step
2022-12-29 21:05:45.679 DEBUG: Loss 5: {'policy_loss': 0.0005281472537421155, 'entropy_loss': -0.015323825413361192, 'vf_loss': 0.00011398910864870181, 'total_loss': -0.014681689050970372, 'approx_kl': 0.011778431362472475, 'clip_fraction': 0.11979166697710752, 'grad_norm': 10.037931442260742}
2022-12-29 21:05:46.500 DEBUG: Taking gradient step
2022-12-29 21:05:46.511 DEBUG: Loss 6: {'policy_loss': -0.01626913944521366, 'entropy_loss': -0.016097161453217268, 'vf_loss': 0.00011417595946205627, 'total_loss': -0.03225212493896887, 'approx_kl': 0.009540808852761984, 'clip_fraction': 0.1171875, 'grad_norm': 13.035476684570312}
2022-12-29 21:05:47.394 DEBUG: Taking gradient step
2022-12-29 21:05:47.404 DEBUG: Loss 7: {'policy_loss': 0.0030954890198788935, 'entropy_loss': -0.016386864706873894, 'vf_loss': 0.00011271447401339098, 'total_loss': -0.013178661212981588, 'approx_kl': 0.007071990636177361, 'clip_fraction': 0.1432291679084301, 'grad_norm': 12.120465278625488}
2022-12-29 21:05:48.238 DEBUG: Taking gradient step
2022-12-29 21:05:48.247 DEBUG: Loss 8: {'policy_loss': -0.04136352025786932, 'entropy_loss': -0.016958874417468905, 'vf_loss': 0.00011349860471889684, 'total_loss': -0.058208896070619326, 'approx_kl': 0.005888437852263451, 'clip_fraction': 0.09114583395421505, 'grad_norm': 10.63782787322998}
2022-12-29 21:05:49.099 DEBUG: Taking gradient step
2022-12-29 21:05:49.108 DEBUG: Loss 9: {'policy_loss': -0.013865614598093, 'entropy_loss': -0.01647274033166468, 'vf_loss': 0.00010778959370201144, 'total_loss': -0.03023056533605567, 'approx_kl': 0.008677806705236435, 'clip_fraction': 0.12890625, 'grad_norm': 9.87794303894043}
2022-12-29 21:05:49.109 INFO: Optimization: policy loss=-0.014, vf loss=0.000, entropy loss=-0.016, total loss=-0.030, num steps=10
2022-12-29 21:05:49.109 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 21:05:50.475 INFO: Evaluation rollout: return=0.799 (0.0), episode length=5.0
2022-12-29 21:05:50.476 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 21:05:50.478 INFO: Iteration: 125/137, steps: 27000
2022-12-29 21:06:48.612 INFO: Training rollout: return=0.718 (0.1), episode length=5.0
2022-12-29 21:06:48.614 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 21:06:48.619 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-27000_train.pkl
2022-12-29 21:06:49.455 DEBUG: Taking gradient step
2022-12-29 21:06:49.465 DEBUG: Loss 0: {'policy_loss': -0.0018085398479595971, 'entropy_loss': -0.01743703824467957, 'vf_loss': 0.00011289924751865656, 'total_loss': -0.019132678845120515, 'approx_kl': 1.707424956975956e-08, 'clip_fraction': 0.0, 'grad_norm': 18.9782657623291}
2022-12-29 21:06:50.285 DEBUG: Taking gradient step
2022-12-29 21:06:50.294 DEBUG: Loss 1: {'policy_loss': 0.00044958390750490275, 'entropy_loss': -0.017518680077046156, 'vf_loss': 0.00011286029922313908, 'total_loss': -0.016956235870318116, 'approx_kl': -0.00368491536937654, 'clip_fraction': 0.1627604179084301, 'grad_norm': 23.178142547607422}
2022-12-29 21:06:51.110 DEBUG: Taking gradient step
2022-12-29 21:06:51.119 DEBUG: Loss 2: {'policy_loss': -0.01956577130912825, 'entropy_loss': -0.018527040723711252, 'vf_loss': 0.00010739508644890358, 'total_loss': -0.037985416946390585, 'approx_kl': 0.007858382305130363, 'clip_fraction': 0.2421875, 'grad_norm': 22.355684280395508}
2022-12-29 21:06:51.879 DEBUG: Taking gradient step
2022-12-29 21:06:51.888 DEBUG: Loss 3: {'policy_loss': -0.00863137447766805, 'entropy_loss': -0.018095482140779495, 'vf_loss': 0.00010678078729888249, 'total_loss': -0.02662007583114867, 'approx_kl': 0.0076062449952587485, 'clip_fraction': 0.11848958395421505, 'grad_norm': 13.407793045043945}
2022-12-29 21:06:52.683 DEBUG: Taking gradient step
2022-12-29 21:06:52.694 DEBUG: Loss 4: {'policy_loss': 0.011013069066693602, 'entropy_loss': -0.019415801856666803, 'vf_loss': 0.0001037539921857481, 'total_loss': -0.008298978797787456, 'approx_kl': 0.010167468106374145, 'clip_fraction': 0.06901041697710752, 'grad_norm': 18.050390243530273}
2022-12-29 21:06:53.474 DEBUG: Taking gradient step
2022-12-29 21:06:53.483 DEBUG: Loss 5: {'policy_loss': 0.03584474727722867, 'entropy_loss': -0.019659487530589104, 'vf_loss': 9.966755206963547e-05, 'total_loss': 0.0162849272987092, 'approx_kl': 0.020598181057721376, 'clip_fraction': 0.08984375, 'grad_norm': 18.397628784179688}
2022-12-29 21:06:54.313 DEBUG: Taking gradient step
2022-12-29 21:06:54.324 DEBUG: Loss 6: {'policy_loss': -0.03132282605472428, 'entropy_loss': -0.0197560153901577, 'vf_loss': 0.0001024005053971576, 'total_loss': -0.05097644093948482, 'approx_kl': 0.014191774069331586, 'clip_fraction': 0.045572916977107525, 'grad_norm': 11.95520305633545}
2022-12-29 21:06:55.091 DEBUG: Taking gradient step
2022-12-29 21:06:55.101 DEBUG: Loss 7: {'policy_loss': 0.00811067043383978, 'entropy_loss': -0.019330297131091356, 'vf_loss': 9.535295424293696e-05, 'total_loss': -0.011124273743008634, 'approx_kl': 0.016532676643691957, 'clip_fraction': 0.2135416716337204, 'grad_norm': 13.6679105758667}
2022-12-29 21:06:55.885 DEBUG: Taking gradient step
2022-12-29 21:06:55.895 DEBUG: Loss 8: {'policy_loss': 0.013639935521590256, 'entropy_loss': -0.01991219911724329, 'vf_loss': 9.454604857311462e-05, 'total_loss': -0.006177717547079922, 'approx_kl': 0.024078297894448042, 'clip_fraction': 0.2825520858168602, 'grad_norm': 19.428129196166992}
2022-12-29 21:06:56.663 DEBUG: Taking gradient step
2022-12-29 21:06:56.674 DEBUG: Loss 9: {'policy_loss': -0.032268554536428934, 'entropy_loss': -0.019893933087587357, 'vf_loss': 9.710814434047534e-05, 'total_loss': -0.05206537947967581, 'approx_kl': 0.025263091549277306, 'clip_fraction': 0.2161458358168602, 'grad_norm': 11.560087203979492}
2022-12-29 21:06:56.674 INFO: Optimization: policy loss=-0.032, vf loss=0.000, entropy loss=-0.020, total loss=-0.052, num steps=10
2022-12-29 21:06:56.675 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 21:06:58.167 INFO: Evaluation rollout: return=0.796 (0.0), episode length=5.0
2022-12-29 21:06:58.168 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 21:06:58.170 INFO: Iteration: 126/137, steps: 27216
2022-12-29 21:07:24.157 DEBUG: Atoms are too close
2022-12-29 21:07:55.829 INFO: Training rollout: return=0.115 (3.4), episode length=5.0
2022-12-29 21:07:55.830 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 21:07:55.833 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-27216_train.pkl
2022-12-29 21:07:56.704 DEBUG: Taking gradient step
2022-12-29 21:07:56.718 DEBUG: Loss 0: {'policy_loss': -0.025754687069719593, 'entropy_loss': -0.02059772377833724, 'vf_loss': 0.006460059164583944, 'total_loss': -0.03989235168347289, 'approx_kl': -4.0822972557919e-08, 'clip_fraction': 0.0, 'grad_norm': 16.872997283935547}
2022-12-29 21:07:57.592 DEBUG: Taking gradient step
2022-12-29 21:07:57.602 DEBUG: Loss 1: {'policy_loss': -0.02723381998170451, 'entropy_loss': -0.02087328676134348, 'vf_loss': 0.00645931751774983, 'total_loss': -0.04164778922529816, 'approx_kl': -0.005620856070891023, 'clip_fraction': 0.041666666977107525, 'grad_norm': 16.36003303527832}
2022-12-29 21:07:58.389 DEBUG: Taking gradient step
2022-12-29 21:07:58.398 DEBUG: Loss 2: {'policy_loss': 0.01845572161767413, 'entropy_loss': -0.02199540426954627, 'vf_loss': 0.009073181882348218, 'total_loss': 0.005533499230476087, 'approx_kl': -0.0038819254841655493, 'clip_fraction': 0.12630208395421505, 'grad_norm': 1.1450115442276}
2022-12-29 21:07:59.196 DEBUG: Taking gradient step
2022-12-29 21:07:59.206 DEBUG: Loss 3: {'policy_loss': -0.03148652829714396, 'entropy_loss': -0.02219410566613078, 'vf_loss': 0.006453725645323394, 'total_loss': -0.047226908317951344, 'approx_kl': -0.014239647425711155, 'clip_fraction': 0.2721354216337204, 'grad_norm': 1.731158971786499}
2022-12-29 21:08:00.011 DEBUG: Taking gradient step
2022-12-29 21:08:00.026 DEBUG: Loss 4: {'policy_loss': -0.03316159885460304, 'entropy_loss': -0.023153003305196762, 'vf_loss': 0.006446674077722754, 'total_loss': -0.04986792808207705, 'approx_kl': -0.013576090103015304, 'clip_fraction': 0.3177083358168602, 'grad_norm': 1.192604899406433}
2022-12-29 21:08:00.862 DEBUG: Taking gradient step
2022-12-29 21:08:00.872 DEBUG: Loss 5: {'policy_loss': -0.03531900834328535, 'entropy_loss': -0.023943716660141945, 'vf_loss': 0.006437991297160308, 'total_loss': -0.05282473370626699, 'approx_kl': -0.015322840074077249, 'clip_fraction': 0.3723958358168602, 'grad_norm': 1.1391624212265015}
2022-12-29 21:08:01.668 DEBUG: Taking gradient step
2022-12-29 21:08:01.678 DEBUG: Loss 6: {'policy_loss': -0.02800844484963852, 'entropy_loss': -0.02490712096914649, 'vf_loss': 0.00642797511125783, 'total_loss': -0.046487590707527185, 'approx_kl': -0.007178622297942638, 'clip_fraction': 0.3463541716337204, 'grad_norm': 7.89285945892334}
2022-12-29 21:08:02.461 DEBUG: Taking gradient step
2022-12-29 21:08:02.474 DEBUG: Loss 7: {'policy_loss': -0.030781037535584255, 'entropy_loss': -0.024635461159050465, 'vf_loss': 0.0064250638485506, 'total_loss': -0.04899143484608412, 'approx_kl': 0.005090919672511518, 'clip_fraction': 0.3841145858168602, 'grad_norm': 7.5149312019348145}
2022-12-29 21:08:03.312 DEBUG: Taking gradient step
2022-12-29 21:08:03.321 DEBUG: Loss 8: {'policy_loss': -0.03329428002633174, 'entropy_loss': -0.025802834425121546, 'vf_loss': 0.006425470991469038, 'total_loss': -0.052671643459984256, 'approx_kl': 0.030237682163715363, 'clip_fraction': 0.3854166716337204, 'grad_norm': 2.526948928833008}
2022-12-29 21:08:04.066 DEBUG: Taking gradient step
2022-12-29 21:08:04.075 DEBUG: Loss 9: {'policy_loss': 0.0069007035503531634, 'entropy_loss': -0.02648122562095523, 'vf_loss': 0.009046234689903024, 'total_loss': -0.010534287380699045, 'approx_kl': 0.026219628751277924, 'clip_fraction': 0.37109375, 'grad_norm': 2.3467373847961426}
2022-12-29 21:08:04.075 INFO: Optimization: policy loss=0.007, vf loss=0.009, entropy loss=-0.026, total loss=-0.011, num steps=10
2022-12-29 21:08:04.075 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 21:08:05.620 INFO: Evaluation rollout: return=0.801 (0.0), episode length=5.0
2022-12-29 21:08:05.621 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 21:08:05.624 INFO: Iteration: 127/137, steps: 27432
2022-12-29 21:08:13.247 DEBUG: Atoms are too close
2022-12-29 21:08:17.999 DEBUG: Atoms are too close
2022-12-29 21:08:20.111 DEBUG: Atoms are too close
2022-12-29 21:09:03.307 INFO: Training rollout: return=-0.971 (5.5), episode length=4.9
2022-12-29 21:09:03.308 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 21:09:03.311 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-27432_train.pkl
2022-12-29 21:09:04.110 DEBUG: Taking gradient step
2022-12-29 21:09:04.119 DEBUG: Loss 0: {'policy_loss': -0.04050472109501181, 'entropy_loss': -0.027446941938251257, 'vf_loss': 0.0201951834760268, 'total_loss': -0.047756479557236275, 'approx_kl': -5.093170329928398e-09, 'clip_fraction': 0.0, 'grad_norm': 9.246265411376953}
2022-12-29 21:09:04.900 DEBUG: Taking gradient step
2022-12-29 21:09:04.912 DEBUG: Loss 1: {'policy_loss': -0.01871596642861951, 'entropy_loss': -0.027842677664011717, 'vf_loss': 0.022681284349279365, 'total_loss': -0.02387735974335186, 'approx_kl': 0.015941282617859542, 'clip_fraction': 0.09114583395421505, 'grad_norm': 8.808507919311523}
2022-12-29 21:09:05.779 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 21:09:05.779 INFO: Optimization: policy loss=-0.019, vf loss=0.023, entropy loss=-0.028, total loss=-0.024, num steps=2
2022-12-29 21:09:05.779 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 21:09:07.247 INFO: Evaluation rollout: return=0.799 (0.0), episode length=5.0
2022-12-29 21:09:07.248 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 21:09:07.250 INFO: Iteration: 128/137, steps: 27648
2022-12-29 21:09:50.102 DEBUG: Atoms are too close
2022-12-29 21:09:51.580 DEBUG: Atoms are too close
2022-12-29 21:09:51.582 DEBUG: Atoms are too close
2022-12-29 21:10:04.278 INFO: Training rollout: return=-1.054 (5.6), episode length=4.9
2022-12-29 21:10:04.281 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 21:10:04.283 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-27648_train.pkl
2022-12-29 21:10:05.080 DEBUG: Taking gradient step
2022-12-29 21:10:05.089 DEBUG: Loss 0: {'policy_loss': -0.03839628125033512, 'entropy_loss': -0.027363481000065804, 'vf_loss': 0.018807992064876582, 'total_loss': -0.046951770185524336, 'approx_kl': -1.0411410400479326e-07, 'clip_fraction': 0.0, 'grad_norm': 16.58414077758789}
2022-12-29 21:10:05.900 DEBUG: Taking gradient step
2022-12-29 21:10:05.910 DEBUG: Loss 1: {'policy_loss': 0.006471630120558396, 'entropy_loss': -0.026714804582297802, 'vf_loss': 0.023980867728442067, 'total_loss': 0.0037376932667026602, 'approx_kl': 0.013012947048991919, 'clip_fraction': 0.0859375, 'grad_norm': 3.6305861473083496}
2022-12-29 21:10:06.727 DEBUG: Taking gradient step
2022-12-29 21:10:06.736 DEBUG: Loss 2: {'policy_loss': 0.007679197264003436, 'entropy_loss': -0.026939328759908676, 'vf_loss': 0.023937316918527844, 'total_loss': 0.004677185422622604, 'approx_kl': 0.04400100326165557, 'clip_fraction': 0.26953125, 'grad_norm': 6.793665409088135}
2022-12-29 21:10:07.551 DEBUG: Early stopping at step 3 for reaching max KL.
2022-12-29 21:10:07.551 INFO: Optimization: policy loss=0.008, vf loss=0.024, entropy loss=-0.027, total loss=0.005, num steps=3
2022-12-29 21:10:07.552 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 21:10:09.010 INFO: Evaluation rollout: return=0.796 (0.0), episode length=5.0
2022-12-29 21:10:09.011 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 21:10:09.014 INFO: Iteration: 129/137, steps: 27864
2022-12-29 21:10:56.833 DEBUG: Atoms are too close
2022-12-29 21:11:07.043 INFO: Training rollout: return=0.104 (3.3), episode length=5.0
2022-12-29 21:11:07.044 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 21:11:07.047 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-27864_train.pkl
2022-12-29 21:11:07.867 DEBUG: Taking gradient step
2022-12-29 21:11:07.877 DEBUG: Loss 0: {'policy_loss': 0.021290918009852347, 'entropy_loss': -0.026408291421830654, 'vf_loss': 0.010343300262346596, 'total_loss': 0.0052259268503682885, 'approx_kl': -2.8715767541598325e-09, 'clip_fraction': 0.0, 'grad_norm': 6.748998165130615}
2022-12-29 21:11:08.669 DEBUG: Taking gradient step
2022-12-29 21:11:08.678 DEBUG: Loss 1: {'policy_loss': 0.024845459256780004, 'entropy_loss': -0.027128467336297035, 'vf_loss': 0.010355228778269483, 'total_loss': 0.00807222069875245, 'approx_kl': 0.006897299666889012, 'clip_fraction': 0.0651041679084301, 'grad_norm': 11.565095901489258}
2022-12-29 21:11:09.583 DEBUG: Taking gradient step
2022-12-29 21:11:09.593 DEBUG: Loss 2: {'policy_loss': -0.02169852476895738, 'entropy_loss': -0.025934719014912844, 'vf_loss': 0.007823343530114843, 'total_loss': -0.03980990025375539, 'approx_kl': 0.009037853684276342, 'clip_fraction': 0.2057291679084301, 'grad_norm': 11.257282257080078}
2022-12-29 21:11:10.380 DEBUG: Taking gradient step
2022-12-29 21:11:10.389 DEBUG: Loss 3: {'policy_loss': -0.022534132999165728, 'entropy_loss': -0.026260676328092813, 'vf_loss': 0.007816313595974497, 'total_loss': -0.040978495731284044, 'approx_kl': 0.032979102339595556, 'clip_fraction': 0.3151041716337204, 'grad_norm': 9.142333030700684}
2022-12-29 21:11:11.191 DEBUG: Early stopping at step 4 for reaching max KL.
2022-12-29 21:11:11.191 INFO: Optimization: policy loss=-0.023, vf loss=0.008, entropy loss=-0.026, total loss=-0.041, num steps=4
2022-12-29 21:11:11.192 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 21:11:12.609 INFO: Evaluation rollout: return=0.783 (0.0), episode length=5.0
2022-12-29 21:11:12.610 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 21:11:12.613 INFO: Iteration: 130/137, steps: 28080
2022-12-29 21:12:10.506 INFO: Training rollout: return=0.698 (0.1), episode length=5.0
2022-12-29 21:12:10.508 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 21:12:10.510 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-28080_train.pkl
2022-12-29 21:12:11.332 DEBUG: Taking gradient step
2022-12-29 21:12:11.341 DEBUG: Loss 0: {'policy_loss': -0.0696274820381109, 'entropy_loss': -0.0254763369448483, 'vf_loss': 0.00028433460123869015, 'total_loss': -0.09481948438172051, 'approx_kl': -1.0291114449501038e-07, 'clip_fraction': 0.0, 'grad_norm': 15.666483879089355}
2022-12-29 21:12:12.139 DEBUG: Taking gradient step
2022-12-29 21:12:12.149 DEBUG: Loss 1: {'policy_loss': -0.02942061662105531, 'entropy_loss': -0.024280291981995106, 'vf_loss': 0.00029432297764948884, 'total_loss': -0.05340658562540093, 'approx_kl': 0.0001759393489919603, 'clip_fraction': 0.0078125, 'grad_norm': 22.29593276977539}
2022-12-29 21:12:12.959 DEBUG: Taking gradient step
2022-12-29 21:12:12.969 DEBUG: Loss 2: {'policy_loss': -0.03831312499276153, 'entropy_loss': -0.024500155821442604, 'vf_loss': 0.0003070687373180761, 'total_loss': -0.06250621207688606, 'approx_kl': 0.0003834216622635722, 'clip_fraction': 0.08984375, 'grad_norm': 8.062849044799805}
2022-12-29 21:12:13.800 DEBUG: Taking gradient step
2022-12-29 21:12:13.810 DEBUG: Loss 3: {'policy_loss': -0.013437344506907738, 'entropy_loss': -0.023472985718399286, 'vf_loss': 0.0003130485314971506, 'total_loss': -0.03659728169380988, 'approx_kl': 0.006579107721336186, 'clip_fraction': 0.30859375, 'grad_norm': 22.013792037963867}
2022-12-29 21:12:14.654 DEBUG: Taking gradient step
2022-12-29 21:12:14.662 DEBUG: Loss 4: {'policy_loss': -0.02870527526863807, 'entropy_loss': -0.02324187057092786, 'vf_loss': 0.0003245483476135083, 'total_loss': -0.051622597491952416, 'approx_kl': 0.019632528070360422, 'clip_fraction': 0.4075520858168602, 'grad_norm': 20.668214797973633}
2022-12-29 21:12:15.539 DEBUG: Taking gradient step
2022-12-29 21:12:15.549 DEBUG: Loss 5: {'policy_loss': 0.011487494408432797, 'entropy_loss': -0.022342710755765438, 'vf_loss': 0.00032099776131203734, 'total_loss': -0.010534218586020602, 'approx_kl': 0.028648461680859327, 'clip_fraction': 0.4010416716337204, 'grad_norm': 19.60578727722168}
2022-12-29 21:12:16.398 DEBUG: Taking gradient step
2022-12-29 21:12:16.412 DEBUG: Loss 6: {'policy_loss': -0.004265202455885161, 'entropy_loss': -0.02210150472819805, 'vf_loss': 0.0003246689632975927, 'total_loss': -0.02604203822078562, 'approx_kl': 0.00909337680786848, 'clip_fraction': 0.3671875, 'grad_norm': 8.006561279296875}
2022-12-29 21:12:17.266 DEBUG: Taking gradient step
2022-12-29 21:12:17.276 DEBUG: Loss 7: {'policy_loss': 0.019914981002388973, 'entropy_loss': -0.0220193681307137, 'vf_loss': 0.0003217216788274895, 'total_loss': -0.0017826654494972408, 'approx_kl': 0.02198075409978628, 'clip_fraction': 0.2135416679084301, 'grad_norm': 23.484949111938477}
2022-12-29 21:12:18.040 DEBUG: Taking gradient step
2022-12-29 21:12:18.049 DEBUG: Loss 8: {'policy_loss': -0.06616926898859278, 'entropy_loss': -0.022385151125490665, 'vf_loss': 0.00033061709562171384, 'total_loss': -0.08822380301846172, 'approx_kl': 0.011293874587863684, 'clip_fraction': 0.2252604179084301, 'grad_norm': 21.28392791748047}
2022-12-29 21:12:18.920 DEBUG: Taking gradient step
2022-12-29 21:12:18.930 DEBUG: Loss 9: {'policy_loss': 0.026480629033454495, 'entropy_loss': -0.02138731861487031, 'vf_loss': 0.0003123294855722216, 'total_loss': 0.005405639904156406, 'approx_kl': 0.02763546328060329, 'clip_fraction': 0.1627604179084301, 'grad_norm': 21.45841407775879}
2022-12-29 21:12:18.930 INFO: Optimization: policy loss=0.026, vf loss=0.000, entropy loss=-0.021, total loss=0.005, num steps=10
2022-12-29 21:12:18.931 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 21:12:20.523 INFO: Evaluation rollout: return=0.785 (0.0), episode length=5.0
2022-12-29 21:12:20.524 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 21:12:20.527 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-26136.model
2022-12-29 21:12:20.529 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-28296.model
2022-12-29 21:12:20.564 INFO: Iteration: 131/137, steps: 28296
2022-12-29 21:12:34.609 DEBUG: Atoms are too close
2022-12-29 21:13:19.093 INFO: Training rollout: return=0.106 (3.3), episode length=5.0
2022-12-29 21:13:19.094 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 21:13:19.097 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-28296_train.pkl
2022-12-29 21:13:19.916 DEBUG: Taking gradient step
2022-12-29 21:13:19.925 DEBUG: Loss 0: {'policy_loss': 0.020934250155968216, 'entropy_loss': -0.021020574495196342, 'vf_loss': 0.01032567313067147, 'total_loss': 0.010239348791443344, 'approx_kl': 6.404782970648171e-08, 'clip_fraction': 0.0, 'grad_norm': 8.559200286865234}
2022-12-29 21:13:20.731 DEBUG: Taking gradient step
2022-12-29 21:13:20.740 DEBUG: Loss 1: {'policy_loss': 0.0031801117073923715, 'entropy_loss': -0.02090129302814603, 'vf_loss': 0.010294985569597138, 'total_loss': -0.007426195751156522, 'approx_kl': 0.016703973291441798, 'clip_fraction': 0.109375, 'grad_norm': 4.162696361541748}
2022-12-29 21:13:21.545 DEBUG: Taking gradient step
2022-12-29 21:13:21.555 DEBUG: Loss 2: {'policy_loss': -0.023821644398942, 'entropy_loss': -0.020853972528129816, 'vf_loss': 0.0077925397509099655, 'total_loss': -0.03688307717616184, 'approx_kl': 0.03674187930300832, 'clip_fraction': 0.31640625, 'grad_norm': 4.942805290222168}
2022-12-29 21:13:22.384 DEBUG: Taking gradient step
2022-12-29 21:13:22.394 DEBUG: Loss 3: {'policy_loss': -0.031784563558551866, 'entropy_loss': -0.021172933746129274, 'vf_loss': 0.007804085612203015, 'total_loss': -0.04515341169247812, 'approx_kl': 0.032963099190965295, 'clip_fraction': 0.3190104216337204, 'grad_norm': 4.743241310119629}
2022-12-29 21:13:23.177 DEBUG: Taking gradient step
2022-12-29 21:13:23.186 DEBUG: Loss 4: {'policy_loss': 0.01265617114791399, 'entropy_loss': -0.019850348820909858, 'vf_loss': 0.010283371786989702, 'total_loss': 0.0030891941139938386, 'approx_kl': 0.034540506545454264, 'clip_fraction': 0.3385416716337204, 'grad_norm': 8.497143745422363}
2022-12-29 21:13:23.994 DEBUG: Taking gradient step
2022-12-29 21:13:24.003 DEBUG: Loss 5: {'policy_loss': -0.02695374938999289, 'entropy_loss': -0.020897209644317627, 'vf_loss': 0.007789513707481176, 'total_loss': -0.04006144532682934, 'approx_kl': 0.02466376218944788, 'clip_fraction': 0.3567708358168602, 'grad_norm': 3.8580641746520996}
2022-12-29 21:13:24.804 DEBUG: Taking gradient step
2022-12-29 21:13:24.814 DEBUG: Loss 6: {'policy_loss': -0.023109689904996844, 'entropy_loss': -0.02182034170255065, 'vf_loss': 0.00778655966342183, 'total_loss': -0.037143471944125664, 'approx_kl': 0.03908080677501857, 'clip_fraction': 0.3919270858168602, 'grad_norm': 8.703304290771484}
2022-12-29 21:13:25.621 DEBUG: Taking gradient step
2022-12-29 21:13:25.635 DEBUG: Loss 7: {'policy_loss': 0.035690153635730956, 'entropy_loss': -0.022203982807695866, 'vf_loss': 0.012801957649887903, 'total_loss': 0.026288128477922994, 'approx_kl': 0.025144565850496292, 'clip_fraction': 0.3893229216337204, 'grad_norm': 10.119790077209473}
2022-12-29 21:13:26.439 DEBUG: Taking gradient step
2022-12-29 21:13:26.454 DEBUG: Loss 8: {'policy_loss': -0.034806789889951804, 'entropy_loss': -0.021346531342715025, 'vf_loss': 0.007794744262148732, 'total_loss': -0.0483585769705181, 'approx_kl': 0.024920760421082377, 'clip_fraction': 0.3138020858168602, 'grad_norm': 3.5821375846862793}
2022-12-29 21:13:27.330 DEBUG: Taking gradient step
2022-12-29 21:13:27.339 DEBUG: Loss 9: {'policy_loss': -0.03338249746768142, 'entropy_loss': -0.022606567479670048, 'vf_loss': 0.00779807229994996, 'total_loss': -0.0481909926474015, 'approx_kl': 0.037750125396996737, 'clip_fraction': 0.3528645858168602, 'grad_norm': 7.467503070831299}
2022-12-29 21:13:27.339 INFO: Optimization: policy loss=-0.033, vf loss=0.008, entropy loss=-0.023, total loss=-0.048, num steps=10
2022-12-29 21:13:27.340 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 21:13:28.820 INFO: Evaluation rollout: return=0.780 (0.0), episode length=5.0
2022-12-29 21:13:28.822 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 21:13:28.824 INFO: Iteration: 132/137, steps: 28512
2022-12-29 21:14:26.251 INFO: Training rollout: return=0.691 (0.1), episode length=5.0
2022-12-29 21:14:26.253 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 21:14:26.256 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-28512_train.pkl
2022-12-29 21:14:27.087 DEBUG: Taking gradient step
2022-12-29 21:14:27.097 DEBUG: Loss 0: {'policy_loss': -0.01446684768073166, 'entropy_loss': -0.02274281717836857, 'vf_loss': 0.000275354750730632, 'total_loss': -0.03693431010836959, 'approx_kl': -2.9511284083127975e-08, 'clip_fraction': 0.0, 'grad_norm': 19.6785945892334}
2022-12-29 21:14:27.886 DEBUG: Taking gradient step
2022-12-29 21:14:27.895 DEBUG: Loss 1: {'policy_loss': 0.010702575156497782, 'entropy_loss': -0.02286997577175498, 'vf_loss': 0.0002682748885635971, 'total_loss': -0.011899125726693605, 'approx_kl': 0.0025485691730864346, 'clip_fraction': 0.0, 'grad_norm': 17.244304656982422}
2022-12-29 21:14:28.703 DEBUG: Taking gradient step
2022-12-29 21:14:28.712 DEBUG: Loss 2: {'policy_loss': 0.013879893974177687, 'entropy_loss': -0.024159899912774563, 'vf_loss': 0.0002592631809955123, 'total_loss': -0.010020742757601363, 'approx_kl': 0.010085807356517762, 'clip_fraction': 0.022135416977107525, 'grad_norm': 14.70651626586914}
2022-12-29 21:14:29.623 DEBUG: Taking gradient step
2022-12-29 21:14:29.633 DEBUG: Loss 3: {'policy_loss': -0.027369716234188338, 'entropy_loss': -0.023510120809078217, 'vf_loss': 0.00025340967043733663, 'total_loss': -0.05062642737282922, 'approx_kl': 0.036792591447010636, 'clip_fraction': 0.2747395858168602, 'grad_norm': 12.488109588623047}
2022-12-29 21:14:30.426 DEBUG: Taking gradient step
2022-12-29 21:14:30.435 DEBUG: Loss 4: {'policy_loss': 0.007519515385485426, 'entropy_loss': -0.022875063586980104, 'vf_loss': 0.0002400300635528032, 'total_loss': -0.015115518137941873, 'approx_kl': 0.041128252632915974, 'clip_fraction': 0.328125, 'grad_norm': 20.156660079956055}
2022-12-29 21:14:31.283 DEBUG: Taking gradient step
2022-12-29 21:14:31.293 DEBUG: Loss 5: {'policy_loss': 0.063695982874214, 'entropy_loss': -0.023817503824830055, 'vf_loss': 0.00021633973582937358, 'total_loss': 0.040094818785213315, 'approx_kl': 0.04335304535925388, 'clip_fraction': 0.3489583432674408, 'grad_norm': 21.25094985961914}
2022-12-29 21:14:32.104 DEBUG: Taking gradient step
2022-12-29 21:14:32.113 DEBUG: Loss 6: {'policy_loss': 0.005503729749614567, 'entropy_loss': -0.024098367895931005, 'vf_loss': 0.00021269562973419076, 'total_loss': -0.018381942516582244, 'approx_kl': 0.029639917891472578, 'clip_fraction': 0.10286458395421505, 'grad_norm': 6.3437700271606445}
2022-12-29 21:14:32.899 DEBUG: Taking gradient step
2022-12-29 21:14:32.910 DEBUG: Loss 7: {'policy_loss': 0.026174262323380177, 'entropy_loss': -0.023439906537532806, 'vf_loss': 0.00019501509071449545, 'total_loss': 0.0029293708765618653, 'approx_kl': 0.015622545965015888, 'clip_fraction': 0.01953125, 'grad_norm': 10.291059494018555}
2022-12-29 21:14:33.690 DEBUG: Taking gradient step
2022-12-29 21:14:33.701 DEBUG: Loss 8: {'policy_loss': -0.06082589950888454, 'entropy_loss': -0.024487642105668783, 'vf_loss': 0.00018734921455084504, 'total_loss': -0.08512619240000249, 'approx_kl': 0.013179399771615863, 'clip_fraction': 0.015625, 'grad_norm': 13.822138786315918}
2022-12-29 21:14:34.551 DEBUG: Taking gradient step
2022-12-29 21:14:34.565 DEBUG: Loss 9: {'policy_loss': -0.02768942558197727, 'entropy_loss': -0.024470293894410133, 'vf_loss': 0.00016729656577804057, 'total_loss': -0.051992422910609355, 'approx_kl': 0.023482847027480602, 'clip_fraction': 0.14583333395421505, 'grad_norm': 9.793444633483887}
2022-12-29 21:14:34.565 INFO: Optimization: policy loss=-0.028, vf loss=0.000, entropy loss=-0.024, total loss=-0.052, num steps=10
2022-12-29 21:14:34.565 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 21:14:36.035 INFO: Evaluation rollout: return=0.782 (0.0), episode length=5.0
2022-12-29 21:14:36.036 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 21:14:36.039 INFO: Iteration: 133/137, steps: 28728
2022-12-29 21:15:35.093 INFO: Training rollout: return=0.666 (0.1), episode length=5.0
2022-12-29 21:15:35.095 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 21:15:35.097 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-28728_train.pkl
2022-12-29 21:15:35.891 DEBUG: Taking gradient step
2022-12-29 21:15:35.901 DEBUG: Loss 0: {'policy_loss': -0.031220004995598967, 'entropy_loss': -0.022806009277701378, 'vf_loss': 0.00014600133365030054, 'total_loss': -0.05388001293965004, 'approx_kl': -7.776543498039246e-08, 'clip_fraction': 0.0, 'grad_norm': 15.380240440368652}
2022-12-29 21:15:36.710 DEBUG: Taking gradient step
2022-12-29 21:15:36.721 DEBUG: Loss 1: {'policy_loss': -0.025131951292746167, 'entropy_loss': -0.02342375833541155, 'vf_loss': 0.0001320248458496579, 'total_loss': -0.04842368478230807, 'approx_kl': 0.0006403340958058834, 'clip_fraction': 0.00390625, 'grad_norm': 8.173492431640625}
2022-12-29 21:15:37.510 DEBUG: Taking gradient step
2022-12-29 21:15:37.521 DEBUG: Loss 2: {'policy_loss': -0.043749458943023754, 'entropy_loss': -0.02312701614573598, 'vf_loss': 0.00012164683098305168, 'total_loss': -0.06675482825777668, 'approx_kl': 0.005913385306484997, 'clip_fraction': 0.09635416697710752, 'grad_norm': 12.679788589477539}
2022-12-29 21:15:38.324 DEBUG: Taking gradient step
2022-12-29 21:15:38.334 DEBUG: Loss 3: {'policy_loss': -0.06658416838834348, 'entropy_loss': -0.022185378707945347, 'vf_loss': 0.00012103386216387387, 'total_loss': -0.08864851323412495, 'approx_kl': 0.01868529655621387, 'clip_fraction': 0.3346354216337204, 'grad_norm': 16.08689308166504}
2022-12-29 21:15:39.134 DEBUG: Taking gradient step
2022-12-29 21:15:39.143 DEBUG: Loss 4: {'policy_loss': -0.003378092971517834, 'entropy_loss': -0.02271898929029703, 'vf_loss': 0.00010814240553826254, 'total_loss': -0.0259889398562766, 'approx_kl': 0.034142439253628254, 'clip_fraction': 0.34765625, 'grad_norm': 10.953493118286133}
2022-12-29 21:15:40.040 DEBUG: Taking gradient step
2022-12-29 21:15:40.055 DEBUG: Loss 5: {'policy_loss': -0.0008794542756414333, 'entropy_loss': -0.02219683676958084, 'vf_loss': 0.00010283563488105164, 'total_loss': -0.022973455410341223, 'approx_kl': 0.029978090897202492, 'clip_fraction': 0.2291666679084301, 'grad_norm': 9.734722137451172}
2022-12-29 21:15:40.904 DEBUG: Taking gradient step
2022-12-29 21:15:40.916 DEBUG: Loss 6: {'policy_loss': -0.027636528236598748, 'entropy_loss': -0.021558634005486965, 'vf_loss': 0.00010335334422007662, 'total_loss': -0.04909180889786564, 'approx_kl': 0.03447421611053869, 'clip_fraction': 0.16145833395421505, 'grad_norm': 14.00959587097168}
2022-12-29 21:15:41.693 DEBUG: Taking gradient step
2022-12-29 21:15:41.702 DEBUG: Loss 7: {'policy_loss': 0.03231734869986895, 'entropy_loss': -0.021703468170017004, 'vf_loss': 0.0001016953788181883, 'total_loss': 0.010715575908670144, 'approx_kl': 0.03150281426496804, 'clip_fraction': 0.11197916697710752, 'grad_norm': 13.126898765563965}
2022-12-29 21:15:42.500 DEBUG: Taking gradient step
2022-12-29 21:15:42.509 DEBUG: Loss 8: {'policy_loss': 0.018760179377457047, 'entropy_loss': -0.022954048588871956, 'vf_loss': 0.00011136987095608997, 'total_loss': -0.00408249934045881, 'approx_kl': 0.024190498399548233, 'clip_fraction': 0.09114583395421505, 'grad_norm': 4.5919189453125}
2022-12-29 21:15:43.327 DEBUG: Taking gradient step
2022-12-29 21:15:43.337 DEBUG: Loss 9: {'policy_loss': -0.034749363202908196, 'entropy_loss': -0.0220805904828012, 'vf_loss': 0.0001103775034714952, 'total_loss': -0.0567195761822379, 'approx_kl': 0.019943302730098367, 'clip_fraction': 0.2200520858168602, 'grad_norm': 18.811708450317383}
2022-12-29 21:15:43.338 INFO: Optimization: policy loss=-0.035, vf loss=0.000, entropy loss=-0.022, total loss=-0.057, num steps=10
2022-12-29 21:15:43.338 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 21:15:44.822 INFO: Evaluation rollout: return=0.774 (0.0), episode length=5.0
2022-12-29 21:15:44.824 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 21:15:44.826 INFO: Iteration: 134/137, steps: 28944
2022-12-29 21:16:43.138 INFO: Training rollout: return=0.700 (0.1), episode length=5.0
2022-12-29 21:16:43.140 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 21:16:43.142 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-28944_train.pkl
2022-12-29 21:16:44.010 DEBUG: Taking gradient step
2022-12-29 21:16:44.020 DEBUG: Loss 0: {'policy_loss': 0.030101596008111048, 'entropy_loss': -0.02352060889825225, 'vf_loss': 0.0001474291654389685, 'total_loss': 0.006728416275297765, 'approx_kl': 2.764863893389702e-08, 'clip_fraction': 0.0, 'grad_norm': 7.2779316902160645}
2022-12-29 21:16:44.803 DEBUG: Taking gradient step
2022-12-29 21:16:44.812 DEBUG: Loss 1: {'policy_loss': -0.023188039433066493, 'entropy_loss': -0.022934697568416595, 'vf_loss': 0.00014673511577625062, 'total_loss': -0.045976001885706844, 'approx_kl': 0.008659627521410584, 'clip_fraction': 0.07291666697710752, 'grad_norm': 9.89034652709961}
2022-12-29 21:16:45.607 DEBUG: Taking gradient step
2022-12-29 21:16:45.620 DEBUG: Loss 2: {'policy_loss': 0.002855163861611939, 'entropy_loss': -0.023165981750935316, 'vf_loss': 0.00013297073366931605, 'total_loss': -0.020177847155654065, 'approx_kl': 0.020527649845462292, 'clip_fraction': 0.1588541679084301, 'grad_norm': 13.307441711425781}
2022-12-29 21:16:46.424 DEBUG: Taking gradient step
2022-12-29 21:16:46.433 DEBUG: Loss 3: {'policy_loss': 0.019125736639266235, 'entropy_loss': -0.023289592936635017, 'vf_loss': 0.00013434933171314962, 'total_loss': -0.004029506965655634, 'approx_kl': 0.017166053410619497, 'clip_fraction': 0.1341145858168602, 'grad_norm': 9.161970138549805}
2022-12-29 21:16:47.225 DEBUG: Taking gradient step
2022-12-29 21:16:47.234 DEBUG: Loss 4: {'policy_loss': -0.004915606246623745, 'entropy_loss': -0.02428887691348791, 'vf_loss': 0.00013409741282796857, 'total_loss': -0.02907038574728369, 'approx_kl': 0.02970837103202939, 'clip_fraction': 0.1510416679084301, 'grad_norm': 16.97507667541504}
2022-12-29 21:16:48.072 DEBUG: Taking gradient step
2022-12-29 21:16:48.082 DEBUG: Loss 5: {'policy_loss': -0.03464296703680862, 'entropy_loss': -0.023686803877353668, 'vf_loss': 0.00012813172140417785, 'total_loss': -0.05820163919275811, 'approx_kl': 0.03794516483321786, 'clip_fraction': 0.15364583395421505, 'grad_norm': 13.154441833496094}
2022-12-29 21:16:48.880 DEBUG: Early stopping at step 6 for reaching max KL.
2022-12-29 21:16:48.880 INFO: Optimization: policy loss=-0.035, vf loss=0.000, entropy loss=-0.024, total loss=-0.058, num steps=6
2022-12-29 21:16:48.880 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 21:16:50.343 INFO: Evaluation rollout: return=0.775 (0.0), episode length=5.0
2022-12-29 21:16:50.344 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 21:16:50.347 INFO: Iteration: 135/137, steps: 29160
2022-12-29 21:17:49.123 INFO: Training rollout: return=0.684 (0.1), episode length=5.0
2022-12-29 21:17:49.125 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 21:17:49.128 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-29160_train.pkl
2022-12-29 21:17:49.949 DEBUG: Taking gradient step
2022-12-29 21:17:49.959 DEBUG: Loss 0: {'policy_loss': -0.04239646473217643, 'entropy_loss': -0.02433214196935296, 'vf_loss': 0.00010909609723939785, 'total_loss': -0.06661951060428999, 'approx_kl': 1.437729224562645e-08, 'clip_fraction': 0.0, 'grad_norm': 9.194968223571777}
2022-12-29 21:17:50.780 DEBUG: Taking gradient step
2022-12-29 21:17:50.794 DEBUG: Loss 1: {'policy_loss': -0.005260500095994684, 'entropy_loss': -0.024592329282313585, 'vf_loss': 9.939637166116077e-05, 'total_loss': -0.029753433006647113, 'approx_kl': 0.007162895664805546, 'clip_fraction': 0.00390625, 'grad_norm': 8.978105545043945}
2022-12-29 21:17:51.598 DEBUG: Taking gradient step
2022-12-29 21:17:51.608 DEBUG: Loss 2: {'policy_loss': -0.04894855669596386, 'entropy_loss': -0.02435813518241048, 'vf_loss': 9.733883358813565e-05, 'total_loss': -0.07320935304478621, 'approx_kl': 0.024972144223283976, 'clip_fraction': 0.15625, 'grad_norm': 10.148507118225098}
2022-12-29 21:17:52.385 DEBUG: Taking gradient step
2022-12-29 21:17:52.394 DEBUG: Loss 3: {'policy_loss': -0.03390737601292501, 'entropy_loss': -0.026814449578523636, 'vf_loss': 9.925333487598718e-05, 'total_loss': -0.06062257225657265, 'approx_kl': 0.04313743906095624, 'clip_fraction': 0.2278645858168602, 'grad_norm': 9.8486967086792}
2022-12-29 21:17:53.161 DEBUG: Taking gradient step
2022-12-29 21:17:53.170 DEBUG: Loss 4: {'policy_loss': -0.03345160062655955, 'entropy_loss': -0.026326139457523823, 'vf_loss': 9.089185860044868e-05, 'total_loss': -0.059686848225482925, 'approx_kl': 0.042722880374640226, 'clip_fraction': 0.2526041716337204, 'grad_norm': 5.670094966888428}
2022-12-29 21:17:53.943 DEBUG: Early stopping at step 5 for reaching max KL.
2022-12-29 21:17:53.943 INFO: Optimization: policy loss=-0.033, vf loss=0.000, entropy loss=-0.026, total loss=-0.060, num steps=5
2022-12-29 21:17:53.944 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 21:17:55.395 INFO: Evaluation rollout: return=0.784 (0.0), episode length=5.0
2022-12-29 21:17:55.397 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 21:17:55.400 INFO: Iteration: 136/137, steps: 29376
2022-12-29 21:18:53.071 INFO: Training rollout: return=0.654 (0.1), episode length=5.0
2022-12-29 21:18:53.072 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 21:18:53.075 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-29376_train.pkl
2022-12-29 21:18:53.862 DEBUG: Taking gradient step
2022-12-29 21:18:53.872 DEBUG: Loss 0: {'policy_loss': 0.00044234152004112937, 'entropy_loss': -0.0273403599858284, 'vf_loss': 7.449592811201015e-05, 'total_loss': -0.02682352253767526, 'approx_kl': -4.5207951160364246e-08, 'clip_fraction': 0.0, 'grad_norm': 16.878446578979492}
2022-12-29 21:18:54.679 DEBUG: Taking gradient step
2022-12-29 21:18:54.688 DEBUG: Loss 1: {'policy_loss': -0.01665970474827164, 'entropy_loss': -0.026375457644462585, 'vf_loss': 6.97058481882122e-05, 'total_loss': -0.04296545654454602, 'approx_kl': 0.013834710465744138, 'clip_fraction': 0.0234375, 'grad_norm': 11.017571449279785}
2022-12-29 21:18:55.469 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 21:18:55.469 INFO: Optimization: policy loss=-0.017, vf loss=0.000, entropy loss=-0.026, total loss=-0.043, num steps=2
2022-12-29 21:18:55.470 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 21:18:56.924 INFO: Evaluation rollout: return=0.788 (0.0), episode length=5.0
2022-12-29 21:18:56.926 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 21:18:56.928 INFO: Iteration: 137/137, steps: 29592
2022-12-29 21:19:11.143 DEBUG: There is a single atom floating around
2022-12-29 21:19:27.197 DEBUG: There is a single atom floating around
2022-12-29 21:19:27.539 DEBUG: There is a single atom floating around
2022-12-29 21:19:42.998 DEBUG: There is a single atom floating around
2022-12-29 21:19:53.776 INFO: Training rollout: return=-1.594 (6.4), episode length=5.0
2022-12-29 21:19:53.777 DEBUG: Saving info: runs/CH4/results/CH4_run-1_train.txt
2022-12-29 21:19:53.780 DEBUG: Saving rollout: runs/CH4/data/CH4_run-1_steps-29592_train.pkl
2022-12-29 21:19:54.616 DEBUG: Taking gradient step
2022-12-29 21:19:54.625 DEBUG: Loss 0: {'policy_loss': -0.007232540858211418, 'entropy_loss': -0.028384405188262463, 'vf_loss': 0.03692004823361296, 'total_loss': 0.0013031021871390928, 'approx_kl': 3.620516508817673e-08, 'clip_fraction': 0.0, 'grad_norm': 15.40894603729248}
2022-12-29 21:19:55.460 DEBUG: Taking gradient step
2022-12-29 21:19:55.469 DEBUG: Loss 1: {'policy_loss': 0.055245389036830114, 'entropy_loss': -0.026986886281520128, 'vf_loss': 0.044540839047983075, 'total_loss': 0.07279934180329306, 'approx_kl': 0.02204391488339752, 'clip_fraction': 0.10677083395421505, 'grad_norm': 16.558725357055664}
2022-12-29 21:19:56.354 DEBUG: Early stopping at step 2 for reaching max KL.
2022-12-29 21:19:56.354 INFO: Optimization: policy loss=0.055, vf loss=0.045, entropy loss=-0.027, total loss=0.073, num steps=2
2022-12-29 21:19:56.355 DEBUG: Saving info: runs/CH4/results/CH4_run-1_opt.txt
2022-12-29 21:19:57.721 INFO: Evaluation rollout: return=0.794 (0.0), episode length=5.0
2022-12-29 21:19:57.722 DEBUG: Saving info: runs/CH4/results/CH4_run-1_eval.txt
2022-12-29 21:19:57.725 DEBUG: Deleting old model: runs/CH4/models/CH4_run-1_steps-28296.model
2022-12-29 21:19:57.727 DEBUG: Saving model: runs/CH4/models/CH4_run-1_steps-29808.model
2022-12-29 21:19:57.757 INFO: Finished PPO
